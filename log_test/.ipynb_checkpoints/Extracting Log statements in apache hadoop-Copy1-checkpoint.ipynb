{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining problem statement\n",
    "- Extract and analyse and visualise logging statements used in Apache Hadoop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats in the project\n",
    "- Highly open ended task\n",
    "- Apache Hadoop project comprise of 7 different programming languages\n",
    "- Inconsistancy of using logging statements in the code base\n",
    "- logging levels and context of using them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches Used\n",
    "- Considered only Java (92.8% of the code base)\n",
    "- Understand why develpers need logging statemtents in code\n",
    "- What are the importance of logging levels in logging statement\n",
    "- What are the libraries or approaches of using logging statements in JAVA and which all libraries are used in this project\n",
    "- Distribution of logging statement usages over libraries\n",
    "- Most populat logging library and logging level\n",
    "- Dependencies of logging level to \n",
    "    - number of lines of code in a file\n",
    "    - context of using the log level - evaluate in which scenario they used this log level (evaluate previos code)\n",
    "- **Where we can get the logging statements?**\n",
    "    - Along with source code the development history(git log) provides valuable information about logging practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some observations\n",
    "- basic libraries are Log4j\n",
    "- Abstraction libraries JCL and Slf4j\n",
    "- Unification libraries Logback and Log4j2\n",
    "- This given project(hadoop has used Log4j and slf4j in the code base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding importance of logging statements in codebase\n",
    "- It helps the developers to trace the runtime behaviour \n",
    "- Logging statement practices are seems to be not very consistent\n",
    "- In each code base there is trace of either **insufficient logging** or **excessive logging**\n",
    "- The logging statements in the code base stores the timestamped log message to a prespecified log file\n",
    "- One of the challenge that software engineer faces is that how to use most effective logging statements or in other words, maximise the value of the logged information while minimizing logging overhead\n",
    "- proactive logging --> excessive logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior studies\n",
    "- Mining logging code\n",
    "- Mining logging message\n",
    "- Automatic log insertion\n",
    "- Learning to log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this analysis can help the software engineers\n",
    "- There are many studies based on the logging practices because selection of logging statement is somethin the SE always find difficults\n",
    "- Unfortunately, all the current researches are post development - this papers are focusing more on suggesting log statements on a realtime scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('text.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commit 5092ea62ecbac840d56978a31bb11cfc14c6fe83</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Author: Steve Loughran &lt;stevel@cloudera.com&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Date:   Sat Aug 15 12:51:08 2020 +0100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HADOOP-13230. S3A to optionally retain directo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This adds an option to disable \"empty director...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>so avoid throttling and other scale problems.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This feature is *not* backwards compatible.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Consult the documentation and use with care.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Contributed by Steve Loughran.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Change-Id: I69a61e7584dc36e485d5e39ff25b1e3e55...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>diff --git a/hadoop-common-project/hadoop-comm...</td>\n",
       "      <td>b/hadoop-common-project/hadoop-common/src/test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>new file mode 100644</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>index 00000000000..8c5e553f71e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>--- /dev/null</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>+++ b/hadoop-common-project/hadoop-common/src/...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@@ -0,0 +1,74 @@</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>+/*</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>+ * Licensed to the Apache Software Foundation...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>+ * or more contributor license agreements.  S...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>+ * distributed with this work for additional ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>+ * regarding copyright ownership.  The ASF li...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>+ * to you under the Apache License, Version 2...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>+ * \"License\"); you may not use this file exce...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>+ * with the License.  You may obtain a copy o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10662</th>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10663</th>\n",
       "      <td>+  @Override</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664</th>\n",
       "      <td>+  public void setup() throws Exception {</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10665</th>\n",
       "      <td>+    super.setup();</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10666</th>\n",
       "      <td>+    rootPath = getFileSystem().makeQualified(...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10667</th>\n",
       "      <td>+  }</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10668</th>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10669</th>\n",
       "      <td>+  @Test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10670</th>\n",
       "      <td>+  public void test_100_audit_root_noauth() th...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10671</th>\n",
       "      <td>+    describe(\"Run a verbose audit\");</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10672</th>\n",
       "      <td>+    final File audit = tempAuditFile();</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10673</th>\n",
       "      <td>+    run(MARKERS, V,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10674</th>\n",
       "      <td>+        AUDIT,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10675</th>\n",
       "      <td>+        m(OPT_OUT), audit,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10676</th>\n",
       "      <td>+        rootPath);</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10677</th>\n",
       "      <td>+    readOutput(audit);</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10678</th>\n",
       "      <td>+  }</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10679</th>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10680</th>\n",
       "      <td>+  @Test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10681</th>\n",
       "      <td>+  public void test_200_clean_root() throws Th...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10682</th>\n",
       "      <td>+    describe(\"Clean the root path\");</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683</th>\n",
       "      <td>+    final File audit = tempAuditFile();</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10684</th>\n",
       "      <td>+    run(MARKERS, V,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10685</th>\n",
       "      <td>+        CLEAN,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>+        m(OPT_OUT), audit,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10687</th>\n",
       "      <td>+        rootPath);</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10688</th>\n",
       "      <td>+    readOutput(audit);</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689</th>\n",
       "      <td>+  }</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10691</th>\n",
       "      <td>+}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10692 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0        commit 5092ea62ecbac840d56978a31bb11cfc14c6fe83   \n",
       "1           Author: Steve Loughran <stevel@cloudera.com>   \n",
       "2                 Date:   Sat Aug 15 12:51:08 2020 +0100   \n",
       "3                                                    NaN   \n",
       "4      HADOOP-13230. S3A to optionally retain directo...   \n",
       "5                                                    NaN   \n",
       "6      This adds an option to disable \"empty director...   \n",
       "7          so avoid throttling and other scale problems.   \n",
       "8                                                    NaN   \n",
       "9            This feature is *not* backwards compatible.   \n",
       "10          Consult the documentation and use with care.   \n",
       "11                                                   NaN   \n",
       "12                        Contributed by Steve Loughran.   \n",
       "13                                                   NaN   \n",
       "14     Change-Id: I69a61e7584dc36e485d5e39ff25b1e3e55...   \n",
       "15                                                   NaN   \n",
       "16     diff --git a/hadoop-common-project/hadoop-comm...   \n",
       "17                                  new file mode 100644   \n",
       "18                        index 00000000000..8c5e553f71e   \n",
       "19                                         --- /dev/null   \n",
       "20     +++ b/hadoop-common-project/hadoop-common/src/...   \n",
       "21                                      @@ -0,0 +1,74 @@   \n",
       "22                                                   +/*   \n",
       "23     + * Licensed to the Apache Software Foundation...   \n",
       "24     + * or more contributor license agreements.  S...   \n",
       "25     + * distributed with this work for additional ...   \n",
       "26     + * regarding copyright ownership.  The ASF li...   \n",
       "27     + * to you under the Apache License, Version 2...   \n",
       "28     + * \"License\"); you may not use this file exce...   \n",
       "29     + * with the License.  You may obtain a copy o...   \n",
       "...                                                  ...   \n",
       "10662                                                  +   \n",
       "10663                                       +  @Override   \n",
       "10664          +  public void setup() throws Exception {   \n",
       "10665                                +    super.setup();   \n",
       "10666  +    rootPath = getFileSystem().makeQualified(...   \n",
       "10667                                               +  }   \n",
       "10668                                                  +   \n",
       "10669                                           +  @Test   \n",
       "10670  +  public void test_100_audit_root_noauth() th...   \n",
       "10671              +    describe(\"Run a verbose audit\");   \n",
       "10672           +    final File audit = tempAuditFile();   \n",
       "10673                               +    run(MARKERS, V,   \n",
       "10674                                    +        AUDIT,   \n",
       "10675                        +        m(OPT_OUT), audit,   \n",
       "10676                                +        rootPath);   \n",
       "10677                            +    readOutput(audit);   \n",
       "10678                                               +  }   \n",
       "10679                                                  +   \n",
       "10680                                           +  @Test   \n",
       "10681  +  public void test_200_clean_root() throws Th...   \n",
       "10682              +    describe(\"Clean the root path\");   \n",
       "10683           +    final File audit = tempAuditFile();   \n",
       "10684                               +    run(MARKERS, V,   \n",
       "10685                                    +        CLEAN,   \n",
       "10686                        +        m(OPT_OUT), audit,   \n",
       "10687                                +        rootPath);   \n",
       "10688                            +    readOutput(audit);   \n",
       "10689                                               +  }   \n",
       "10690                                                  +   \n",
       "10691                                                 +}   \n",
       "\n",
       "                                                       1  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "5                                                    NaN  \n",
       "6                                                    NaN  \n",
       "7                                                    NaN  \n",
       "8                                                    NaN  \n",
       "9                                                    NaN  \n",
       "10                                                   NaN  \n",
       "11                                                   NaN  \n",
       "12                                                   NaN  \n",
       "13                                                   NaN  \n",
       "14                                                   NaN  \n",
       "15                                                   NaN  \n",
       "16     b/hadoop-common-project/hadoop-common/src/test...  \n",
       "17                                                   NaN  \n",
       "18                                                   NaN  \n",
       "19                                                   NaN  \n",
       "20                                                   NaN  \n",
       "21                                                   NaN  \n",
       "22                                                   NaN  \n",
       "23                                                   NaN  \n",
       "24                                                   NaN  \n",
       "25                                                   NaN  \n",
       "26                                                   NaN  \n",
       "27                                                   NaN  \n",
       "28                                                   NaN  \n",
       "29                                                   NaN  \n",
       "...                                                  ...  \n",
       "10662                                                NaN  \n",
       "10663                                                NaN  \n",
       "10664                                                NaN  \n",
       "10665                                                NaN  \n",
       "10666                                                NaN  \n",
       "10667                                                NaN  \n",
       "10668                                                NaN  \n",
       "10669                                                NaN  \n",
       "10670                                                NaN  \n",
       "10671                                                NaN  \n",
       "10672                                                NaN  \n",
       "10673                                                NaN  \n",
       "10674                                                NaN  \n",
       "10675                                                NaN  \n",
       "10676                                                NaN  \n",
       "10677                                                NaN  \n",
       "10678                                                NaN  \n",
       "10679                                                NaN  \n",
       "10680                                                NaN  \n",
       "10681                                                NaN  \n",
       "10682                                                NaN  \n",
       "10683                                                NaN  \n",
       "10684                                                NaN  \n",
       "10685                                                NaN  \n",
       "10686                                                NaN  \n",
       "10687                                                NaN  \n",
       "10688                                                NaN  \n",
       "10689                                                NaN  \n",
       "10690                                                NaN  \n",
       "10691                                                NaN  \n",
       "\n",
       "[10692 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['commit 5092ea62ecbac840d56978a31bb11cfc14c6fe83', nan]\n",
      "['Author: Steve Loughran <stevel@cloudera.com>', nan]\n",
      "['Date:   Sat Aug 15 12:51:08 2020 +0100', nan]\n",
      "[nan, nan]\n",
      "['HADOOP-13230. S3A to optionally retain directory markers.', nan]\n",
      "[nan, nan]\n",
      "['This adds an option to disable \"empty directory\" marker deletion,', nan]\n",
      "['so avoid throttling and other scale problems.', nan]\n",
      "[nan, nan]\n",
      "['This feature is *not* backwards compatible.', nan]\n",
      "['Consult the documentation and use with care.', nan]\n",
      "[nan, nan]\n",
      "['Contributed by Steve Loughran.', nan]\n",
      "[nan, nan]\n",
      "['Change-Id: I69a61e7584dc36e485d5e39ff25b1e3e559a1958', nan]\n",
      "[nan, nan]\n",
      "['diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/AssertExtensions.java', 'b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/AssertExtensions.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..8c5e553f71e', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/AssertExtensions.java', nan]\n",
      "['@@ -0,0 +1,74 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.test;', nan]\n",
      "['+', nan]\n",
      "['+import java.util.concurrent.Callable;', nan]\n",
      "['+', nan]\n",
      "['+import org.assertj.core.description.Description;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Extra classes to work with AssertJ.', nan]\n",
      "[\"+ * These are kept separate from {@link LambdaTestUtils} so there's\", nan]\n",
      "['+ * no requirement for AssertJ to be on the classpath in that broadly', nan]\n",
      "['+ * used class.', nan]\n",
      "['+ */', nan]\n",
      "['+public final class AssertExtensions {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(AssertExtensions.class);', nan]\n",
      "['+', nan]\n",
      "['+  private AssertExtensions() {', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A description for AssertJ \"describedAs\" clauses which evaluates the', nan]\n",
      "['+   * lambda-expression only on failure. That must return a string', nan]\n",
      "['+   * or null/\"\" to be skipped.', nan]\n",
      "['+   * @param eval lambda expression to invoke', nan]\n",
      "['+   * @return a description for AssertJ', nan]\n",
      "['+   */', nan]\n",
      "['+  public static Description dynamicDescription(Callable<String> eval) {', nan]\n",
      "['+    return new DynamicDescription(eval);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  private static final class DynamicDescription extends Description {', nan]\n",
      "['+    private final Callable<String> eval;', nan]\n",
      "['+', nan]\n",
      "['+    private DynamicDescription(final Callable<String> eval) {', nan]\n",
      "['+      this.eval = eval;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public String value() {', nan]\n",
      "['+      try {', nan]\n",
      "['+        return eval.call();', nan]\n",
      "['+      } catch (Exception e) {', nan]\n",
      "['+        LOG.warn(\"Failed to evaluate description: \" + e);', nan]\n",
      "['+        LOG.debug(\"Evaluation failure\", e);', nan]\n",
      "['+        // return null so that the description evaluation chain', nan]\n",
      "['+        // will skip this one', nan]\n",
      "['+        return null;', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/pom.xml b/hadoop-tools/hadoop-aws/pom.xml', nan]\n",
      "['index 5cdaf26007f..af8983e2ebe 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/pom.xml', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/pom.xml', nan]\n",
      "['@@ -52,6 +52,10 @@', nan]\n",
      "['<!-- Set a longer timeout for integration test (in milliseconds) -->', nan]\n",
      "['<test.integration.timeout>200000</test.integration.timeout>', nan]\n",
      "[nan, nan]\n",
      "['+    <!--   should directory marker retention be audited? -->', nan]\n",
      "['+    <fs.s3a.directory.marker.audit>false</fs.s3a.directory.marker.audit>', nan]\n",
      "['+    <!--    marker retention policy -->', nan]\n",
      "['+    <fs.s3a.directory.marker.retention></fs.s3a.directory.marker.retention>', nan]\n",
      "['</properties>', nan]\n",
      "[nan, nan]\n",
      "['<profiles>', nan]\n",
      "['@@ -123,6 +127,9 @@', nan]\n",
      "['<fs.s3a.scale.test.huge.filesize>${fs.s3a.scale.test.huge.filesize}</fs.s3a.scale.test.hug', '.filesize>']\n",
      "['<fs.s3a.scale.test.huge.huge.partitionsize>${fs.s3a.scale.test.huge.partitionsize}</fs.s3a', 'scale.test.huge.huge.partitionsize>']\n",
      "['<fs.s3a.scale.test.timeout>${fs.s3a.scale.test.timeout}</fs.s3a.scale.test.timeout>', nan]\n",
      "['+                <!-- Markers-->', nan]\n",
      "['+                <fs.s3a.directory.marker.retention>${fs.s3a.directory.marker.retention}</fs.s3a.directory.', 'arker.retention>']\n",
      "['+                <fs.s3a.directory.marker.audit>${fs.s3a.directory.marker.audit}</fs.s3a.directory.marker.a', 'dit>']\n",
      "['</systemPropertyVariables>', nan]\n",
      "['</configuration>', nan]\n",
      "['</plugin>', nan]\n",
      "['@@ -163,6 +170,7 @@', nan]\n",
      "['<fs.s3a.s3guard.test.enabled>${fs.s3a.s3guard.test.enabled}</fs.s3a.s3guard.test.enabl', 'd>']\n",
      "['<fs.s3a.s3guard.test.authoritative>${fs.s3a.s3guard.test.authoritative}</fs.s3a.s3guar', '.test.authoritative>']\n",
      "['<fs.s3a.s3guard.test.implementation>${fs.s3a.s3guard.test.implementation}</fs.s3a.s3gu', 'rd.test.implementation>']\n",
      "['+                    <fs.s3a.directory.marker.retention>${fs.s3a.directory.marker.retention}</fs.s3a.direct', 'ry.marker.retention>']\n",
      "[nan, nan]\n",
      "['<test.default.timeout>${test.integration.timeout}</test.default.timeout>', nan]\n",
      "['</systemPropertyVariables>', nan]\n",
      "['@@ -189,6 +197,8 @@', nan]\n",
      "['<exclude>**/ITestDynamoDBMetadataStoreScale.java</exclude>', nan]\n",
      "['<!-- Terasort MR jobs spawn enough processes that they use up all RAM -->', nan]\n",
      "['<exclude>**/ITestTerasort*.java</exclude>', nan]\n",
      "['+                    <!-- Root marker tool tests -->', nan]\n",
      "['+                    <exclude>**/ITestMarkerToolRootOperations.java</exclude>', nan]\n",
      "['<!-- operations across the metastore -->', nan]\n",
      "['<exclude>**/ITestS3GuardDDBRootOperations.java</exclude>', nan]\n",
      "['</excludes>', nan]\n",
      "['@@ -215,6 +225,9 @@', nan]\n",
      "['<fs.s3a.s3guard.test.enabled>${fs.s3a.s3guard.test.enabled}</fs.s3a.s3guard.test.enabl', 'd>']\n",
      "['<fs.s3a.s3guard.test.implementation>${fs.s3a.s3guard.test.implementation}</fs.s3a.s3gu', 'rd.test.implementation>']\n",
      "['<fs.s3a.s3guard.test.authoritative>${fs.s3a.s3guard.test.authoritative}</fs.s3a.s3guar', '.test.authoritative>']\n",
      "['+                    <!-- Markers-->', nan]\n",
      "['+                    <fs.s3a.directory.marker.retention>${fs.s3a.directory.marker.retention}</fs.s3a.direct', 'ry.marker.retention>']\n",
      "['+                    <fs.s3a.directory.marker.audit>${fs.s3a.directory.marker.audit}</fs.s3a.directory.mark', 'r.audit>']\n",
      "['</systemPropertyVariables>', nan]\n",
      "['<!-- Do a sequential run for tests that cannot handle -->', nan]\n",
      "['<!-- parallel execution. -->', nan]\n",
      "['@@ -230,6 +243,10 @@', nan]\n",
      "['<!--  the local FS. Running them sequentially guarantees isolation -->', nan]\n",
      "[\"<!-- and that they don't conflict with the other MR jobs for RAM -->\", nan]\n",
      "['<include>**/ITestTerasort*.java</include>', nan]\n",
      "['+                    <!-- Root marker tool tests -->', nan]\n",
      "[\"+                    <!-- MUST be run before the other root ops so there's\", nan]\n",
      "['+                     more likelihood of files in the bucket -->', nan]\n",
      "['+                    <include>**/ITestMarkerToolRootOperations.java</include>', nan]\n",
      "['<!-- operations across the metastore -->', nan]\n",
      "['<include>**/ITestS3AContractRootDir.java</include>', nan]\n",
      "['<include>**/ITestS3GuardDDBRootOperations.java</include>', nan]\n",
      "['@@ -269,6 +286,9 @@', nan]\n",
      "['<fs.s3a.s3guard.test.enabled>${fs.s3a.s3guard.test.enabled}</fs.s3a.s3guard.test.enabl', 'd>']\n",
      "['<fs.s3a.s3guard.test.implementation>${fs.s3a.s3guard.test.implementation}</fs.s3a.s3gu', 'rd.test.implementation>']\n",
      "['<fs.s3a.s3guard.test.authoritative>${fs.s3a.s3guard.test.authoritative}</fs.s3a.s3guar', '.test.authoritative>']\n",
      "['+                    <!-- Markers-->', nan]\n",
      "['+                    <fs.s3a.directory.marker.retention>${fs.s3a.directory.marker.retention}</fs.s3a.direct', 'ry.marker.retention>']\n",
      "['+                    <fs.s3a.directory.marker.audit>${fs.s3a.directory.marker.audit}</fs.s3a.directory.mark', 'r.audit>']\n",
      "['</systemPropertyVariables>', nan]\n",
      "['<forkedProcessTimeoutInSeconds>${fs.s3a.scale.test.timeout}</forkedProcessTimeoutInSecon', 's>']\n",
      "['</configuration>', nan]\n",
      "['@@ -332,6 +352,44 @@', nan]\n",
      "['</properties>', nan]\n",
      "['</profile>', nan]\n",
      "[nan, nan]\n",
      "['+    <!-- Directory marker retention options, all from the -Dmarkers value-->', nan]\n",
      "['+    <profile>', nan]\n",
      "['+      <id>keep-markers</id>', nan]\n",
      "['+      <activation>', nan]\n",
      "['+        <property>', nan]\n",
      "['+          <name>markers</name>', nan]\n",
      "['+          <value>keep</value>', nan]\n",
      "['+        </property>', nan]\n",
      "['+      </activation>', nan]\n",
      "['+      <properties >', nan]\n",
      "['+        <fs.s3a.directory.marker.retention>keep</fs.s3a.directory.marker.retention>', nan]\n",
      "['+      </properties>', nan]\n",
      "['+    </profile>', nan]\n",
      "['+    <profile>', nan]\n",
      "['+      <id>delete-markers</id>', nan]\n",
      "['+      <activation>', nan]\n",
      "['+        <property>', nan]\n",
      "['+          <name>markers</name>', nan]\n",
      "['+          <value>delete</value>', nan]\n",
      "['+        </property>', nan]\n",
      "['+      </activation>', nan]\n",
      "['+      <properties >', nan]\n",
      "['+        <fs.s3a.directory.marker.retention>delete</fs.s3a.directory.marker.retention>', nan]\n",
      "['+      </properties>', nan]\n",
      "['+    </profile>', nan]\n",
      "['+    <profile>', nan]\n",
      "['+      <id>auth-markers</id>', nan]\n",
      "['+      <activation>', nan]\n",
      "['+        <property>', nan]\n",
      "['+          <name>markers</name>', nan]\n",
      "['+          <value>authoritative</value>', nan]\n",
      "['+        </property>', nan]\n",
      "['+      </activation>', nan]\n",
      "['+      <properties >', nan]\n",
      "['+        <fs.s3a.directory.marker.retention>authoritative</fs.s3a.directory.marker.retention>', nan]\n",
      "['+      </properties>', nan]\n",
      "['+    </profile>', nan]\n",
      "['+', nan]\n",
      "['</profiles>', nan]\n",
      "[nan, nan]\n",
      "['<build>', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java b/hadoop-tools/h', 'doop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java']\n",
      "['index 22a0b45f1c7..a1c1d969a82 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java', nan]\n",
      "['@@ -953,4 +953,92 @@ private Constants() {', nan]\n",
      "['* Value: {@value} seconds.', nan]\n",
      "['*/', nan]\n",
      "['public static final int THREAD_POOL_SHUTDOWN_DELAY_SECONDS = 30;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Policy for directory markers.', nan]\n",
      "['+   * This is a new feature of HADOOP-13230 which addresses', nan]\n",
      "['+   * some scale, performance and permissions issues -but', nan]\n",
      "['+   * at the risk of backwards compatibility.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String DIRECTORY_MARKER_POLICY =', nan]\n",
      "['+      \"fs.s3a.directory.marker.retention\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Delete directory markers. This is the backwards compatible option.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String DIRECTORY_MARKER_POLICY_DELETE =', nan]\n",
      "['+      \"delete\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Retain directory markers.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String DIRECTORY_MARKER_POLICY_KEEP =', nan]\n",
      "['+      \"keep\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Retain directory markers in authoritative directory trees only.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String DIRECTORY_MARKER_POLICY_AUTHORITATIVE =', nan]\n",
      "['+      \"authoritative\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Default retention policy: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String DEFAULT_DIRECTORY_MARKER_POLICY =', nan]\n",
      "['+      DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * {@code PathCapabilities} probe to verify that an S3A Filesystem', nan]\n",
      "['+   * has the changes needed to safely work with buckets where', nan]\n",
      "['+   * directoy markers have not been deleted.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String STORE_CAPABILITY_DIRECTORY_MARKER_AWARE', nan]\n",
      "['+      = \"fs.s3a.capability.directory.marker.aware\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * {@code PathCapabilities} probe to indicate that the filesystem', nan]\n",
      "['+   * keeps directory markers.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_KEEP', nan]\n",
      "['+      = \"fs.s3a.capability.directory.marker.policy.keep\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * {@code PathCapabilities} probe to indicate that the filesystem', nan]\n",
      "['+   * deletes directory markers.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_DELETE', nan]\n",
      "['+      = \"fs.s3a.capability.directory.marker.policy.delete\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * {@code PathCapabilities} probe to indicate that the filesystem', nan]\n",
      "['+   * keeps directory markers in authoritative paths only.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String', nan]\n",
      "['+      STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_AUTHORITATIVE =', nan]\n",
      "['+      \"fs.s3a.capability.directory.marker.policy.authoritative\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * {@code PathCapabilities} probe to indicate that a path', nan]\n",
      "['+   * keeps directory markers.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP', nan]\n",
      "['+      = \"fs.s3a.capability.directory.marker.action.keep\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * {@code PathCapabilities} probe to indicate that a path', nan]\n",
      "['+   * deletes directory markers.', nan]\n",
      "['+   * Value: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE', nan]\n",
      "['+      = \"fs.s3a.capability.directory.marker.action.delete\";', nan]\n",
      "['+', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentAmazonS3Client.java', 'b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentAmazonS3Client.java']\n",
      "['index 34c043be9cb..4cb05ae9e65 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentAmazonS3Client.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentAmazonS3Client.java', nan]\n",
      "['@@ -330,7 +330,9 @@ private boolean isDescendant(String parent, String child, boolean recursive) {', nan]\n",
      "['} else {', nan]\n",
      "['Path actualParentPath = new Path(child).getParent();', nan]\n",
      "['Path expectedParentPath = new Path(parent);', nan]\n",
      "['-      return actualParentPath.equals(expectedParentPath);', nan]\n",
      "['+      // children which are directory markers are excluded here', nan]\n",
      "['+      return actualParentPath.equals(expectedParentPath)', nan]\n",
      "['+          && !child.endsWith(\"/\");', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java b/hadoop-tools/had', 'op-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java']\n",
      "['index fcb492857e6..34129e0bf1a 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java', nan]\n",
      "['@@ -142,12 +142,27 @@ public FileStatusListingIterator createFileStatusListingIterator(', nan]\n",
      "['Listing.FileStatusAcceptor acceptor,', nan]\n",
      "['RemoteIterator<S3AFileStatus> providedStatus) throws IOException {', nan]\n",
      "['return new FileStatusListingIterator(', nan]\n",
      "['-        new ObjectListingIterator(listPath, request),', nan]\n",
      "['+        createObjectListingIterator(listPath, request),', nan]\n",
      "['filter,', nan]\n",
      "['acceptor,', nan]\n",
      "['providedStatus);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create an object listing iterator against a path, with a given', nan]\n",
      "['+   * list object request.', nan]\n",
      "['+   * @param listPath path of the listing', nan]\n",
      "['+   * @param request initial request to make', nan]\n",
      "['+   * @return the iterator', nan]\n",
      "['+   * @throws IOException IO Problems', nan]\n",
      "['+   */', nan]\n",
      "['+  @Retries.RetryRaw', nan]\n",
      "['+  public ObjectListingIterator createObjectListingIterator(', nan]\n",
      "['+      final Path listPath,', nan]\n",
      "['+      final S3ListRequest request) throws IOException {', nan]\n",
      "['+    return new ObjectListingIterator(listPath, request);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Create a located status iterator over a file status iterator.', nan]\n",
      "['* @param statusIterator an iterator over the remote status entries', nan]\n",
      "['@@ -194,8 +209,12 @@ TombstoneReconcilingIterator createTombstoneReconcilingIterator(', nan]\n",
      "[nan, nan]\n",
      "['String key = maybeAddTrailingSlash(pathToKey(path));', nan]\n",
      "['String delimiter = recursive ? null : \"/\";', nan]\n",
      "['-    LOG.debug(\"Requesting all entries under {} with delimiter \\'{}\\'\",', nan]\n",
      "['-            key, delimiter);', nan]\n",
      "['+    if (recursive) {', nan]\n",
      "['+      LOG.debug(\"Recursive list of all entries under {}\", key);', nan]\n",
      "['+    } else {', nan]\n",
      "['+      LOG.debug(\"Requesting all entries under {} with delimiter \\'{}\\'\",', nan]\n",
      "['+          key, delimiter);', nan]\n",
      "['+    }', nan]\n",
      "['final RemoteIterator<S3AFileStatus> cachedFilesIterator;', nan]\n",
      "['final Set<Path> tombstones;', nan]\n",
      "['boolean allowAuthoritative = listingOperationCallbacks', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java b/hadoop-too', 's/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java']\n",
      "['index 2cd23255c4b..ac9904a867e 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java', nan]\n",
      "['@@ -67,7 +67,7 @@', nan]\n",
      "['import com.amazonaws.services.s3.model.ObjectMetadata;', nan]\n",
      "['import com.amazonaws.services.s3.model.PutObjectRequest;', nan]\n",
      "['import com.amazonaws.services.s3.model.PutObjectResult;', nan]\n",
      "['-import com.amazonaws.services.s3.model.S3ObjectSummary;', nan]\n",
      "['+', nan]\n",
      "['import com.amazonaws.services.s3.model.SSEAwsKeyManagementParams;', nan]\n",
      "['import com.amazonaws.services.s3.model.SSECustomerKey;', nan]\n",
      "['import com.amazonaws.services.s3.model.UploadPartRequest;', nan]\n",
      "['@@ -104,6 +104,8 @@', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.ContextAccessors;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.CopyOutcome;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.DeleteOperation;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.DirectoryPolicy;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.InternalConstants;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.ListingOperationCallbacks;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport;', nan]\n",
      "['@@ -116,6 +118,8 @@', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.statistics.S3AMultipartUploaderStatisticsImpl;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.s3guard.BulkOperationState;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.select.InternalSelectConstants;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.tools.MarkerToolOperations;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.tools.MarkerToolOperationsImpl;', nan]\n",
      "['import org.apache.hadoop.io.IOUtils;', nan]\n",
      "['import org.apache.hadoop.io.Text;', nan]\n",
      "['import org.apache.hadoop.security.token.DelegationTokenIssuer;', nan]\n",
      "['@@ -295,6 +299,15 @@', nan]\n",
      "[nan, nan]\n",
      "['private final ListingOperationCallbacks listingOperationCallbacks =', nan]\n",
      "['new ListingOperationCallbacksImpl();', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Directory policy.', nan]\n",
      "['+   */', nan]\n",
      "['+  private DirectoryPolicy directoryPolicy;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Context accessors for re-use.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final ContextAccessors contextAccessors = new ContextAccessorsImpl();', nan]\n",
      "[nan, nan]\n",
      "['/** Add any deprecated keys. */', nan]\n",
      "['@SuppressWarnings(\"deprecation\")', nan]\n",
      "['@@ -452,6 +465,10 @@ public void initialize(URI name, Configuration originalConf)', nan]\n",
      "['DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);', nan]\n",
      "['S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);', nan]\n",
      "['}', nan]\n",
      "['+      // directory policy, which may look at authoritative paths', nan]\n",
      "['+      directoryPolicy = DirectoryPolicyImpl.getDirectoryPolicy(conf,', nan]\n",
      "['+          this::allowAuthoritative);', nan]\n",
      "['+      LOG.debug(\"Directory marker retention policy is {}\", directoryPolicy);', nan]\n",
      "[nan, nan]\n",
      "['initMultipartUploads(conf);', nan]\n",
      "[nan, nan]\n",
      "['@@ -1285,7 +1302,7 @@ public WriteOperationHelper getWriteOperationHelper() {', nan]\n",
      "['* is not a directory.', nan]\n",
      "['*/', nan]\n",
      "['@Override', nan]\n",
      "['-  public FSDataOutputStream createNonRecursive(Path path,', nan]\n",
      "['+  public FSDataOutputStream createNonRecursive(Path p,', nan]\n",
      "['FsPermission permission,', nan]\n",
      "['EnumSet<CreateFlag> flags,', nan]\n",
      "['int bufferSize,', nan]\n",
      "['@@ -1293,10 +1310,22 @@ public FSDataOutputStream createNonRecursive(Path path,', nan]\n",
      "['long blockSize,', nan]\n",
      "['Progressable progress) throws IOException {', nan]\n",
      "['entryPoint(INVOCATION_CREATE_NON_RECURSIVE);', nan]\n",
      "['+    final Path path = makeQualified(p);', nan]\n",
      "['Path parent = path.getParent();', nan]\n",
      "['-    if (parent != null) {', nan]\n",
      "['-      // expect this to raise an exception if there is no parent', nan]\n",
      "['-      if (!getFileStatus(parent).isDirectory()) {', nan]\n",
      "['+    // expect this to raise an exception if there is no parent dir', nan]\n",
      "['+    if (parent != null && !parent.isRoot()) {', nan]\n",
      "['+      S3AFileStatus status;', nan]\n",
      "['+      try {', nan]\n",
      "['+        // optimize for the directory existing: Call list first', nan]\n",
      "['+        status = innerGetFileStatus(parent, false,', nan]\n",
      "['+            StatusProbeEnum.DIRECTORIES);', nan]\n",
      "['+      } catch (FileNotFoundException e) {', nan]\n",
      "['+        // no dir, fall back to looking for a file', nan]\n",
      "['+        // (failure condition if true)', nan]\n",
      "['+        status = innerGetFileStatus(parent, false,', nan]\n",
      "['+            StatusProbeEnum.HEAD_ONLY);', nan]\n",
      "['+      }', nan]\n",
      "['+      if (!status.isDirectory()) {', nan]\n",
      "['throw new FileAlreadyExistsException(\"Not a directory: \" + parent);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['@@ -1431,10 +1460,13 @@ public boolean rename(Path src, Path dst) throws IOException {', nan]\n",
      "['LOG.debug(\"rename: destination path {} not found\", dst);', nan]\n",
      "['// Parent must exist', nan]\n",
      "['Path parent = dst.getParent();', nan]\n",
      "['-      if (!pathToKey(parent).isEmpty()) {', nan]\n",
      "['+      if (!pathToKey(parent).isEmpty()', nan]\n",
      "['+          && !parent.equals(src.getParent())) {', nan]\n",
      "['try {', nan]\n",
      "['-          S3AFileStatus dstParentStatus = innerGetFileStatus(dst.getParent(),', nan]\n",
      "['-              false, StatusProbeEnum.ALL);', nan]\n",
      "['+          // only look against S3 for directories; saves', nan]\n",
      "['+          // a HEAD request on all normal operations.', nan]\n",
      "['+          S3AFileStatus dstParentStatus = innerGetFileStatus(parent,', nan]\n",
      "['+              false, StatusProbeEnum.DIRECTORIES);', nan]\n",
      "['if (!dstParentStatus.isDirectory()) {', nan]\n",
      "['throw new RenameFailedException(src, dst,', nan]\n",
      "['\"destination parent is not a directory\");', nan]\n",
      "['@@ -1535,7 +1567,7 @@ public void deleteObjectAtPath(final Path path,', nan]\n",
      "['final boolean isFile,', nan]\n",
      "['final BulkOperationState operationState)', nan]\n",
      "['throws IOException {', nan]\n",
      "['-      once(\"delete\", key, () ->', nan]\n",
      "['+      once(\"delete\", path.toString(), () ->', nan]\n",
      "['S3AFileSystem.this.deleteObjectAtPath(path, key, isFile,', nan]\n",
      "['operationState));', nan]\n",
      "['}', nan]\n",
      "['@@ -1585,7 +1617,9 @@ public void finishRename(final Path sourceRenamed, final Path destCreated)', nan]\n",
      "['Path destParent = destCreated.getParent();', nan]\n",
      "['if (!sourceRenamed.getParent().equals(destParent)) {', nan]\n",
      "['LOG.debug(\"source & dest parents are different; fix up dir markers\");', nan]\n",
      "['-        deleteUnnecessaryFakeDirectories(destParent);', nan]\n",
      "['+        if (!keepDirectoryMarkers(destParent)) {', nan]\n",
      "['+          deleteUnnecessaryFakeDirectories(destParent, null);', nan]\n",
      "['+        }', nan]\n",
      "['maybeCreateFakeParentDirectory(sourceRenamed);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['@@ -1940,6 +1974,7 @@ protected ObjectMetadata getObjectMetadata(String key,', nan]\n",
      "['protected S3ListResult listObjects(S3ListRequest request) throws IOException {', nan]\n",
      "['incrementReadOperations();', nan]\n",
      "['incrementStatistic(OBJECT_LIST_REQUESTS);', nan]\n",
      "['+    LOG.debug(\"LIST {}\", request);', nan]\n",
      "['validateListArguments(request);', nan]\n",
      "['try(DurationInfo ignored =', nan]\n",
      "['new DurationInfo(LOG, false, \"LIST\")) {', nan]\n",
      "['@@ -2381,6 +2416,14 @@ private DeleteObjectsResult removeKeysS3(', nan]\n",
      "['boolean quiet)', nan]\n",
      "['throws MultiObjectDeleteException, AmazonClientException,', nan]\n",
      "['IOException {', nan]\n",
      "['+    if (LOG.isDebugEnabled()) {', nan]\n",
      "['+      LOG.debug(\"Initiating delete operation for {} objects\",', nan]\n",
      "['+          keysToDelete.size());', nan]\n",
      "['+      for (DeleteObjectsRequest.KeyVersion key : keysToDelete) {', nan]\n",
      "['+        LOG.debug(\" {} {}\", key.getKey(),', nan]\n",
      "['+            key.getVersion() != null ? key.getVersion() : \"\");', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['DeleteObjectsResult result = null;', nan]\n",
      "['if (keysToDelete.isEmpty()) {', nan]\n",
      "['// exit fast if there are no keys to delete', nan]\n",
      "['@@ -2490,7 +2533,8 @@ DeleteObjectsResult removeKeys(', nan]\n",
      "['final boolean quiet)', nan]\n",
      "['throws MultiObjectDeleteException, AmazonClientException, IOException {', nan]\n",
      "['undeletedObjectsOnFailure.clear();', nan]\n",
      "['-    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Deleting\")) {', nan]\n",
      "['+    try (DurationInfo ignored = new DurationInfo(LOG, false,', nan]\n",
      "['+        \"Deleting %d keys\", keysToDelete.size())) {', nan]\n",
      "['return removeKeysS3(keysToDelete, deleteFakeDir, quiet);', nan]\n",
      "['} catch (MultiObjectDeleteException ex) {', nan]\n",
      "['LOG.debug(\"Partial delete failure\");', nan]\n",
      "['@@ -2573,7 +2617,7 @@ private void createFakeDirectoryIfNecessary(Path f)', nan]\n",
      "['// we only make the LIST call; the codepaths to get here should not', nan]\n",
      "['// be reached if there is an empty dir marker -and if they do, it', nan]\n",
      "['// is mostly harmless to create a new one.', nan]\n",
      "['-    if (!key.isEmpty() && !s3Exists(f, EnumSet.of(StatusProbeEnum.List))) {', nan]\n",
      "['+    if (!key.isEmpty() && !s3Exists(f, StatusProbeEnum.DIRECTORIES)) {', nan]\n",
      "['LOG.debug(\"Creating new fake directory at {}\", f);', nan]\n",
      "['createFakeDirectory(key);', nan]\n",
      "['}', nan]\n",
      "['@@ -2589,7 +2633,7 @@ private void createFakeDirectoryIfNecessary(Path f)', nan]\n",
      "['void maybeCreateFakeParentDirectory(Path path)', nan]\n",
      "['throws IOException, AmazonClientException {', nan]\n",
      "['Path parent = path.getParent();', nan]\n",
      "['-    if (parent != null) {', nan]\n",
      "['+    if (parent != null && !parent.isRoot()) {', nan]\n",
      "['createFakeDirectoryIfNecessary(parent);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['@@ -2618,7 +2662,7 @@ void maybeCreateFakeParentDirectory(Path path)', nan]\n",
      "['* @throws IOException due to an IO problem.', nan]\n",
      "['* @throws AmazonClientException on failures inside the AWS SDK', nan]\n",
      "['*/', nan]\n",
      "['-  public FileStatus[] innerListStatus(Path f) throws FileNotFoundException,', nan]\n",
      "['+  private S3AFileStatus[] innerListStatus(Path f) throws FileNotFoundException,', nan]\n",
      "['IOException, AmazonClientException {', nan]\n",
      "['Path path = qualify(f);', nan]\n",
      "['String key = pathToKey(path);', nan]\n",
      "['@@ -2626,7 +2670,8 @@ void maybeCreateFakeParentDirectory(Path path)', nan]\n",
      "['entryPoint(INVOCATION_LIST_STATUS);', nan]\n",
      "[nan, nan]\n",
      "['List<S3AFileStatus> result;', nan]\n",
      "['-    final FileStatus fileStatus =  getFileStatus(path);', nan]\n",
      "['+    final S3AFileStatus fileStatus = innerGetFileStatus(path, false,', nan]\n",
      "['+        StatusProbeEnum.ALL);', nan]\n",
      "[nan, nan]\n",
      "['if (fileStatus.isDirectory()) {', nan]\n",
      "['if (!key.isEmpty()) {', nan]\n",
      "['@@ -2658,7 +2703,7 @@ void maybeCreateFakeParentDirectory(Path path)', nan]\n",
      "['allowAuthoritative, ttlTimeProvider);', nan]\n",
      "['} else {', nan]\n",
      "['LOG.debug(\"Adding: rd (not a dir): {}\", path);', nan]\n",
      "['-      FileStatus[] stats = new FileStatus[1];', nan]\n",
      "['+      S3AFileStatus[] stats = new S3AFileStatus[1];', nan]\n",
      "['stats[0]= fileStatus;', nan]\n",
      "['return stats;', nan]\n",
      "['}', nan]\n",
      "['@@ -2769,9 +2814,10 @@ public UserGroupInformation getOwner() {', nan]\n",
      "['public boolean mkdirs(Path path, FsPermission permission) throws IOException,', nan]\n",
      "['FileAlreadyExistsException {', nan]\n",
      "['try {', nan]\n",
      "['+      entryPoint(INVOCATION_MKDIRS);', nan]\n",
      "['return innerMkdirs(path, permission);', nan]\n",
      "['} catch (AmazonClientException e) {', nan]\n",
      "['-      throw translateException(\"innerMkdirs\", path, e);', nan]\n",
      "['+      throw translateException(\"mkdirs\", path, e);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@@ -2791,11 +2837,15 @@ private boolean innerMkdirs(Path p, FsPermission permission)', nan]\n",
      "['throws IOException, FileAlreadyExistsException, AmazonClientException {', nan]\n",
      "['Path f = qualify(p);', nan]\n",
      "['LOG.debug(\"Making directory: {}\", f);', nan]\n",
      "['-    entryPoint(INVOCATION_MKDIRS);', nan]\n",
      "['+    if (p.isRoot()) {', nan]\n",
      "['+      // fast exit for root.', nan]\n",
      "['+      return true;', nan]\n",
      "['+    }', nan]\n",
      "['FileStatus fileStatus;', nan]\n",
      "[nan, nan]\n",
      "['try {', nan]\n",
      "['-      fileStatus = getFileStatus(f);', nan]\n",
      "['+      fileStatus = innerGetFileStatus(f, false,', nan]\n",
      "['+          StatusProbeEnum.ALL);', nan]\n",
      "[nan, nan]\n",
      "['if (fileStatus.isDirectory()) {', nan]\n",
      "['return true;', nan]\n",
      "['@@ -2805,7 +2855,7 @@ private boolean innerMkdirs(Path p, FsPermission permission)', nan]\n",
      "['} catch (FileNotFoundException e) {', nan]\n",
      "['// Walk path to root, ensuring closest ancestor is a directory, not file', nan]\n",
      "['Path fPart = f.getParent();', nan]\n",
      "['-      while (fPart != null) {', nan]\n",
      "['+      while (fPart != null && !fPart.isRoot()) {', nan]\n",
      "['try {', nan]\n",
      "['fileStatus = getFileStatus(fPart);', nan]\n",
      "['if (fileStatus.isDirectory()) {', nan]\n",
      "['@@ -2866,7 +2916,8 @@ S3AFileStatus innerGetFileStatus(final Path f,', nan]\n",
      "['final Set<StatusProbeEnum> probes) throws IOException {', nan]\n",
      "['final Path path = qualify(f);', nan]\n",
      "['String key = pathToKey(path);', nan]\n",
      "['-    LOG.debug(\"Getting path status for {}  ({})\", path, key);', nan]\n",
      "['+    LOG.debug(\"Getting path status for {}  ({}); needEmptyDirectory={}\",', nan]\n",
      "['+        path, key, needEmptyDirectoryFlag);', nan]\n",
      "[nan, nan]\n",
      "['boolean allowAuthoritative = allowAuthoritative(path);', nan]\n",
      "['// Check MetadataStore, if any.', nan]\n",
      "['@@ -2877,9 +2928,10 @@ S3AFileStatus innerGetFileStatus(final Path f,', nan]\n",
      "['}', nan]\n",
      "['Set<Path> tombstones = Collections.emptySet();', nan]\n",
      "['if (pm != null) {', nan]\n",
      "['+      S3AFileStatus msStatus = pm.getFileStatus();', nan]\n",
      "['if (pm.isDeleted()) {', nan]\n",
      "['OffsetDateTime deletedAt = OffsetDateTime.ofInstant(', nan]\n",
      "['-            Instant.ofEpochMilli(pm.getFileStatus().getModificationTime()),', nan]\n",
      "['+            Instant.ofEpochMilli(msStatus.getModificationTime()),', nan]\n",
      "['ZoneOffset.UTC);', nan]\n",
      "['throw new FileNotFoundException(\"Path \" + path + \" is recorded as \" +', nan]\n",
      "['\"deleted by S3Guard at \" + deletedAt);', nan]\n",
      "['@@ -2890,72 +2942,114 @@ S3AFileStatus innerGetFileStatus(final Path f,', nan]\n",
      "['// Skip going to s3 if the file checked is a directory. Because if the', nan]\n",
      "[\"// dest is also a directory, there's no difference.\", nan]\n",
      "[nan, nan]\n",
      "['-      if (!pm.getFileStatus().isDirectory() &&', nan]\n",
      "['+      if (!msStatus.isDirectory() &&', nan]\n",
      "['!allowAuthoritative &&', nan]\n",
      "['probes.contains(StatusProbeEnum.Head)) {', nan]\n",
      "['// a file has been found in a non-auth path and the caller has not said', nan]\n",
      "['// they only care about directories', nan]\n",
      "['LOG.debug(\"Metadata for {} found in the non-auth metastore.\", path);', nan]\n",
      "['-        final long msModTime = pm.getFileStatus().getModificationTime();', nan]\n",
      "['-', nan]\n",
      "['-        S3AFileStatus s3AFileStatus;', nan]\n",
      "['-        try {', nan]\n",
      "['-          s3AFileStatus = s3GetFileStatus(path, key, probes, tombstones);', nan]\n",
      "['-        } catch (FileNotFoundException fne) {', nan]\n",
      "['-          s3AFileStatus = null;', nan]\n",
      "['-        }', nan]\n",
      "['-        if (s3AFileStatus == null) {', nan]\n",
      "['-          LOG.warn(\"Failed to find file {}. Either it is not yet visible, or \"', nan]\n",
      "['-              + \"it has been deleted.\", path);', nan]\n",
      "['-        } else {', nan]\n",
      "['-          final long s3ModTime = s3AFileStatus.getModificationTime();', nan]\n",
      "['-', nan]\n",
      "['-          if(s3ModTime > msModTime) {', nan]\n",
      "['-            LOG.debug(\"S3Guard metadata for {} is outdated;\"', nan]\n",
      "['-                + \" s3modtime={}; msModTime={} updating metastore\",', nan]\n",
      "['-                path, s3ModTime, msModTime);', nan]\n",
      "['-            return S3Guard.putAndReturn(metadataStore, s3AFileStatus,', nan]\n",
      "['-                ttlTimeProvider);', nan]\n",
      "['+        // If the timestamp of the pm is close to \"now\", we don\\'t need to', nan]\n",
      "['+        // bother with a check of S3. that means:', nan]\n",
      "['+        // one of : status modtime is close to now,', nan]\n",
      "['+        //  or pm.getLastUpdated() == now', nan]\n",
      "['+', nan]\n",
      "['+        // get the time in which a status modtime is considered valid', nan]\n",
      "['+        // in a non-auth metastore', nan]\n",
      "['+        long validTime =', nan]\n",
      "['+            ttlTimeProvider.getNow() - ttlTimeProvider.getMetadataTtl();', nan]\n",
      "['+        final long msModTime = msStatus.getModificationTime();', nan]\n",
      "['+', nan]\n",
      "['+        if (msModTime < validTime) {', nan]\n",
      "['+          LOG.debug(\"Metastore entry of {} is out of date, probing S3\", path);', nan]\n",
      "['+          try {', nan]\n",
      "['+            S3AFileStatus s3AFileStatus = s3GetFileStatus(path,', nan]\n",
      "['+                key,', nan]\n",
      "['+                probes,', nan]\n",
      "['+                tombstones,', nan]\n",
      "['+                needEmptyDirectoryFlag);', nan]\n",
      "['+            // if the new status is more current than that in the metastore,', nan]\n",
      "['+            // it means S3 has changed and the store needs updating', nan]\n",
      "['+            final long s3ModTime = s3AFileStatus.getModificationTime();', nan]\n",
      "['+', nan]\n",
      "['+            if (s3ModTime > msModTime) {', nan]\n",
      "[\"+              // there's new data in S3\", nan]\n",
      "['+              LOG.debug(\"S3Guard metadata for {} is outdated;\"', nan]\n",
      "['+                      + \" s3modtime={}; msModTime={} updating metastore\",', nan]\n",
      "['+                  path, s3ModTime, msModTime);', nan]\n",
      "['+              // add to S3Guard', nan]\n",
      "['+              S3Guard.putAndReturn(metadataStore, s3AFileStatus,', nan]\n",
      "['+                  ttlTimeProvider);', nan]\n",
      "['+            } else {', nan]\n",
      "['+              // the modtime of the data is the same as/older than the s3guard', nan]\n",
      "['+              // value either an old object has been found, or the existing one', nan]\n",
      "['+              // was retrieved in both cases -refresh the S3Guard entry so the', nan]\n",
      "[\"+              // record's TTL is updated.\", nan]\n",
      "['+              S3Guard.refreshEntry(metadataStore, pm, s3AFileStatus,', nan]\n",
      "['+                  ttlTimeProvider);', nan]\n",
      "['+            }', nan]\n",
      "['+            // return the value', nan]\n",
      "['+            // note that the checks for empty dir status below can be skipped', nan]\n",
      "['+            // because the call to s3GetFileStatus include the checks there', nan]\n",
      "['+            return s3AFileStatus;', nan]\n",
      "['+          } catch (FileNotFoundException fne) {', nan]\n",
      "['+            // the attempt to refresh the record failed because there was', nan]\n",
      "['+            // no entry. Either it is a new file not visible, or it', nan]\n",
      "['+            // has been deleted (and therefore S3Guard is out of sync with S3)', nan]\n",
      "['+            LOG.warn(\"Failed to find file {}. Either it is not yet visible, or \"', nan]\n",
      "['+                + \"it has been deleted.\", path);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-      S3AFileStatus msStatus = pm.getFileStatus();', nan]\n",
      "['if (needEmptyDirectoryFlag && msStatus.isDirectory()) {', nan]\n",
      "['+        // the caller needs to know if a directory is empty,', nan]\n",
      "['+        // and that this is a directory.', nan]\n",
      "['if (pm.isEmptyDirectory() != Tristate.UNKNOWN) {', nan]\n",
      "['// We have a definitive true / false from MetadataStore, we are done.', nan]\n",
      "['return msStatus;', nan]\n",
      "['} else {', nan]\n",
      "['+          // execute a S3Guard listChildren command to list tombstones under the', nan]\n",
      "['+          // path.', nan]\n",
      "['+          // This list will be used in the forthcoming s3GetFileStatus call.', nan]\n",
      "['DirListingMetadata children =', nan]\n",
      "['S3Guard.listChildrenWithTtl(metadataStore, path, ttlTimeProvider,', nan]\n",
      "['allowAuthoritative);', nan]\n",
      "['if (children != null) {', nan]\n",
      "['tombstones = children.listTombstones();', nan]\n",
      "['}', nan]\n",
      "['-          LOG.debug(\"MetadataStore doesn\\'t know if dir is empty, using S3.\");', nan]\n",
      "['+          LOG.debug(\"MetadataStore doesn\\'t know if {} is empty, using S3.\",', nan]\n",
      "['+              path);', nan]\n",
      "['}', nan]\n",
      "['} else {', nan]\n",
      "[\"// Either this is not a directory, or we don't care if it is empty\", nan]\n",
      "['return msStatus;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "[\"-      // If the metadata store has no children for it and it's not listed in\", nan]\n",
      "[\"-      // S3 yet, we'll assume the empty directory is true;\", nan]\n",
      "['-      S3AFileStatus s3FileStatus;', nan]\n",
      "['+      // now issue the S3 getFileStatus call.', nan]\n",
      "['try {', nan]\n",
      "['-        s3FileStatus = s3GetFileStatus(path, key, probes, tombstones);', nan]\n",
      "['+        S3AFileStatus s3FileStatus = s3GetFileStatus(path,', nan]\n",
      "['+            key,', nan]\n",
      "['+            probes,', nan]\n",
      "['+            tombstones,', nan]\n",
      "['+            true);', nan]\n",
      "['+        // entry was found, so save in S3Guard and return the final value.', nan]\n",
      "['+        return S3Guard.putAndReturn(metadataStore, s3FileStatus,', nan]\n",
      "['+            ttlTimeProvider);', nan]\n",
      "['} catch (FileNotFoundException e) {', nan]\n",
      "[\"+        // If the metadata store has no children for it and it's not listed in\", nan]\n",
      "[\"+        // S3 yet, we'll conclude that it is an empty directory\", nan]\n",
      "['return S3AFileStatus.fromFileStatus(msStatus, Tristate.TRUE,', nan]\n",
      "['null, null);', nan]\n",
      "['}', nan]\n",
      "['-      // entry was found, save in S3Guard', nan]\n",
      "['-      return S3Guard.putAndReturn(metadataStore, s3FileStatus,', nan]\n",
      "['-          ttlTimeProvider);', nan]\n",
      "['} else {', nan]\n",
      "['// there was no entry in S3Guard', nan]\n",
      "['// retrieve the data and update the metadata store in the process.', nan]\n",
      "['return S3Guard.putAndReturn(metadataStore,', nan]\n",
      "['-          s3GetFileStatus(path, key, probes, tombstones),', nan]\n",
      "['+          s3GetFileStatus(path,', nan]\n",
      "['+              key,', nan]\n",
      "['+              probes,', nan]\n",
      "['+              tombstones,', nan]\n",
      "['+              needEmptyDirectoryFlag),', nan]\n",
      "['ttlTimeProvider);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['@@ -3010,6 +3104,8 @@ S3AFileStatus innerGetFileStatus(final Path f,', nan]\n",
      "['* @param key  Key string for the path', nan]\n",
      "['* @param probes probes to make', nan]\n",
      "['* @param tombstones tombstones to filter', nan]\n",
      "['+   * @param needEmptyDirectoryFlag if true, implementation will calculate', nan]\n",
      "['+   *        a TRUE or FALSE value for {@link S3AFileStatus#isEmptyDirectory()}', nan]\n",
      "['* @return Status', nan]\n",
      "['* @throws FileNotFoundException the supplied probes failed.', nan]\n",
      "['* @throws IOException on other problems.', nan]\n",
      "['@@ -3019,88 +3115,88 @@ S3AFileStatus innerGetFileStatus(final Path f,', nan]\n",
      "['S3AFileStatus s3GetFileStatus(final Path path,', nan]\n",
      "['final String key,', nan]\n",
      "['final Set<StatusProbeEnum> probes,', nan]\n",
      "['-      @Nullable Set<Path> tombstones) throws IOException {', nan]\n",
      "['-    if (!key.isEmpty()) {', nan]\n",
      "['-      if (probes.contains(StatusProbeEnum.Head) && !key.endsWith(\"/\")) {', nan]\n",
      "['-        try {', nan]\n",
      "['-          // look for the simple file', nan]\n",
      "['-          ObjectMetadata meta = getObjectMetadata(key);', nan]\n",
      "['-          LOG.debug(\"Found exact file: normal file {}\", key);', nan]\n",
      "['-          return new S3AFileStatus(meta.getContentLength(),', nan]\n",
      "['-              dateToLong(meta.getLastModified()),', nan]\n",
      "['-              path,', nan]\n",
      "['-              getDefaultBlockSize(path),', nan]\n",
      "['-              username,', nan]\n",
      "['-              meta.getETag(),', nan]\n",
      "['-              meta.getVersionId());', nan]\n",
      "['-        } catch (AmazonServiceException e) {', nan]\n",
      "['-          // if the response is a 404 error, it just means that there is', nan]\n",
      "['-          // no file at that path...the remaining checks will be needed.', nan]\n",
      "['-          if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {', nan]\n",
      "['-            throw translateException(\"getFileStatus\", path, e);', nan]\n",
      "['-          }', nan]\n",
      "['-        } catch (AmazonClientException e) {', nan]\n",
      "['+      @Nullable final Set<Path> tombstones,', nan]\n",
      "['+      final boolean needEmptyDirectoryFlag) throws IOException {', nan]\n",
      "['+    LOG.debug(\"S3GetFileStatus {}\", path);', nan]\n",
      "[\"+    // either you aren't looking for the directory flag, or you are,\", nan]\n",
      "['+    // and if you are, the probe list must contain list.', nan]\n",
      "['+    Preconditions.checkArgument(!needEmptyDirectoryFlag', nan]\n",
      "['+        || probes.contains(StatusProbeEnum.List),', nan]\n",
      "['+        \"s3GetFileStatus(%s) wants to know if a directory is empty but\"', nan]\n",
      "['+            + \" does not request a list probe\", path);', nan]\n",
      "['+', nan]\n",
      "['+    if (!key.isEmpty() && !key.endsWith(\"/\")', nan]\n",
      "['+        && probes.contains(StatusProbeEnum.Head)) {', nan]\n",
      "['+      try {', nan]\n",
      "['+        // look for the simple file', nan]\n",
      "['+        ObjectMetadata meta = getObjectMetadata(key);', nan]\n",
      "['+        LOG.debug(\"Found exact file: normal file {}\", key);', nan]\n",
      "['+        return new S3AFileStatus(meta.getContentLength(),', nan]\n",
      "['+            dateToLong(meta.getLastModified()),', nan]\n",
      "['+            path,', nan]\n",
      "['+            getDefaultBlockSize(path),', nan]\n",
      "['+            username,', nan]\n",
      "['+            meta.getETag(),', nan]\n",
      "['+            meta.getVersionId());', nan]\n",
      "['+      } catch (AmazonServiceException e) {', nan]\n",
      "['+        // if the response is a 404 error, it just means that there is', nan]\n",
      "['+        // no file at that path...the remaining checks will be needed.', nan]\n",
      "['+        if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {', nan]\n",
      "['throw translateException(\"getFileStatus\", path, e);', nan]\n",
      "['}', nan]\n",
      "['-      }', nan]\n",
      "['-', nan]\n",
      "['-      // Either a normal file was not found or the probe was skipped.', nan]\n",
      "['-      // because the key ended in \"/\" or it was not in the set of probes.', nan]\n",
      "['-      // Look for the dir marker', nan]\n",
      "['-      if (probes.contains(StatusProbeEnum.DirMarker)) {', nan]\n",
      "['-        String newKey = maybeAddTrailingSlash(key);', nan]\n",
      "['-        try {', nan]\n",
      "['-          ObjectMetadata meta = getObjectMetadata(newKey);', nan]\n",
      "['-', nan]\n",
      "['-          if (objectRepresentsDirectory(newKey, meta.getContentLength())) {', nan]\n",
      "['-            LOG.debug(\"Found file (with /): fake directory\");', nan]\n",
      "['-            return new S3AFileStatus(Tristate.TRUE, path, username);', nan]\n",
      "['-          } else {', nan]\n",
      "['-            LOG.warn(\"Found file (with /): real file? should not happen: {}\",', nan]\n",
      "['-                key);', nan]\n",
      "['-', nan]\n",
      "['-            return new S3AFileStatus(meta.getContentLength(),', nan]\n",
      "['-                    dateToLong(meta.getLastModified()),', nan]\n",
      "['-                    path,', nan]\n",
      "['-                    getDefaultBlockSize(path),', nan]\n",
      "['-                    username,', nan]\n",
      "['-                    meta.getETag(),', nan]\n",
      "['-                    meta.getVersionId());', nan]\n",
      "['-          }', nan]\n",
      "['-        } catch (AmazonServiceException e) {', nan]\n",
      "['-          if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {', nan]\n",
      "['-            throw translateException(\"getFileStatus\", newKey, e);', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-          }', nan]\n",
      "['-        } catch (AmazonClientException e) {', nan]\n",
      "['-          throw translateException(\"getFileStatus\", newKey, e);', nan]\n",
      "['-        }', nan]\n",
      "['+      } catch (AmazonClientException e) {', nan]\n",
      "['+        throw translateException(\"getFileStatus\", path, e);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['// execute the list', nan]\n",
      "['if (probes.contains(StatusProbeEnum.List)) {', nan]\n",
      "['try {', nan]\n",
      "['+        // this will find a marker dir / as well as an entry.', nan]\n",
      "['+        // When making a simple \"is this a dir check\" all is good.', nan]\n",
      "['+        // but when looking for an empty dir, we need to verify there are no', nan]\n",
      "['+        // children, so ask for two entries, so as to find', nan]\n",
      "['+        // a child', nan]\n",
      "['String dirKey = maybeAddTrailingSlash(key);', nan]\n",
      "['-        S3ListRequest request = createListObjectsRequest(dirKey, \"/\", 1);', nan]\n",
      "['+        // list size is dir marker + at least one non-tombstone entry', nan]\n",
      "[\"+        // there's a corner case: more tombstones than you have in a\", nan]\n",
      "['+        // single page list. We assume that if you have been deleting', nan]\n",
      "['+        // that many files, then the AWS listing will have purged some', nan]\n",
      "['+        // by the time of listing so that the response includes some', nan]\n",
      "['+        // which have not.', nan]\n",
      "['+', nan]\n",
      "['+        int listSize;', nan]\n",
      "['+        if (tombstones == null) {', nan]\n",
      "['+          // no tombstones so look for a marker and at least one child.', nan]\n",
      "['+          listSize = 2;', nan]\n",
      "['+        } else {', nan]\n",
      "['+          // build a listing > tombstones. If the caller has many thousands', nan]\n",
      "[\"+          // of tombstones this won't work properly, which is why pruning\", nan]\n",
      "['+          // of expired tombstones matters.', nan]\n",
      "['+          listSize = Math.min(2 + tombstones.size(), Math.max(2, maxKeys));', nan]\n",
      "['+        }', nan]\n",
      "['+        S3ListRequest request = createListObjectsRequest(dirKey, \"/\",', nan]\n",
      "['+            listSize);', nan]\n",
      "['+        // execute the request', nan]\n",
      "['+        S3ListResult listResult = listObjects(request);', nan]\n",
      "[nan, nan]\n",
      "['-        S3ListResult objects = listObjects(request);', nan]\n",
      "[nan, nan]\n",
      "['-        Collection<String> prefixes = objects.getCommonPrefixes();', nan]\n",
      "['-        Collection<S3ObjectSummary> summaries = objects.getObjectSummaries();', nan]\n",
      "['-        if (!isEmptyOfKeys(prefixes, tombstones) ||', nan]\n",
      "['-            !isEmptyOfObjects(summaries, tombstones)) {', nan]\n",
      "['+        if (listResult.hasPrefixesOrObjects(contextAccessors, tombstones)) {', nan]\n",
      "['if (LOG.isDebugEnabled()) {', nan]\n",
      "['-            LOG.debug(\"Found path as directory (with /): {}/{}\",', nan]\n",
      "['-                prefixes.size(), summaries.size());', nan]\n",
      "['-', nan]\n",
      "['-            for (S3ObjectSummary summary : summaries) {', nan]\n",
      "['-              LOG.debug(\"Summary: {} {}\", summary.getKey(), summary.getSize());', nan]\n",
      "['-            }', nan]\n",
      "['-            for (String prefix : prefixes) {', nan]\n",
      "['-              LOG.debug(\"Prefix: {}\", prefix);', nan]\n",
      "['-            }', nan]\n",
      "['+            LOG.debug(\"Found path as directory (with /)\");', nan]\n",
      "['+            listResult.logAtDebug(LOG);', nan]\n",
      "['}', nan]\n",
      "['-', nan]\n",
      "['+          // At least one entry has been found.', nan]\n",
      "['+          // If looking for an empty directory, the marker must exist but no', nan]\n",
      "['+          // children.', nan]\n",
      "['+          // So the listing must contain the marker entry only.', nan]\n",
      "['+          if (needEmptyDirectoryFlag', nan]\n",
      "['+              && listResult.representsEmptyDirectory(', nan]\n",
      "['+                  contextAccessors, dirKey, tombstones)) {', nan]\n",
      "['+            return new S3AFileStatus(Tristate.TRUE, path, username);', nan]\n",
      "['+          }', nan]\n",
      "['+          // either an empty directory is not needed, or the', nan]\n",
      "['+          // listing does not meet the requirements.', nan]\n",
      "['return new S3AFileStatus(Tristate.FALSE, path, username);', nan]\n",
      "['} else if (key.isEmpty()) {', nan]\n",
      "['LOG.debug(\"Found root directory\");', nan]\n",
      "['@@ -3119,48 +3215,6 @@ S3AFileStatus s3GetFileStatus(final Path path,', nan]\n",
      "['throw new FileNotFoundException(\"No such file or directory: \" + path);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-  /**', nan]\n",
      "['-   * Helper function to determine if a collection of paths is empty', nan]\n",
      "['-   * after accounting for tombstone markers (if provided).', nan]\n",
      "['-   * @param keys Collection of path (prefixes / directories or keys).', nan]\n",
      "['-   * @param tombstones Set of tombstone markers, or null if not applicable.', nan]\n",
      "['-   * @return false if summaries contains objects not accounted for by', nan]\n",
      "['-   * tombstones.', nan]\n",
      "['-   */', nan]\n",
      "['-  private boolean isEmptyOfKeys(Collection<String> keys, Set<Path>', nan]\n",
      "['-      tombstones) {', nan]\n",
      "['-    if (tombstones == null) {', nan]\n",
      "['-      return keys.isEmpty();', nan]\n",
      "['-    }', nan]\n",
      "['-    for (String key : keys) {', nan]\n",
      "['-      Path qualified = keyToQualifiedPath(key);', nan]\n",
      "['-      if (!tombstones.contains(qualified)) {', nan]\n",
      "['-        return false;', nan]\n",
      "['-      }', nan]\n",
      "['-    }', nan]\n",
      "['-    return true;', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-  /**', nan]\n",
      "['-   * Helper function to determine if a collection of object summaries is empty', nan]\n",
      "['-   * after accounting for tombstone markers (if provided).', nan]\n",
      "['-   * @param summaries Collection of objects as returned by listObjects.', nan]\n",
      "['-   * @param tombstones Set of tombstone markers, or null if not applicable.', nan]\n",
      "['-   * @return false if summaries contains objects not accounted for by', nan]\n",
      "['-   * tombstones.', nan]\n",
      "['-   */', nan]\n",
      "['-  private boolean isEmptyOfObjects(Collection<S3ObjectSummary> summaries,', nan]\n",
      "['-      Set<Path> tombstones) {', nan]\n",
      "['-    if (tombstones == null) {', nan]\n",
      "['-      return summaries.isEmpty();', nan]\n",
      "['-    }', nan]\n",
      "['-    Collection<String> stringCollection = new ArrayList<>(summaries.size());', nan]\n",
      "['-    for (S3ObjectSummary summary : summaries) {', nan]\n",
      "['-      stringCollection.add(summary.getKey());', nan]\n",
      "['-    }', nan]\n",
      "['-    return isEmptyOfKeys(stringCollection, tombstones);', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['/**', nan]\n",
      "['* Raw version of {@link FileSystem#exists(Path)} which uses S3 only:', nan]\n",
      "['* S3Guard MetadataStore, if any, will be skipped.', nan]\n",
      "['@@ -3175,7 +3229,7 @@ private boolean s3Exists(final Path path, final Set<StatusProbeEnum> probes)', nan]\n",
      "['throws IOException {', nan]\n",
      "['String key = pathToKey(path);', nan]\n",
      "['try {', nan]\n",
      "['-      s3GetFileStatus(path, key, probes, null);', nan]\n",
      "['+      s3GetFileStatus(path, key, probes, null, false);', nan]\n",
      "['return true;', nan]\n",
      "['} catch (FileNotFoundException e) {', nan]\n",
      "['return false;', nan]\n",
      "['@@ -3578,6 +3632,7 @@ private CopyResult copyFile(String srcKey, String dstKey, long size,', nan]\n",
      "['copyObjectRequest.setNewObjectMetadata(dstom);', nan]\n",
      "['Optional.ofNullable(srcom.getStorageClass())', nan]\n",
      "['.ifPresent(copyObjectRequest::setStorageClass);', nan]\n",
      "['+          incrementStatistic(OBJECT_COPY_REQUESTS);', nan]\n",
      "['Copy copy = transfers.copy(copyObjectRequest);', nan]\n",
      "['copy.addProgressListener(progressListener);', nan]\n",
      "['CopyOutcome copyOutcome = CopyOutcome.waitForCopy(copy);', nan]\n",
      "['@@ -3711,16 +3766,21 @@ private void setOptionalObjectMetadata(ObjectMetadata metadata) {', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* Perform post-write actions.', nan]\n",
      "['-   * Calls {@link #deleteUnnecessaryFakeDirectories(Path)} and then', nan]\n",
      "['-   * updates any metastore.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['* This operation MUST be called after any PUT/multipart PUT completes', nan]\n",
      "['* successfully.', nan]\n",
      "['-   *', nan]\n",
      "['-   * The operations actions include', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * The actions include:', nan]\n",
      "['* <ol>', nan]\n",
      "['-   *   <li>Calling {@link #deleteUnnecessaryFakeDirectories(Path)}</li>', nan]\n",
      "['-   *   <li>Updating any metadata store with details on the newly created', nan]\n",
      "['-   *   object.</li>', nan]\n",
      "['+   *   <li>', nan]\n",
      "['+   *     Calling', nan]\n",
      "['+   *     {@link #deleteUnnecessaryFakeDirectories(Path, BulkOperationState)}', nan]\n",
      "['+   *     if directory markers are not being retained.', nan]\n",
      "['+   *   </li>', nan]\n",
      "['+   *   <li>', nan]\n",
      "['+   *     Updating any metadata store with details on the newly created', nan]\n",
      "['+   *     object.', nan]\n",
      "['+   *     </li>', nan]\n",
      "['* </ol>', nan]\n",
      "['* @param key key written to', nan]\n",
      "['* @param length  total length of file written', nan]\n",
      "['@@ -3743,12 +3803,19 @@ void finishedWrite(String key, long length, String eTag, String versionId,', nan]\n",
      "['Preconditions.checkArgument(length >= 0, \"content length is negative\");', nan]\n",
      "['final boolean isDir = objectRepresentsDirectory(key, length);', nan]\n",
      "['// kick off an async delete', nan]\n",
      "['-    final CompletableFuture<?> deletion = submit(', nan]\n",
      "['-        unboundedThreadPool,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          deleteUnnecessaryFakeDirectories(p.getParent());', nan]\n",
      "['-          return null;', nan]\n",
      "['-        });', nan]\n",
      "['+    CompletableFuture<?> deletion;', nan]\n",
      "['+    if (!keepDirectoryMarkers(p)) {', nan]\n",
      "['+      deletion = submit(', nan]\n",
      "['+          unboundedThreadPool,', nan]\n",
      "['+          () -> {', nan]\n",
      "['+            deleteUnnecessaryFakeDirectories(', nan]\n",
      "['+                p.getParent(),', nan]\n",
      "['+                operationState);', nan]\n",
      "['+            return null;', nan]\n",
      "['+          });', nan]\n",
      "['+    } else {', nan]\n",
      "['+      deletion = null;', nan]\n",
      "['+    }', nan]\n",
      "['// this is only set if there is a metastore to update and the', nan]\n",
      "['// operationState parameter passed in was null.', nan]\n",
      "['BulkOperationState stateToClose = null;', nan]\n",
      "['@@ -3807,13 +3874,26 @@ void finishedWrite(String key, long length, String eTag, String versionId,', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Should we keep directory markers under the path being created', nan]\n",
      "['+   * by mkdir/file creation/rename?', nan]\n",
      "['+   * @param path path to probe', nan]\n",
      "['+   * @return true if the markers MAY be retained,', nan]\n",
      "['+   * false if they MUST be deleted', nan]\n",
      "['+   */', nan]\n",
      "['+  private boolean keepDirectoryMarkers(Path path) {', nan]\n",
      "['+    return directoryPolicy.keepDirectoryMarkers(path);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Delete mock parent directories which are no longer needed.', nan]\n",
      "['* Retry policy: retrying; exceptions swallowed.', nan]\n",
      "['* @param path path', nan]\n",
      "['+   * @param operationState (nullable) operational state for a bulk update', nan]\n",
      "['*/', nan]\n",
      "['@Retries.RetryExceptionsSwallowed', nan]\n",
      "['-  private void deleteUnnecessaryFakeDirectories(Path path) {', nan]\n",
      "['+  private void deleteUnnecessaryFakeDirectories(Path path,', nan]\n",
      "['+      final BulkOperationState operationState) {', nan]\n",
      "['List<DeleteObjectsRequest.KeyVersion> keysToRemove = new ArrayList<>();', nan]\n",
      "['while (!path.isRoot()) {', nan]\n",
      "['String key = pathToKey(path);', nan]\n",
      "['@@ -3823,7 +3903,7 @@ private void deleteUnnecessaryFakeDirectories(Path path) {', nan]\n",
      "['path = path.getParent();', nan]\n",
      "['}', nan]\n",
      "['try {', nan]\n",
      "['-      removeKeys(keysToRemove, true, null);', nan]\n",
      "['+      removeKeys(keysToRemove, true, operationState);', nan]\n",
      "['} catch(AmazonClientException | IOException e) {', nan]\n",
      "['instrumentation.errorIgnored();', nan]\n",
      "['if (LOG.isDebugEnabled()) {', nan]\n",
      "['@@ -3952,6 +4032,14 @@ public long getDefaultBlockSize() {', nan]\n",
      "['return getConf().getLongBytes(FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the directory marker policy of this filesystem.', nan]\n",
      "['+   * @return the marker policy.', nan]\n",
      "['+   */', nan]\n",
      "['+  public DirectoryPolicy getDirectoryMarkerPolicy() {', nan]\n",
      "['+    return directoryPolicy;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['@Override', nan]\n",
      "['public String toString() {', nan]\n",
      "['final StringBuilder sb = new StringBuilder(', nan]\n",
      "['@@ -3990,6 +4078,7 @@ public String toString() {', nan]\n",
      "['sb.append(\", credentials=\").append(credentials);', nan]\n",
      "['sb.append(\", delegation tokens=\")', nan]\n",
      "['.append(delegationTokens.map(Objects::toString).orElse(\"disabled\"));', nan]\n",
      "['+    sb.append(\", \").append(directoryPolicy);', nan]\n",
      "['sb.append(\", statistics {\")', nan]\n",
      "['.append(statistics)', nan]\n",
      "['.append(\"}\");', nan]\n",
      "['@@ -4086,25 +4175,41 @@ public boolean exists(Path f) throws IOException {', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['-   * Override superclass so as to add statistic collection.', nan]\n",
      "['+   * Optimized probe for a path referencing a dir.', nan]\n",
      "['+   * Even though it is optimized to a single HEAD, applications', nan]\n",
      "['+   * should not over-use this method...it is all too common.', nan]\n",
      "['* {@inheritDoc}', nan]\n",
      "['*/', nan]\n",
      "['@Override', nan]\n",
      "['@SuppressWarnings(\"deprecation\")', nan]\n",
      "['public boolean isDirectory(Path f) throws IOException {', nan]\n",
      "['entryPoint(INVOCATION_IS_DIRECTORY);', nan]\n",
      "['-    return super.isDirectory(f);', nan]\n",
      "['+    try {', nan]\n",
      "['+      return innerGetFileStatus(f, false, StatusProbeEnum.DIRECTORIES)', nan]\n",
      "['+          .isDirectory();', nan]\n",
      "['+    } catch (FileNotFoundException e) {', nan]\n",
      "['+      // not found or it is a file.', nan]\n",
      "['+      return false;', nan]\n",
      "['+    }', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['-   * Override superclass so as to add statistic collection.', nan]\n",
      "['+   * Optimized probe for a path referencing a file.', nan]\n",
      "['+   * Even though it is optimized to a single HEAD, applications', nan]\n",
      "['+   * should not over-use this method...it is all too common.', nan]\n",
      "['* {@inheritDoc}', nan]\n",
      "['*/', nan]\n",
      "['@Override', nan]\n",
      "['@SuppressWarnings(\"deprecation\")', nan]\n",
      "['public boolean isFile(Path f) throws IOException {', nan]\n",
      "['entryPoint(INVOCATION_IS_FILE);', nan]\n",
      "['-    return super.isFile(f);', nan]\n",
      "['+    try {', nan]\n",
      "['+      return innerGetFileStatus(f, false, StatusProbeEnum.HEAD_ONLY)', nan]\n",
      "['+          .isFile();', nan]\n",
      "['+    } catch (FileNotFoundException e) {', nan]\n",
      "['+      // not found or it is a dir.', nan]\n",
      "['+      return false;', nan]\n",
      "['+    }', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -4511,7 +4616,8 @@ void abortMultipartUpload(MultipartUpload upload) {', nan]\n",
      "['public boolean hasPathCapability(final Path path, final String capability)', nan]\n",
      "['throws IOException {', nan]\n",
      "['final Path p = makeQualified(path);', nan]\n",
      "['-    switch (validatePathCapabilityArgs(p, capability)) {', nan]\n",
      "['+    String cap = validatePathCapabilityArgs(p, capability);', nan]\n",
      "['+    switch (cap) {', nan]\n",
      "[nan, nan]\n",
      "['case CommitConstants.STORE_CAPABILITY_MAGIC_COMMITTER:', nan]\n",
      "['case CommitConstants.STORE_CAPABILITY_MAGIC_COMMITTER_OLD:', nan]\n",
      "['@@ -4530,8 +4636,24 @@ public boolean hasPathCapability(final Path path, final String capability)', nan]\n",
      "['case CommonPathCapabilities.FS_MULTIPART_UPLOADER:', nan]\n",
      "['return true;', nan]\n",
      "[nan, nan]\n",
      "['+    // this client is safe to use with buckets', nan]\n",
      "['+    // containing directory markers anywhere in', nan]\n",
      "['+    // the hierarchy', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_AWARE:', nan]\n",
      "['+      return true;', nan]\n",
      "['+', nan]\n",
      "['+    /*', nan]\n",
      "['+     * Marker policy capabilities are handed off.', nan]\n",
      "['+     */', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_KEEP:', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_DELETE:', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_AUTHORITATIVE:', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP:', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE:', nan]\n",
      "['+      return getDirectoryMarkerPolicy().hasPathCapability(path, cap);', nan]\n",
      "['+', nan]\n",
      "['default:', nan]\n",
      "['-      return super.hasPathCapability(p, capability);', nan]\n",
      "['+      return super.hasPathCapability(p, cap);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@@ -4546,7 +4668,7 @@ public boolean hasPathCapability(final Path path, final String capability)', nan]\n",
      "['@Override', nan]\n",
      "['public boolean hasCapability(String capability) {', nan]\n",
      "['try {', nan]\n",
      "['-      return hasPathCapability(workingDir, capability);', nan]\n",
      "['+      return hasPathCapability(new Path(\"/\"), capability);', nan]\n",
      "['} catch (IOException ex) {', nan]\n",
      "['// should never happen, so log and downgrade.', nan]\n",
      "['LOG.debug(\"Ignoring exception on hasCapability({}})\", capability, ex);', nan]\n",
      "['@@ -4800,6 +4922,15 @@ public StoreContext createStoreContext() {', nan]\n",
      "['.build();', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a marker tools operations binding for this store.', nan]\n",
      "['+   * @return callbacks for operations.', nan]\n",
      "['+   */', nan]\n",
      "['+  @InterfaceAudience.Private', nan]\n",
      "['+  public MarkerToolOperations createMarkerToolOperations() {', nan]\n",
      "['+    return new MarkerToolOperationsImpl(operationCallbacks);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* The implementation of context accessors.', nan]\n",
      "['*/', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListRequest.java b/hadoop-too', 's/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListRequest.java']\n",
      "['index 1a0d2c3378c..d51211516f2 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListRequest.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListRequest.java', nan]\n",
      "['@@ -24,11 +24,18 @@', nan]\n",
      "['/**', nan]\n",
      "['* API version-independent container for S3 List requests.', nan]\n",
      "['*/', nan]\n",
      "['-public class S3ListRequest {', nan]\n",
      "['-  private ListObjectsRequest v1Request;', nan]\n",
      "['-  private ListObjectsV2Request v2Request;', nan]\n",
      "['+public final class S3ListRequest {', nan]\n",
      "[nan, nan]\n",
      "['-  protected S3ListRequest(ListObjectsRequest v1, ListObjectsV2Request v2) {', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Format for the toString() method: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final String DESCRIPTION', nan]\n",
      "['+      = \"List %s:/%s delimiter=%s keys=%d requester pays=%s\";', nan]\n",
      "['+', nan]\n",
      "['+  private final ListObjectsRequest v1Request;', nan]\n",
      "['+  private final ListObjectsV2Request v2Request;', nan]\n",
      "['+', nan]\n",
      "['+  private S3ListRequest(ListObjectsRequest v1, ListObjectsV2Request v2) {', nan]\n",
      "['v1Request = v1;', nan]\n",
      "['v2Request = v2;', nan]\n",
      "['}', nan]\n",
      "['@@ -70,11 +77,15 @@ public ListObjectsV2Request getV2() {', nan]\n",
      "['@Override', nan]\n",
      "['public String toString() {', nan]\n",
      "['if (isV1()) {', nan]\n",
      "['-      return String.format(\"List %s:/%s\",', nan]\n",
      "['-          v1Request.getBucketName(), v1Request.getPrefix());', nan]\n",
      "['+      return String.format(DESCRIPTION,', nan]\n",
      "['+          v1Request.getBucketName(), v1Request.getPrefix(),', nan]\n",
      "['+          v1Request.getDelimiter(), v1Request.getMaxKeys(),', nan]\n",
      "['+          v1Request.isRequesterPays());', nan]\n",
      "['} else {', nan]\n",
      "['-      return String.format(\"List %s:/%s\",', nan]\n",
      "['-          v2Request.getBucketName(), v2Request.getPrefix());', nan]\n",
      "['+      return String.format(DESCRIPTION,', nan]\n",
      "['+          v2Request.getBucketName(), v2Request.getPrefix(),', nan]\n",
      "['+          v2Request.getDelimiter(), v2Request.getMaxKeys(),', nan]\n",
      "['+          v2Request.isRequesterPays());', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListResult.java b/hadoop-tool', '/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListResult.java']\n",
      "['index e8aff329070..69794c04db5 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListResult.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListResult.java', nan]\n",
      "['@@ -18,11 +18,18 @@', nan]\n",
      "[nan, nan]\n",
      "['package org.apache.hadoop.fs.s3a;', nan]\n",
      "[nan, nan]\n",
      "['+import java.util.Collection;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+import java.util.Set;', nan]\n",
      "['+import java.util.stream.Collectors;', nan]\n",
      "['+', nan]\n",
      "['import com.amazonaws.services.s3.model.ListObjectsV2Result;', nan]\n",
      "['import com.amazonaws.services.s3.model.ObjectListing;', nan]\n",
      "['import com.amazonaws.services.s3.model.S3ObjectSummary;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "[nan, nan]\n",
      "['-import java.util.List;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.ContextAccessors;', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* API version-independent container for S3 List responses.', nan]\n",
      "['@@ -92,6 +99,110 @@ public boolean isTruncated() {', nan]\n",
      "['} else {', nan]\n",
      "['return v2Result.getCommonPrefixes();', nan]\n",
      "['}', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Is the list of object summaries empty', nan]\n",
      "['+   * after accounting for tombstone markers (if provided)?', nan]\n",
      "['+   * @param accessors callback for key to path mapping.', nan]\n",
      "['+   * @param tombstones Set of tombstone markers, or null if not applicable.', nan]\n",
      "['+   * @return false if summaries contains objects not accounted for by', nan]\n",
      "['+   * tombstones.', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean isEmptyOfObjects(', nan]\n",
      "['+      final ContextAccessors accessors,', nan]\n",
      "['+      final Set<Path> tombstones) {', nan]\n",
      "['+    if (tombstones == null) {', nan]\n",
      "['+      return getObjectSummaries().isEmpty();', nan]\n",
      "['+    }', nan]\n",
      "['+    return isEmptyOfKeys(accessors,', nan]\n",
      "['+        objectSummaryKeys(),', nan]\n",
      "['+        tombstones);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the list of keys in the object summary.', nan]\n",
      "['+   * @return a possibly empty list', nan]\n",
      "['+   */', nan]\n",
      "['+  private List<String> objectSummaryKeys() {', nan]\n",
      "['+    return getObjectSummaries().stream()', nan]\n",
      "['+        .map(S3ObjectSummary::getKey)', nan]\n",
      "['+        .collect(Collectors.toList());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Does this listing have prefixes or objects after entries with', nan]\n",
      "['+   * tombstones have been stripped?', nan]\n",
      "['+   * @param accessors callback for key to path mapping.', nan]\n",
      "['+   * @param tombstones Set of tombstone markers, or null if not applicable.', nan]\n",
      "['+   * @return true if the reconciled list is non-empty', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean hasPrefixesOrObjects(', nan]\n",
      "['+      final ContextAccessors accessors,', nan]\n",
      "['+      final Set<Path> tombstones) {', nan]\n",
      "['+', nan]\n",
      "['+    return !isEmptyOfKeys(accessors, getCommonPrefixes(), tombstones)', nan]\n",
      "['+        || !isEmptyOfObjects(accessors, tombstones);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Helper function to determine if a collection of keys is empty', nan]\n",
      "['+   * after accounting for tombstone markers (if provided).', nan]\n",
      "['+   * @param accessors callback for key to path mapping.', nan]\n",
      "['+   * @param keys Collection of path (prefixes / directories or keys).', nan]\n",
      "['+   * @param tombstones Set of tombstone markers, or null if not applicable.', nan]\n",
      "['+   * @return true if the list is considered empty.', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean isEmptyOfKeys(', nan]\n",
      "['+      final ContextAccessors accessors,', nan]\n",
      "['+      final Collection<String> keys,', nan]\n",
      "['+      final Set<Path> tombstones) {', nan]\n",
      "['+    if (tombstones == null) {', nan]\n",
      "['+      return keys.isEmpty();', nan]\n",
      "['+    }', nan]\n",
      "['+    for (String key : keys) {', nan]\n",
      "['+      Path qualified = accessors.keyToPath(key);', nan]\n",
      "['+      if (!tombstones.contains(qualified)) {', nan]\n",
      "['+        return false;', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+    return true;', nan]\n",
      "['+  }', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Does this listing represent an empty directory?', nan]\n",
      "['+   * @param contextAccessors callback for key to path mapping.', nan]\n",
      "['+   * @param dirKey directory key', nan]\n",
      "['+   * @param tombstones Set of tombstone markers, or null if not applicable.', nan]\n",
      "['+   * @return true if the list is considered empty.', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean representsEmptyDirectory(', nan]\n",
      "['+      final ContextAccessors contextAccessors,', nan]\n",
      "['+      final String dirKey,', nan]\n",
      "['+      final Set<Path> tombstones) {', nan]\n",
      "['+    // If looking for an empty directory, the marker must exist but', nan]\n",
      "['+    // no children.', nan]\n",
      "['+    // So the listing must contain the marker entry only as an object,', nan]\n",
      "['+    // and prefixes is null', nan]\n",
      "['+    List<String> keys = objectSummaryKeys();', nan]\n",
      "['+    return keys.size() == 1 && keys.contains(dirKey)', nan]\n",
      "['+        && getCommonPrefixes().isEmpty();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Dmp the result at debug level.', nan]\n",
      "['+   * @param log log to use', nan]\n",
      "['+   */', nan]\n",
      "['+  public void logAtDebug(Logger log) {', nan]\n",
      "['+    Collection<String> prefixes = getCommonPrefixes();', nan]\n",
      "['+    Collection<S3ObjectSummary> summaries = getObjectSummaries();', nan]\n",
      "['+    log.debug(\"Prefix count = {}; object count={}\",', nan]\n",
      "['+        prefixes.size(), summaries.size());', nan]\n",
      "['+    for (S3ObjectSummary summary : summaries) {', nan]\n",
      "['+      log.debug(\"Summary: {} {}\", summary.getKey(), summary.getSize());', nan]\n",
      "['+    }', nan]\n",
      "['+    for (String prefix : prefixes) {', nan]\n",
      "['+      log.debug(\"Prefix: {}\", prefix);', nan]\n",
      "['+    }', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java b/ha', 'oop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..ca04fed65a5', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java', nan]\n",
      "['@@ -0,0 +1,352 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.impl;', nan]\n",
      "['+', nan]\n",
      "['+import java.util.ArrayList;', nan]\n",
      "['+import java.util.Iterator;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+import java.util.Map;', nan]\n",
      "['+import java.util.TreeMap;', nan]\n",
      "['+', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3ALocatedFileStatus;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Tracks directory markers which have been reported in object listings.', nan]\n",
      "['+ * This is needed for auditing and cleanup, including during rename', nan]\n",
      "['+ * operations.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Designed to be used while scanning through the results of listObject', nan]\n",
      "['+ * calls, where are we assume the results come in alphanumeric sort order', nan]\n",
      "['+ * and parent entries before children.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * This lets as assume that we can identify all leaf markers as those', nan]\n",
      "['+ * markers which were added to set of leaf markers and not subsequently', nan]\n",
      "['+ * removed as a child entries were discovered.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * To avoid scanning datastructures excessively, the path of the parent', nan]\n",
      "['+ * directory of the last file added is cached. This allows for a', nan]\n",
      "['+ * quick bailout when many children of the same directory are', nan]\n",
      "['+ * returned in a listing.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Consult the directory_markers document for details on this feature,', nan]\n",
      "['+ * including terminology.', nan]\n",
      "['+ */', nan]\n",
      "['+public class DirMarkerTracker {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(DirMarkerTracker.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * all leaf markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final Map<Path, Marker> leafMarkers', nan]\n",
      "['+      = new TreeMap<>();', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * all surplus markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final Map<Path, Marker> surplusMarkers', nan]\n",
      "['+      = new TreeMap<>();', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Base path of the tracking operation.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final Path basePath;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Should surplus markers be recorded in', nan]\n",
      "['+   * the {@link #surplusMarkers} map?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean recordSurplusMarkers;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * last parent directory checked.', nan]\n",
      "['+   */', nan]\n",
      "['+  private Path lastDirChecked;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Count of scans; used for test assertions.', nan]\n",
      "['+   */', nan]\n",
      "['+  private int scanCount;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many files were found.', nan]\n",
      "['+   */', nan]\n",
      "['+  private int filesFound;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many markers were found.', nan]\n",
      "['+   */', nan]\n",
      "['+  private int markersFound;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many objects of any kind were found?', nan]\n",
      "['+   */', nan]\n",
      "['+  private int objectsFound;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Construct.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * The base path is currently only used for information rather than', nan]\n",
      "['+   * validating paths supplied in other methods.', nan]\n",
      "['+   * @param basePath base path of track', nan]\n",
      "['+   * @param recordSurplusMarkers save surplus markers to a map?', nan]\n",
      "['+   */', nan]\n",
      "['+  public DirMarkerTracker(final Path basePath,', nan]\n",
      "['+      boolean recordSurplusMarkers) {', nan]\n",
      "['+    this.basePath = basePath;', nan]\n",
      "['+    this.recordSurplusMarkers = recordSurplusMarkers;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the base path of the tracker.', nan]\n",
      "['+   * @return the path', nan]\n",
      "['+   */', nan]\n",
      "['+  public Path getBasePath() {', nan]\n",
      "['+    return basePath;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A marker has been found; this may or may not be a leaf.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Trigger a move of all markers above it into the surplus map.', nan]\n",
      "['+   * @param path marker path', nan]\n",
      "['+   * @param key object key', nan]\n",
      "['+   * @param source listing source', nan]\n",
      "['+   * @return the surplus markers found.', nan]\n",
      "['+   */', nan]\n",
      "['+  public List<Marker> markerFound(Path path,', nan]\n",
      "['+      final String key,', nan]\n",
      "['+      final S3ALocatedFileStatus source) {', nan]\n",
      "['+    markersFound++;', nan]\n",
      "['+    leafMarkers.put(path, new Marker(path, key, source));', nan]\n",
      "['+    return pathFound(path, key, source);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A file has been found. Trigger a move of all', nan]\n",
      "['+   * markers above it into the surplus map.', nan]\n",
      "['+   * @param path marker path', nan]\n",
      "['+   * @param key object key', nan]\n",
      "['+   * @param source listing source', nan]\n",
      "['+   * @return the surplus markers found.', nan]\n",
      "['+   */', nan]\n",
      "['+  public List<Marker> fileFound(Path path,', nan]\n",
      "['+      final String key,', nan]\n",
      "['+      final S3ALocatedFileStatus source) {', nan]\n",
      "['+    filesFound++;', nan]\n",
      "['+    return pathFound(path, key, source);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A path has been found.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Declare all markers above it as surplus', nan]\n",
      "['+   * @param path marker path', nan]\n",
      "['+   * @param key object key', nan]\n",
      "['+   * @param source listing source', nan]\n",
      "['+   * @return the surplus markers found.', nan]\n",
      "['+   */', nan]\n",
      "['+  private List<Marker> pathFound(Path path,', nan]\n",
      "['+      final String key,', nan]\n",
      "['+      final S3ALocatedFileStatus source) {', nan]\n",
      "['+    objectsFound++;', nan]\n",
      "['+    List<Marker> removed = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+    // all parent entries are superfluous', nan]\n",
      "['+    final Path parent = path.getParent();', nan]\n",
      "['+    if (parent == null || parent.equals(lastDirChecked)) {', nan]\n",
      "['+      // short cut exit', nan]\n",
      "['+      return removed;', nan]\n",
      "['+    }', nan]\n",
      "['+    removeParentMarkers(parent, removed);', nan]\n",
      "['+    lastDirChecked = parent;', nan]\n",
      "['+    return removed;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Remove all markers from the path and its parents from the', nan]\n",
      "['+   * {@link #leafMarkers} map.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * if {@link #recordSurplusMarkers} is true, the marker is', nan]\n",
      "['+   * moved to the surplus map. Not doing this is simply an', nan]\n",
      "['+   * optimisation designed to reduce risk of excess memory consumption', nan]\n",
      "['+   * when renaming (hypothetically) large directory trees.', nan]\n",
      "['+   * @param path path to start at', nan]\n",
      "['+   * @param removed list of markers removed; is built up during the', nan]\n",
      "['+   * recursive operation.', nan]\n",
      "['+   */', nan]\n",
      "['+  private void removeParentMarkers(final Path path,', nan]\n",
      "['+      List<Marker> removed) {', nan]\n",
      "['+    if (path == null || path.isRoot()) {', nan]\n",
      "['+      return;', nan]\n",
      "['+    }', nan]\n",
      "['+    scanCount++;', nan]\n",
      "['+    removeParentMarkers(path.getParent(), removed);', nan]\n",
      "['+    final Marker value = leafMarkers.remove(path);', nan]\n",
      "['+    if (value != null) {', nan]\n",
      "['+      // marker is surplus', nan]\n",
      "['+      removed.add(value);', nan]\n",
      "['+      if (recordSurplusMarkers) {', nan]\n",
      "['+        surplusMarkers.put(path, value);', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the map of leaf markers.', nan]\n",
      "['+   * @return all leaf markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  public Map<Path, Marker> getLeafMarkers() {', nan]\n",
      "['+    return leafMarkers;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the map of surplus markers.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Empty if they were not being recorded.', nan]\n",
      "['+   * @return all surplus markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  public Map<Path, Marker> getSurplusMarkers() {', nan]\n",
      "['+    return surplusMarkers;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public Path getLastDirChecked() {', nan]\n",
      "['+    return lastDirChecked;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many objects were found.', nan]\n",
      "['+   * @return count', nan]\n",
      "['+   */', nan]\n",
      "['+  public int getObjectsFound() {', nan]\n",
      "['+    return objectsFound;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public int getScanCount() {', nan]\n",
      "['+    return scanCount;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public int getFilesFound() {', nan]\n",
      "['+    return filesFound;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public int getMarkersFound() {', nan]\n",
      "['+    return markersFound;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public String toString() {', nan]\n",
      "['+    return \"DirMarkerTracker{\" +', nan]\n",
      "['+        \"leafMarkers=\" + leafMarkers.size() +', nan]\n",
      "['+        \", surplusMarkers=\" + surplusMarkers.size() +', nan]\n",
      "['+        \", lastDirChecked=\" + lastDirChecked +', nan]\n",
      "['+        \", filesFound=\" + filesFound +', nan]\n",
      "['+        \", scanCount=\" + scanCount +', nan]\n",
      "[\"+        '}';\", nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Scan the surplus marker list and remove from it all where the directory', nan]\n",
      "['+   * policy says \"keep\". This is useful when auditing', nan]\n",
      "['+   * @param policy policy to use when auditing markers for', nan]\n",
      "['+   * inclusion/exclusion.', nan]\n",
      "['+   * @return list of markers stripped', nan]\n",
      "['+   */', nan]\n",
      "['+  public List<Path> removeAllowedMarkers(DirectoryPolicy policy) {', nan]\n",
      "['+    List<Path> removed = new ArrayList<>();', nan]\n",
      "['+    Iterator<Map.Entry<Path, Marker>> entries =', nan]\n",
      "['+        surplusMarkers.entrySet().iterator();', nan]\n",
      "['+    while (entries.hasNext()) {', nan]\n",
      "['+      Map.Entry<Path, Marker> entry = entries.next();', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+      Path path = entry.getKey();', nan]\n",
      "['+      if (policy.keepDirectoryMarkers(path)) {', nan]\n",
      "[\"+        // there's a match\", nan]\n",
      "['+        // remove it from the map.', nan]\n",
      "['+        entries.remove();', nan]\n",
      "['+        LOG.debug(\"Removing {}\", entry.getValue());', nan]\n",
      "['+        removed.add(path);', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+    return removed;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * This is a marker entry stored in the map and', nan]\n",
      "['+   * returned as markers are deleted.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final class Marker {', nan]\n",
      "['+    /** Path of the marker. */', nan]\n",
      "['+    private final Path path;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Key in the store.', nan]\n",
      "['+     */', nan]\n",
      "['+    private final String key;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * The file status of the marker.', nan]\n",
      "['+     */', nan]\n",
      "['+    private final S3ALocatedFileStatus status;', nan]\n",
      "['+', nan]\n",
      "['+    private Marker(final Path path,', nan]\n",
      "['+        final String key,', nan]\n",
      "['+        final S3ALocatedFileStatus status) {', nan]\n",
      "['+      this.path = path;', nan]\n",
      "['+      this.key = key;', nan]\n",
      "['+      this.status = status;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    public Path getPath() {', nan]\n",
      "['+      return path;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    public String getKey() {', nan]\n",
      "['+      return key;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    public S3ALocatedFileStatus getStatus() {', nan]\n",
      "['+      return status;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Get the version ID of the status object; may be null.', nan]\n",
      "['+     * @return a version ID, if known.', nan]\n",
      "['+     */', nan]\n",
      "['+    public String getVersionId() {', nan]\n",
      "['+      return status.getVersionId();', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public String toString() {', nan]\n",
      "['+      return \"Marker{\" +', nan]\n",
      "['+          \"path=\" + path +', nan]\n",
      "['+          \", key=\\'\" + key + \\'\\\\\\'\\' +', nan]\n",
      "['+          \", status=\" + status +', nan]\n",
      "[\"+          '}';\", nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicy.java b/had', 'op-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicy.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..36dd2e4fd24', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicy.java', nan]\n",
      "['@@ -0,0 +1,110 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.impl;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_AUTHORITATIVE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_KEEP;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Interface for Directory Marker policies to implement.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+public interface DirectoryPolicy {', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Should a directory marker be retained?', nan]\n",
      "['+   * @param path path a file/directory is being created with.', nan]\n",
      "['+   * @return true if the marker MAY be kept, false if it MUST be deleted.', nan]\n",
      "['+   */', nan]\n",
      "['+  boolean keepDirectoryMarkers(Path path);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the marker policy.', nan]\n",
      "['+   * @return policy.', nan]\n",
      "['+   */', nan]\n",
      "['+  MarkerPolicy getMarkerPolicy();', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Describe the policy for marker tools and logs.', nan]\n",
      "['+   * @return description of the current policy.', nan]\n",
      "['+   */', nan]\n",
      "['+  String describe();', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Does a specific path have the relevant option.', nan]\n",
      "['+   * This is to be forwarded from the S3AFileSystem.hasPathCapability', nan]\n",
      "['+   * But only for those capabilities related to markers*', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param capability capability', nan]\n",
      "['+   * @return true if the capability is supported, false if not', nan]\n",
      "['+   * @throws IllegalArgumentException if the capability is unknown.', nan]\n",
      "['+   */', nan]\n",
      "['+  boolean hasPathCapability(Path path, String capability);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Supported retention policies.', nan]\n",
      "['+   */', nan]\n",
      "['+  enum MarkerPolicy {', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Delete markers.', nan]\n",
      "['+     * <p></p>', nan]\n",
      "['+     * This is the classic S3A policy,', nan]\n",
      "['+     */', nan]\n",
      "['+    Delete(DIRECTORY_MARKER_POLICY_DELETE),', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Keep markers.', nan]\n",
      "['+     * <p></p>', nan]\n",
      "['+     * This is <i>Not backwards compatible</i>.', nan]\n",
      "['+     */', nan]\n",
      "['+    Keep(DIRECTORY_MARKER_POLICY_KEEP),', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Keep markers in authoritative paths only.', nan]\n",
      "['+     * <p></p>', nan]\n",
      "['+     * This is <i>Not backwards compatible</i> within the', nan]\n",
      "['+     * auth paths, but is outside these.', nan]\n",
      "['+     */', nan]\n",
      "['+    Authoritative(DIRECTORY_MARKER_POLICY_AUTHORITATIVE);', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * The name of the option as allowed in configuration files', nan]\n",
      "['+     * and marker-aware tooling.', nan]\n",
      "['+     */', nan]\n",
      "['+    private final String optionName;', nan]\n",
      "['+', nan]\n",
      "['+    MarkerPolicy(final String optionName) {', nan]\n",
      "['+      this.optionName = optionName;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Get the option name.', nan]\n",
      "['+     * @return name of the option', nan]\n",
      "['+     */', nan]\n",
      "['+    public String getOptionName() {', nan]\n",
      "['+      return optionName;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicyImpl.java b', 'hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicyImpl.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..a1aa2580b65', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicyImpl.java', nan]\n",
      "['@@ -0,0 +1,212 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.impl;', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+import java.util.EnumSet;', nan]\n",
      "['+import java.util.Locale;', nan]\n",
      "['+import java.util.Set;', nan]\n",
      "['+import java.util.function.Predicate;', nan]\n",
      "['+', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_DIRECTORY_MARKER_POLICY;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_AUTHORITATIVE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_KEEP;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_AWARE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_AUTHORITATIVE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_KEEP;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Implementation of directory policy.', nan]\n",
      "['+ */', nan]\n",
      "['+public final class DirectoryPolicyImpl', nan]\n",
      "['+    implements DirectoryPolicy {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG = LoggerFactory.getLogger(', nan]\n",
      "['+      DirectoryPolicyImpl.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Error string when unable to parse the marker policy option.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String UNKNOWN_MARKER_POLICY =', nan]\n",
      "['+      \"Unknown policy in \"', nan]\n",
      "['+      + DIRECTORY_MARKER_POLICY + \": \";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * All available policies.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final Set<MarkerPolicy> AVAILABLE_POLICIES =', nan]\n",
      "['+      EnumSet.allOf(MarkerPolicy.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Keep all markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final DirectoryPolicy KEEP = new DirectoryPolicyImpl(', nan]\n",
      "['+      MarkerPolicy.Keep, (p) -> false);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Delete all markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final DirectoryPolicy DELETE = new DirectoryPolicyImpl(', nan]\n",
      "['+      MarkerPolicy.Delete, (p) -> false);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Chosen marker policy.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final MarkerPolicy markerPolicy;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Callback to evaluate authoritativeness of a', nan]\n",
      "['+   * path.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final Predicate<Path> authoritativeness;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Constructor.', nan]\n",
      "['+   * @param markerPolicy marker policy', nan]\n",
      "['+   * @param authoritativeness function for authoritativeness', nan]\n",
      "['+   */', nan]\n",
      "['+  public DirectoryPolicyImpl(final MarkerPolicy markerPolicy,', nan]\n",
      "['+      final Predicate<Path> authoritativeness) {', nan]\n",
      "['+    this.markerPolicy = markerPolicy;', nan]\n",
      "['+    this.authoritativeness = authoritativeness;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public boolean keepDirectoryMarkers(final Path path) {', nan]\n",
      "['+    switch (markerPolicy) {', nan]\n",
      "['+    case Keep:', nan]\n",
      "['+      return true;', nan]\n",
      "['+    case Authoritative:', nan]\n",
      "['+      return authoritativeness.test(path);', nan]\n",
      "['+    case Delete:', nan]\n",
      "['+    default:   // which cannot happen', nan]\n",
      "['+      return false;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public MarkerPolicy getMarkerPolicy() {', nan]\n",
      "['+    return markerPolicy;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public String describe() {', nan]\n",
      "['+    return markerPolicy.getOptionName();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public String toString() {', nan]\n",
      "['+    final StringBuilder sb = new StringBuilder(', nan]\n",
      "['+        \"DirectoryMarkerRetention{\");', nan]\n",
      "['+    sb.append(\"policy=\\'\").append(markerPolicy.getOptionName()).append(\\'\\\\\\'\\');', nan]\n",
      "[\"+    sb.append('}');\", nan]\n",
      "['+    return sb.toString();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Return path policy for store and paths.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param capability capability', nan]\n",
      "['+   * @return true if a capability is active', nan]\n",
      "['+   */', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public boolean hasPathCapability(final Path path, final String capability) {', nan]\n",
      "['+', nan]\n",
      "['+    switch (capability) {', nan]\n",
      "['+    /*', nan]\n",
      "['+     * Marker policy is dynamically determined for the given path.', nan]\n",
      "['+     */', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_AWARE:', nan]\n",
      "['+      return true;', nan]\n",
      "['+', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_KEEP:', nan]\n",
      "['+      return markerPolicy == MarkerPolicy.Keep;', nan]\n",
      "['+', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_DELETE:', nan]\n",
      "['+      return markerPolicy == MarkerPolicy.Delete;', nan]\n",
      "['+', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_AUTHORITATIVE:', nan]\n",
      "['+      return markerPolicy == MarkerPolicy.Authoritative;', nan]\n",
      "['+', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP:', nan]\n",
      "['+      return keepDirectoryMarkers(path);', nan]\n",
      "['+', nan]\n",
      "['+    case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE:', nan]\n",
      "['+      return !keepDirectoryMarkers(path);', nan]\n",
      "['+', nan]\n",
      "['+    default:', nan]\n",
      "['+      throw new IllegalArgumentException(\"Unknown capability \" + capability);', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create/Get the policy for this configuration.', nan]\n",
      "['+   * @param conf config', nan]\n",
      "['+   * @param authoritativeness Callback to evaluate authoritativeness of a', nan]\n",
      "['+   * path.', nan]\n",
      "['+   * @return a policy', nan]\n",
      "['+   */', nan]\n",
      "['+  public static DirectoryPolicy getDirectoryPolicy(', nan]\n",
      "['+      final Configuration conf,', nan]\n",
      "['+      final Predicate<Path> authoritativeness) {', nan]\n",
      "['+    DirectoryPolicy policy;', nan]\n",
      "['+    String option = conf.getTrimmed(DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        DEFAULT_DIRECTORY_MARKER_POLICY);', nan]\n",
      "['+    switch (option.toLowerCase(Locale.ENGLISH)) {', nan]\n",
      "['+    case DIRECTORY_MARKER_POLICY_DELETE:', nan]\n",
      "['+      // backwards compatible.', nan]\n",
      "['+      LOG.debug(\"Directory markers will be deleted\");', nan]\n",
      "['+      policy = DELETE;', nan]\n",
      "['+      break;', nan]\n",
      "['+    case DIRECTORY_MARKER_POLICY_KEEP:', nan]\n",
      "['+      LOG.info(\"Directory markers will be kept\");', nan]\n",
      "['+      policy = KEEP;', nan]\n",
      "['+      break;', nan]\n",
      "['+    case DIRECTORY_MARKER_POLICY_AUTHORITATIVE:', nan]\n",
      "['+      LOG.info(\"Directory markers will be kept on authoritative\"', nan]\n",
      "['+          + \" paths\");', nan]\n",
      "['+      policy = new DirectoryPolicyImpl(MarkerPolicy.Authoritative,', nan]\n",
      "['+          authoritativeness);', nan]\n",
      "['+      break;', nan]\n",
      "['+    default:', nan]\n",
      "['+      throw new IllegalArgumentException(UNKNOWN_MARKER_POLICY + option);', nan]\n",
      "['+    }', nan]\n",
      "['+    return policy;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Enumerate all available policies.', nan]\n",
      "['+   * @return set of the policies.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static Set<MarkerPolicy> availablePolicies() {', nan]\n",
      "['+    return AVAILABLE_POLICIES;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java b/had', 'op-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java']\n",
      "['index 750aebf500a..beb19500812 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java', nan]\n",
      "['@@ -21,6 +21,7 @@', nan]\n",
      "['import java.io.IOException;', nan]\n",
      "['import java.util.ArrayList;', nan]\n",
      "['import java.util.List;', nan]\n",
      "['+import java.util.Map;', nan]\n",
      "['import java.util.concurrent.CompletableFuture;', nan]\n",
      "['import java.util.concurrent.atomic.AtomicLong;', nan]\n",
      "[nan, nan]\n",
      "['@@ -43,6 +44,7 @@', nan]\n",
      "['import org.apache.hadoop.fs.s3a.s3guard.MetadataStore;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.s3guard.RenameTracker;', nan]\n",
      "['import org.apache.hadoop.util.DurationInfo;', nan]\n",
      "['+import org.apache.hadoop.util.OperationDuration;', nan]\n",
      "[nan, nan]\n",
      "['import static com.google.common.base.Preconditions.checkNotNull;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.FS_S3A_BLOCK_SIZE;', nan]\n",
      "['@@ -55,19 +57,31 @@', nan]\n",
      "['/**', nan]\n",
      "['* A parallelized rename operation which updates the metastore in the', nan]\n",
      "['* process, through whichever {@link RenameTracker} the store provides.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['* The parallel execution is in groups of size', nan]\n",
      "['* {@link InternalConstants#RENAME_PARALLEL_LIMIT}; it is only', nan]\n",
      "['* after one group completes that the next group is initiated.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['* Once enough files have been copied that they meet the', nan]\n",
      "['* {@link InternalConstants#MAX_ENTRIES_TO_DELETE} threshold, a delete', nan]\n",
      "['* is initiated.', nan]\n",
      "['* If it succeeds, the rename continues with the next group of files.', nan]\n",
      "['- *', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['* The RenameTracker has the task of keeping the metastore up to date', nan]\n",
      "['* as the rename proceeds.', nan]\n",
      "['- *', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Directory Markers which have child entries are never copied; only those', nan]\n",
      "['+ * which represent empty directories are copied in the rename.', nan]\n",
      "['+ * The {@link DirMarkerTracker} tracks which markers must be copied, and', nan]\n",
      "['+ * which can simply be deleted from the source.', nan]\n",
      "['+ * As a result: rename always purges all non-leaf directory markers from', nan]\n",
      "['+ * the copied tree. This is to ensure that even if a directory tree', nan]\n",
      "['+ * is copied from an authoritative path to a non-authoritative one', nan]\n",
      "['+ * there is never any contamination of the non-auth path with markers.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['* The rename operation implements the classic HDFS rename policy of', nan]\n",
      "['* rename(file, dir) renames the file under the directory.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['*', nan]\n",
      "['* There is <i>no</i> validation of input and output paths.', nan]\n",
      "['* Callers are required to themselves verify that destination is not under', nan]\n",
      "['@@ -183,12 +197,59 @@ private void completeActiveCopies(String reason) throws IOException {', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* Queue an object for deletion.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * This object will be deleted when the next page of objects to delete', nan]\n",
      "['+   * is posted to S3. Therefore, the COPY must have finished', nan]\n",
      "['+   * before that deletion operation takes place.', nan]\n",
      "['+   * This is managed by:', nan]\n",
      "['+   * <ol>', nan]\n",
      "['+   *   <li>', nan]\n",
      "['+   *     The delete operation only being executed once all active', nan]\n",
      "['+   *     copies have completed.', nan]\n",
      "['+   *   </li>', nan]\n",
      "['+   *   <li>', nan]\n",
      "['+   *     Only queuing objects here whose copy operation has', nan]\n",
      "['+   *     been submitted and so is in that thread pool.', nan]\n",
      "['+   *   </li>', nan]\n",
      "['+   * </ol>', nan]\n",
      "['+   * This method must only be called from the primary thread.', nan]\n",
      "['* @param path path to the object', nan]\n",
      "['* @param key key of the object.', nan]\n",
      "['+   * @param version object version.', nan]\n",
      "['*/', nan]\n",
      "['-  private void queueToDelete(Path path, String key) {', nan]\n",
      "['+  private void queueToDelete(Path path, String key, String version) {', nan]\n",
      "['+    LOG.debug(\"Queueing to delete {}\", path);', nan]\n",
      "['pathsToDelete.add(path);', nan]\n",
      "['-    keysToDelete.add(new DeleteObjectsRequest.KeyVersion(key));', nan]\n",
      "['+    keysToDelete.add(new DeleteObjectsRequest.KeyVersion(key, version));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Queue a list of markers for deletion.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * no-op if the list is empty.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * See {@link #queueToDelete(Path, String, String)} for', nan]\n",
      "['+   * details on safe use of this method.', nan]\n",
      "['+   *', nan]\n",
      "['+   * @param markersToDelete markers', nan]\n",
      "['+   */', nan]\n",
      "['+  private void queueToDelete(', nan]\n",
      "['+      List<DirMarkerTracker.Marker> markersToDelete) {', nan]\n",
      "['+    markersToDelete.forEach(m ->', nan]\n",
      "['+        queueToDelete(m));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Queue a single marker for deletion.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * See {@link #queueToDelete(Path, String, String)} for', nan]\n",
      "['+   * details on safe use of this method.', nan]\n",
      "['+   *', nan]\n",
      "['+   * @param marker markers', nan]\n",
      "['+   */', nan]\n",
      "['+  private void queueToDelete(final DirMarkerTracker.Marker marker) {', nan]\n",
      "['+    queueToDelete(marker.getPath(), marker.getKey(),', nan]\n",
      "['+        marker.getStatus().getVersionId());', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -225,11 +286,19 @@ public Long execute() throws IOException {', nan]\n",
      "['storeContext,', nan]\n",
      "['sourcePath, sourceStatus, destPath);', nan]\n",
      "[nan, nan]\n",
      "['+    // The path to whichever file or directory is created by the', nan]\n",
      "['+    // rename. When deleting markers all parents of', nan]\n",
      "['+    // this path will need their markers pruned.', nan]\n",
      "['+    Path destCreated = destPath;', nan]\n",
      "[nan, nan]\n",
      "['// Ok! Time to start', nan]\n",
      "['try {', nan]\n",
      "['if (sourceStatus.isFile()) {', nan]\n",
      "['-        renameFileToDest();', nan]\n",
      "['+        // rename the file. The destination path will be different', nan]\n",
      "['+        // from that passed in if the destination is a directory;', nan]\n",
      "['+        // the final value is needed to completely delete parent markers', nan]\n",
      "['+        // when they are not being retained.', nan]\n",
      "['+        destCreated = renameFileToDest();', nan]\n",
      "['} else {', nan]\n",
      "['recursiveDirectoryRename();', nan]\n",
      "['}', nan]\n",
      "['@@ -254,15 +323,17 @@ public Long execute() throws IOException {', nan]\n",
      "['// Tell the metastore this fact and let it complete its changes', nan]\n",
      "['renameTracker.completeRename();', nan]\n",
      "[nan, nan]\n",
      "['-    callbacks.finishRename(sourcePath, destPath);', nan]\n",
      "['+    callbacks.finishRename(sourcePath, destCreated);', nan]\n",
      "['return bytesCopied.get();', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['-   * The source is a file: rename it to the destination.', nan]\n",
      "['+   * The source is a file: rename it to the destination, which', nan]\n",
      "['+   * will be under the current destination path if that is a directory.', nan]\n",
      "['+   * @return the path of the object created.', nan]\n",
      "['* @throws IOException failure', nan]\n",
      "['*/', nan]\n",
      "['-  protected void renameFileToDest() throws IOException {', nan]\n",
      "['+  protected Path renameFileToDest() throws IOException {', nan]\n",
      "['final StoreContext storeContext = getStoreContext();', nan]\n",
      "['// the source is a file.', nan]\n",
      "['Path copyDestinationPath = destPath;', nan]\n",
      "['@@ -295,12 +366,14 @@ protected void renameFileToDest() throws IOException {', nan]\n",
      "['callbacks.deleteObjectAtPath(sourcePath, sourceKey, true, null);', nan]\n",
      "['// and update the tracker', nan]\n",
      "['renameTracker.sourceObjectsDeleted(Lists.newArrayList(sourcePath));', nan]\n",
      "['+    return copyDestinationPath;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* Execute a full recursive rename.', nan]\n",
      "['-   * The source is a file: rename it to the destination.', nan]\n",
      "['-   * @throws IOException failure', nan]\n",
      "['+   * There is a special handling of directly markers here -only leaf markers', nan]\n",
      "['+   * are copied. This reduces incompatibility \"regions\" across versions.', nan]\n",
      "['+Are   * @throws IOException failure', nan]\n",
      "['*/', nan]\n",
      "['protected void recursiveDirectoryRename() throws IOException {', nan]\n",
      "['final StoreContext storeContext = getStoreContext();', nan]\n",
      "['@@ -325,10 +398,18 @@ protected void recursiveDirectoryRename() throws IOException {', nan]\n",
      "['// marker.', nan]\n",
      "['LOG.debug(\"Deleting fake directory marker at destination {}\",', nan]\n",
      "['destStatus.getPath());', nan]\n",
      "[\"+      // Although the dir marker policy doesn't always need to do this,\", nan]\n",
      "[\"+      // it's simplest just to be consistent here.\", nan]\n",
      "['callbacks.deleteObjectAtPath(destStatus.getPath(), dstKey, false, null);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['Path parentPath = storeContext.keyToPath(srcKey);', nan]\n",
      "['+', nan]\n",
      "['+    // Track directory markers so that we know which leaf directories need to be', nan]\n",
      "['+    // recreated', nan]\n",
      "['+    DirMarkerTracker dirMarkerTracker = new DirMarkerTracker(parentPath,', nan]\n",
      "['+        false);', nan]\n",
      "['+', nan]\n",
      "['final RemoteIterator<S3ALocatedFileStatus> iterator =', nan]\n",
      "['callbacks.listFilesAndEmptyDirectories(parentPath,', nan]\n",
      "['sourceStatus,', nan]\n",
      "['@@ -347,36 +428,45 @@ protected void recursiveDirectoryRename() throws IOException {', nan]\n",
      "['// the source object to copy as a path.', nan]\n",
      "['Path childSourcePath = storeContext.keyToPath(key);', nan]\n",
      "[nan, nan]\n",
      "['-      // mark for deletion on a successful copy.', nan]\n",
      "['-      queueToDelete(childSourcePath, key);', nan]\n",
      "['-', nan]\n",
      "['-      // the destination key is that of the key under the source tree,', nan]\n",
      "['-      // remapped under the new destination path.', nan]\n",
      "['-      String newDestKey =', nan]\n",
      "['-          dstKey + key.substring(srcKey.length());', nan]\n",
      "['-      Path childDestPath = storeContext.keyToPath(newDestKey);', nan]\n",
      "['-', nan]\n",
      "['-      // now begin the single copy', nan]\n",
      "['-      CompletableFuture<Path> copy = initiateCopy(child, key,', nan]\n",
      "['-          childSourcePath, newDestKey, childDestPath);', nan]\n",
      "['-      activeCopies.add(copy);', nan]\n",
      "['-      bytesCopied.addAndGet(sourceStatus.getLen());', nan]\n",
      "['+      List<DirMarkerTracker.Marker> markersToDelete;', nan]\n",
      "[nan, nan]\n",
      "['-      if (activeCopies.size() == RENAME_PARALLEL_LIMIT) {', nan]\n",
      "['-        // the limit of active copies has been reached;', nan]\n",
      "['-        // wait for completion or errors to surface.', nan]\n",
      "['-        LOG.debug(\"Waiting for active copies to complete\");', nan]\n",
      "['-        completeActiveCopies(\"batch threshold reached\");', nan]\n",
      "['-      }', nan]\n",
      "['-      if (keysToDelete.size() == pageSize) {', nan]\n",
      "['-        // finish ongoing copies then delete all queued keys.', nan]\n",
      "['-        // provided the parallel limit is a factor of the max entry', nan]\n",
      "['-        // constant, this will not need to block for the copy, and', nan]\n",
      "['-        // simply jump straight to the delete.', nan]\n",
      "['-        completeActiveCopiesAndDeleteSources(\"paged delete\");', nan]\n",
      "['+      boolean isMarker = key.endsWith(\"/\");', nan]\n",
      "['+      if (isMarker) {', nan]\n",
      "['+        // add the marker to the tracker.', nan]\n",
      "['+        // it will not be deleted _yet_ but it may find a list of parent', nan]\n",
      "['+        // markers which may now be deleted.', nan]\n",
      "['+        markersToDelete = dirMarkerTracker.markerFound(', nan]\n",
      "['+            childSourcePath, key, child);', nan]\n",
      "['+      } else {', nan]\n",
      "['+        // it is a file.', nan]\n",
      "['+        // note that it has been found -this may find a list of parent', nan]\n",
      "['+        // markers which may now be deleted.', nan]\n",
      "['+        markersToDelete = dirMarkerTracker.fileFound(', nan]\n",
      "['+            childSourcePath, key, child);', nan]\n",
      "['+        // the destination key is that of the key under the source tree,', nan]\n",
      "['+        // remapped under the new destination path.', nan]\n",
      "['+        String newDestKey =', nan]\n",
      "['+            dstKey + key.substring(srcKey.length());', nan]\n",
      "['+        Path childDestPath = storeContext.keyToPath(newDestKey);', nan]\n",
      "['+', nan]\n",
      "['+        // mark the source file for deletion on a successful copy.', nan]\n",
      "['+        queueToDelete(childSourcePath, key, child.getVersionId());', nan]\n",
      "['+          // now begin the single copy', nan]\n",
      "['+        CompletableFuture<Path> copy = initiateCopy(child, key,', nan]\n",
      "['+            childSourcePath, newDestKey, childDestPath);', nan]\n",
      "['+        activeCopies.add(copy);', nan]\n",
      "['+        bytesCopied.addAndGet(sourceStatus.getLen());', nan]\n",
      "['}', nan]\n",
      "['+      // add any markers to delete to the operation so they get cleaned', nan]\n",
      "['+      // incrementally', nan]\n",
      "['+      queueToDelete(markersToDelete);', nan]\n",
      "['+      // and trigger any end of loop operations', nan]\n",
      "['+      endOfLoopActions();', nan]\n",
      "['} // end of iteration through the list', nan]\n",
      "[nan, nan]\n",
      "['+    // finally process remaining directory markers', nan]\n",
      "['+    copyEmptyDirectoryMarkers(srcKey, dstKey, dirMarkerTracker);', nan]\n",
      "['+', nan]\n",
      "['// await the final set of copies and their deletion', nan]\n",
      "['// This will notify the renameTracker that these objects', nan]\n",
      "['// have been deleted.', nan]\n",
      "['@@ -387,6 +477,93 @@ protected void recursiveDirectoryRename() throws IOException {', nan]\n",
      "['renameTracker.moveSourceDirectory();', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Operations to perform at the end of every loop iteration.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * This may block the thread waiting for copies to complete', nan]\n",
      "['+   * and/or delete a page of data.', nan]\n",
      "['+   */', nan]\n",
      "['+  private void endOfLoopActions() throws IOException {', nan]\n",
      "['+    if (keysToDelete.size() == pageSize) {', nan]\n",
      "['+      // finish ongoing copies then delete all queued keys.', nan]\n",
      "['+      completeActiveCopiesAndDeleteSources(\"paged delete\");', nan]\n",
      "['+    } else {', nan]\n",
      "['+      if (activeCopies.size() == RENAME_PARALLEL_LIMIT) {', nan]\n",
      "['+        // the limit of active copies has been reached;', nan]\n",
      "['+        // wait for completion or errors to surface.', nan]\n",
      "['+        LOG.debug(\"Waiting for active copies to complete\");', nan]\n",
      "['+        completeActiveCopies(\"batch threshold reached\");', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Process all directory markers at the end of the rename.', nan]\n",
      "['+   * All leaf markers are queued to be copied in the store;', nan]\n",
      "['+   * this updates the metastore tracker as it does so.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Why not simply create new markers? All the metadata', nan]\n",
      "['+   * gets copied too, so if there was anything relevant then', nan]\n",
      "['+   * it would be preserved.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "[\"+   * At the same time: markers aren't valued much and may\", nan]\n",
      "['+   * be deleted without any safety checks -so if there was relevant', nan]\n",
      "['+   * data it is at risk of destruction at any point.', nan]\n",
      "['+   * If there are lots of empty directory rename operations taking place,', nan]\n",
      "['+   * the decision to copy the source may need revisiting.', nan]\n",
      "['+   * Be advised though: the costs of the copy not withstanding,', nan]\n",
      "['+   * it is a lot easier to have one single type of scheduled copy operation', nan]\n",
      "['+   * than have copy and touch calls being scheduled.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * The duration returned is the time to initiate all copy/delete operations,', nan]\n",
      "['+   * including any blocking waits for active copies and paged deletes', nan]\n",
      "['+   * to execute. There may still be outstanding operations', nan]\n",
      "['+   * queued by this method -the duration may be an underestimate', nan]\n",
      "['+   * of the time this operation actually takes.', nan]\n",
      "['+   *', nan]\n",
      "['+   * @param srcKey source key with trailing /', nan]\n",
      "['+   * @param dstKey dest key with trailing /', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+   * @param dirMarkerTracker tracker of markers', nan]\n",
      "['+   * @return how long it took.', nan]\n",
      "['+   */', nan]\n",
      "['+  private OperationDuration copyEmptyDirectoryMarkers(', nan]\n",
      "['+      final String srcKey,', nan]\n",
      "['+      final String dstKey,', nan]\n",
      "['+      final DirMarkerTracker dirMarkerTracker) throws IOException {', nan]\n",
      "['+    // directory marker work.', nan]\n",
      "['+    LOG.debug(\"Copying markers from {}\", dirMarkerTracker);', nan]\n",
      "['+    final StoreContext storeContext = getStoreContext();', nan]\n",
      "['+    Map<Path, DirMarkerTracker.Marker> leafMarkers =', nan]\n",
      "['+        dirMarkerTracker.getLeafMarkers();', nan]\n",
      "['+    Map<Path, DirMarkerTracker.Marker> surplus =', nan]\n",
      "['+        dirMarkerTracker.getSurplusMarkers();', nan]\n",
      "['+    // for all leaf markers: copy the original', nan]\n",
      "['+    DurationInfo duration = new DurationInfo(LOG, false,', nan]\n",
      "['+        \"copying %d leaf markers with %d surplus not copied\",', nan]\n",
      "['+        leafMarkers.size(), surplus.size());', nan]\n",
      "['+    for (DirMarkerTracker.Marker entry: leafMarkers.values()) {', nan]\n",
      "['+      Path source = entry.getPath();', nan]\n",
      "['+      String key = entry.getKey();', nan]\n",
      "['+      String newDestKey =', nan]\n",
      "['+          dstKey + key.substring(srcKey.length());', nan]\n",
      "['+      Path childDestPath = storeContext.keyToPath(newDestKey);', nan]\n",
      "['+      LOG.debug(\"copying dir marker from {} to {}\", key, newDestKey);', nan]\n",
      "['+', nan]\n",
      "['+      activeCopies.add(', nan]\n",
      "['+          initiateCopy(', nan]\n",
      "['+              entry.getStatus(),', nan]\n",
      "['+              key,', nan]\n",
      "['+              source,', nan]\n",
      "['+              newDestKey,', nan]\n",
      "['+              childDestPath));', nan]\n",
      "['+      queueToDelete(entry);', nan]\n",
      "['+      // end of loop', nan]\n",
      "['+      endOfLoopActions();', nan]\n",
      "['+    }', nan]\n",
      "['+    duration.close();', nan]\n",
      "['+    return duration;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Initiate a copy operation in the executor.', nan]\n",
      "['* @param source status of the source object.', nan]\n",
      "['@@ -487,6 +664,16 @@ private void removeSourceObjects(', nan]\n",
      "['List<Path> undeletedObjects = new ArrayList<>();', nan]\n",
      "['try {', nan]\n",
      "['// remove the keys', nan]\n",
      "['+', nan]\n",
      "['+      // list what is being deleted for the interest of anyone', nan]\n",
      "['+      // who is trying to debug why objects are no longer there.', nan]\n",
      "['+      if (LOG.isDebugEnabled()) {', nan]\n",
      "['+        LOG.debug(\"Initiating delete operation for {} objects\", keys.size());', nan]\n",
      "['+        for (DeleteObjectsRequest.KeyVersion key : keys) {', nan]\n",
      "['+          LOG.debug(\" {} {}\", key.getKey(),', nan]\n",
      "['+              key.getVersion() != null ? key.getVersion() : \"\");', nan]\n",
      "['+        }', nan]\n",
      "['+      }', nan]\n",
      "['// this will update the metastore on a failure, but on', nan]\n",
      "['// a successful operation leaves the store as is.', nan]\n",
      "['callbacks.removeKeys(', nan]\n",
      "['@@ -498,7 +685,7 @@ private void removeSourceObjects(', nan]\n",
      "['// and clear the list.', nan]\n",
      "['} catch (AmazonClientException | IOException e) {', nan]\n",
      "['// Failed.', nan]\n",
      "['-      // Notify the rename operation.', nan]\n",
      "['+      // Notify the rename tracker.', nan]\n",
      "['// removeKeys will have already purged the metastore of', nan]\n",
      "['// all keys it has known to delete; this is just a final', nan]\n",
      "['// bit of housekeeping and a chance to tune exception', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/StatusProbeEnum.java b/had', 'op-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/StatusProbeEnum.java']\n",
      "['index f843b20ab28..3b69c7efe37 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/StatusProbeEnum.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/StatusProbeEnum.java', nan]\n",
      "['@@ -21,9 +21,12 @@', nan]\n",
      "['import java.util.EnumSet;', nan]\n",
      "['import java.util.Set;', nan]\n",
      "[nan, nan]\n",
      "['+import org.apache.hadoop.classification.InterfaceAudience;', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Enum of probes which can be made of S3.', nan]\n",
      "['*/', nan]\n",
      "['+@InterfaceAudience.Private', nan]\n",
      "['public enum StatusProbeEnum {', nan]\n",
      "[nan, nan]\n",
      "['/** The actual path. */', nan]\n",
      "['@@ -33,28 +36,23 @@', nan]\n",
      "['/** LIST under the path. */', nan]\n",
      "['List;', nan]\n",
      "[nan, nan]\n",
      "['-  /** All probes. */', nan]\n",
      "['-  public static final Set<StatusProbeEnum> ALL = EnumSet.allOf(', nan]\n",
      "['-      StatusProbeEnum.class);', nan]\n",
      "['-', nan]\n",
      "['-  /** Skip the HEAD and only look for directories. */', nan]\n",
      "['-  public static final Set<StatusProbeEnum> DIRECTORIES =', nan]\n",
      "['-      EnumSet.of(DirMarker, List);', nan]\n",
      "['-', nan]\n",
      "['-  /** We only want the HEAD or dir marker. */', nan]\n",
      "['-  public static final Set<StatusProbeEnum> HEAD_OR_DIR_MARKER =', nan]\n",
      "['-      EnumSet.of(Head, DirMarker);', nan]\n",
      "['+  /** Look for files and directories. */', nan]\n",
      "['+  public static final Set<StatusProbeEnum> ALL =', nan]\n",
      "['+      EnumSet.of(Head, List);', nan]\n",
      "[nan, nan]\n",
      "['/** We only want the HEAD. */', nan]\n",
      "['public static final Set<StatusProbeEnum> HEAD_ONLY =', nan]\n",
      "['EnumSet.of(Head);', nan]\n",
      "[nan, nan]\n",
      "['-  /** We only want the dir marker. */', nan]\n",
      "['-  public static final Set<StatusProbeEnum> DIR_MARKER_ONLY =', nan]\n",
      "['-      EnumSet.of(DirMarker);', nan]\n",
      "['-', nan]\n",
      "['-  /** We only want the dir marker. */', nan]\n",
      "['+  /** List operation only. */', nan]\n",
      "['public static final Set<StatusProbeEnum> LIST_ONLY =', nan]\n",
      "['EnumSet.of(List);', nan]\n",
      "[nan, nan]\n",
      "['+  /** Look for files and directories. */', nan]\n",
      "['+  public static final Set<StatusProbeEnum> FILE =', nan]\n",
      "['+      HEAD_ONLY;', nan]\n",
      "['+', nan]\n",
      "['+  /** Skip the HEAD and only look for directories. */', nan]\n",
      "['+  public static final Set<StatusProbeEnum> DIRECTORIES =', nan]\n",
      "['+      LIST_ONLY;', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DirListingMetadata.java', 'b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DirListingMetadata.java']\n",
      "['index 213ffdc9837..5f033fa11f8 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DirListingMetadata.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DirListingMetadata.java', nan]\n",
      "['@@ -25,6 +25,7 @@', nan]\n",
      "['import java.util.Collections;', nan]\n",
      "['import java.util.HashSet;', nan]\n",
      "['import java.util.Iterator;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['import java.util.Map;', nan]\n",
      "['import java.util.Set;', nan]\n",
      "['import java.util.concurrent.ConcurrentHashMap;', nan]\n",
      "['@@ -118,6 +119,10 @@ public Path getPath() {', nan]\n",
      "['return Collections.unmodifiableCollection(listMap.values());', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * List all tombstones.', nan]\n",
      "['+   * @return all tombstones in the listing.', nan]\n",
      "['+   */', nan]\n",
      "['public Set<Path> listTombstones() {', nan]\n",
      "['Set<Path> tombstones = new HashSet<>();', nan]\n",
      "['for (PathMetadata meta : listMap.values()) {', nan]\n",
      "['@@ -128,6 +133,12 @@ public Path getPath() {', nan]\n",
      "['return tombstones;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the directory listing excluding tombstones.', nan]\n",
      "['+   * Returns a new DirListingMetadata instances, without the tombstones -the', nan]\n",
      "['+   * lastUpdated field is copied from this instance.', nan]\n",
      "['+   * @return a new DirListingMetadata without the tombstones.', nan]\n",
      "['+   */', nan]\n",
      "['public DirListingMetadata withoutTombstones() {', nan]\n",
      "['Collection<PathMetadata> filteredList = new ArrayList<>();', nan]\n",
      "['for (PathMetadata meta : listMap.values()) {', nan]\n",
      "['@@ -143,6 +154,7 @@ public DirListingMetadata withoutTombstones() {', nan]\n",
      "['* @return number of entries tracked.  This is not the same as the number', nan]\n",
      "['* of entries in the actual directory unless {@link #isAuthoritative()} is', nan]\n",
      "['* true.', nan]\n",
      "['+   * It will also include any tombstones.', nan]\n",
      "['*/', nan]\n",
      "['public int numEntries() {', nan]\n",
      "['return listMap.size();', nan]\n",
      "['@@ -251,19 +263,24 @@ public String toString() {', nan]\n",
      "['* Remove expired entries from the listing based on TTL.', nan]\n",
      "['* @param ttl the ttl time', nan]\n",
      "['* @param now the current time', nan]\n",
      "['+   * @return the expired values.', nan]\n",
      "['*/', nan]\n",
      "['-  public synchronized void removeExpiredEntriesFromListing(long ttl,', nan]\n",
      "['-      long now) {', nan]\n",
      "['+  public synchronized List<PathMetadata> removeExpiredEntriesFromListing(', nan]\n",
      "['+      long ttl, long now) {', nan]\n",
      "['+    List<PathMetadata> expired = new ArrayList<>();', nan]\n",
      "['final Iterator<Map.Entry<Path, PathMetadata>> iterator =', nan]\n",
      "['listMap.entrySet().iterator();', nan]\n",
      "['while (iterator.hasNext()) {', nan]\n",
      "['final Map.Entry<Path, PathMetadata> entry = iterator.next();', nan]\n",
      "['// we filter iff the lastupdated is not 0 and the entry is expired', nan]\n",
      "['-      if (entry.getValue().getLastUpdated() != 0', nan]\n",
      "['-          && (entry.getValue().getLastUpdated() + ttl) <= now) {', nan]\n",
      "['+      PathMetadata metadata = entry.getValue();', nan]\n",
      "['+      if (metadata.getLastUpdated() != 0', nan]\n",
      "['+          && (metadata.getLastUpdated() + ttl) <= now) {', nan]\n",
      "['+        expired.add(metadata);', nan]\n",
      "['iterator.remove();', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['+    return expired;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/ITtlTimeProvider.java b', 'hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/ITtlTimeProvider.java']\n",
      "['index daee6211b41..aa7fc4721b4 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/ITtlTimeProvider.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/ITtlTimeProvider.java', nan]\n",
      "['@@ -29,6 +29,19 @@', nan]\n",
      "['* Time is measured in milliseconds,', nan]\n",
      "['*/', nan]\n",
      "['public interface ITtlTimeProvider {', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * The current time in milliseconds.', nan]\n",
      "['+   * Assuming this calls System.currentTimeMillis(), this is a native iO call', nan]\n",
      "['+   * and so should be invoked sparingly (i.e. evaluate before any loop, rather', nan]\n",
      "['+   * than inside).', nan]\n",
      "['+   * @return the current time.', nan]\n",
      "['+   */', nan]\n",
      "['long getNow();', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * The TTL of the metadata.', nan]\n",
      "['+   * @return time in millis after which metadata is considered out of date.', nan]\n",
      "['+   */', nan]\n",
      "['long getMetadataTtl();', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/NullMetadataStore.java', '/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/NullMetadataStore.java']\n",
      "['index 666c233575a..722f42176ef 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/NullMetadataStore.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/NullMetadataStore.java', nan]\n",
      "['@@ -172,8 +172,10 @@ public void addAncestors(final Path qualifiedPath,', nan]\n",
      "['private NullRenameTracker(', nan]\n",
      "['final StoreContext storeContext,', nan]\n",
      "['final Path source,', nan]\n",
      "['-        final Path dest, MetadataStore metadataStore) {', nan]\n",
      "['-      super(\"null tracker\", storeContext, metadataStore, source, dest, null);', nan]\n",
      "['+        final Path dest,', nan]\n",
      "['+        MetadataStore metadataStore) {', nan]\n",
      "['+      super(\"NullRenameTracker\", storeContext, metadataStore, source, dest,', nan]\n",
      "['+          null);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Override', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3Guard.java b/hadoop-t', 'ols/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3Guard.java']\n",
      "['index 05ebe671662..ae5c293d639 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3Guard.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3Guard.java', nan]\n",
      "['@@ -159,6 +159,54 @@ public static MetadataStore getMetadataStore(FileSystem fs,', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * We update the metastore for the specific case of S3 value == S3Guard value', nan]\n",
      "['+   * so as to place a more recent modtime in the store.', nan]\n",
      "['+   * because if not, we will continue to probe S3 whenever we look for this', nan]\n",
      "['+   * object, even we only do this if confident the S3 status is the same', nan]\n",
      "['+   * as the one in the store (i.e. it is not an older version)', nan]\n",
      "['+   * @param metadataStore MetadataStore to {@code put()} into.', nan]\n",
      "['+   * @param pm current data', nan]\n",
      "['+   * @param s3AFileStatus status to store', nan]\n",
      "['+   * @param timeProvider Time provider to use when writing entries', nan]\n",
      "['+   * @return true if the entry was updated.', nan]\n",
      "['+   * @throws IOException if metadata store update failed', nan]\n",
      "['+   */', nan]\n",
      "['+  @RetryTranslated', nan]\n",
      "['+  public static boolean refreshEntry(', nan]\n",
      "['+      MetadataStore metadataStore,', nan]\n",
      "['+      PathMetadata pm,', nan]\n",
      "['+      S3AFileStatus s3AFileStatus,', nan]\n",
      "['+      ITtlTimeProvider timeProvider) throws IOException {', nan]\n",
      "['+    // the modtime of the data is the same as/older than the s3guard value', nan]\n",
      "['+    // either an old object has been found, or the existing one was retrieved', nan]\n",
      "['+    // in both cases -return s3guard value', nan]\n",
      "['+    S3AFileStatus msStatus = pm.getFileStatus();', nan]\n",
      "['+', nan]\n",
      "['+    // first check: size', nan]\n",
      "['+    boolean sizeMatch = msStatus.getLen() == s3AFileStatus.getLen();', nan]\n",
      "['+', nan]\n",
      "['+    // etags are expected on all objects, but handle the situation', nan]\n",
      "[\"+    // that a third party store doesn't serve them.\", nan]\n",
      "['+    String s3Etag = s3AFileStatus.getETag();', nan]\n",
      "['+    String pmEtag = msStatus.getETag();', nan]\n",
      "['+    boolean etagsMatch = s3Etag != null && s3Etag.equals(pmEtag);', nan]\n",
      "['+', nan]\n",
      "['+    // version ID: only in some stores, and will be missing in the metastore', nan]\n",
      "['+    // if the entry was created through a list operation.', nan]\n",
      "['+    String s3VersionId = s3AFileStatus.getVersionId();', nan]\n",
      "['+    String pmVersionId = msStatus.getVersionId();', nan]\n",
      "['+    boolean versionsMatchOrMissingInMetastore =', nan]\n",
      "['+        pmVersionId == null || pmVersionId.equals(s3VersionId);', nan]\n",
      "['+    if (sizeMatch && etagsMatch && versionsMatchOrMissingInMetastore) {', nan]\n",
      "['+      // update the store, return the new value', nan]\n",
      "['+      LOG.debug(\"Refreshing the metastore entry/timestamp\");', nan]\n",
      "['+      putAndReturn(metadataStore, s3AFileStatus, timeProvider);', nan]\n",
      "['+      return true;', nan]\n",
      "['+    }', nan]\n",
      "['+    return false;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Helper function which puts a given S3AFileStatus into the MetadataStore and', nan]\n",
      "['* returns the same S3AFileStatus. Instrumentation monitors the put operation.', nan]\n",
      "['@@ -314,14 +362,14 @@ public static BulkOperationState initiateBulkWrite(', nan]\n",
      "['* @return Final result of directory listing.', nan]\n",
      "['* @throws IOException if metadata store update failed', nan]\n",
      "['*/', nan]\n",
      "['-  public static FileStatus[] dirListingUnion(MetadataStore ms, Path path,', nan]\n",
      "['+  public static S3AFileStatus[] dirListingUnion(MetadataStore ms, Path path,', nan]\n",
      "['List<S3AFileStatus> backingStatuses, DirListingMetadata dirMeta,', nan]\n",
      "['boolean isAuthoritative, ITtlTimeProvider timeProvider)', nan]\n",
      "['throws IOException {', nan]\n",
      "[nan, nan]\n",
      "['// Fast-path for NullMetadataStore', nan]\n",
      "['if (isNullMetadataStore(ms)) {', nan]\n",
      "['-      return backingStatuses.toArray(new FileStatus[backingStatuses.size()]);', nan]\n",
      "['+      return backingStatuses.toArray(new S3AFileStatus[backingStatuses.size()]);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['assertQualified(path);', nan]\n",
      "['@@ -927,8 +975,10 @@ public static PathMetadata getWithTtl(MetadataStore ms, Path path,', nan]\n",
      "['if (!pathMetadata.isExpired(ttl, timeProvider.getNow())) {', nan]\n",
      "['return pathMetadata;', nan]\n",
      "['} else {', nan]\n",
      "['-        LOG.debug(\"PathMetadata TTl for {} is expired in metadata store.\",', nan]\n",
      "['-            path);', nan]\n",
      "['+        LOG.debug(\"PathMetadata TTl for {} is expired in metadata store\"', nan]\n",
      "['+                + \" -removing entry\", path);', nan]\n",
      "['+        // delete the tombstone', nan]\n",
      "['+        ms.forgetMetadata(path);', nan]\n",
      "['return null;', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['@@ -940,6 +990,8 @@ public static PathMetadata getWithTtl(MetadataStore ms, Path path,', nan]\n",
      "['* List children; mark the result as non-auth if the TTL has expired.', nan]\n",
      "['* If the allowAuthoritative flag is true, return without filtering or', nan]\n",
      "['* checking for TTL expiry.', nan]\n",
      "['+   * If false: the expiry scan takes place and the', nan]\n",
      "['+   * TODO: should we always purge tombstones? Even in auth?', nan]\n",
      "['* @param ms metastore', nan]\n",
      "['* @param path path to look up.', nan]\n",
      "['* @param timeProvider nullable time provider', nan]\n",
      "['@@ -968,9 +1020,15 @@ public static DirListingMetadata listChildrenWithTtl(MetadataStore ms,', nan]\n",
      "[nan, nan]\n",
      "['// filter expired entries', nan]\n",
      "['if (dlm != null) {', nan]\n",
      "['-      dlm.removeExpiredEntriesFromListing(', nan]\n",
      "['+      List<PathMetadata> expired = dlm.removeExpiredEntriesFromListing(', nan]\n",
      "['timeProvider.getMetadataTtl(),', nan]\n",
      "['timeProvider.getNow());', nan]\n",
      "['+      // now purge the tombstones', nan]\n",
      "['+      for (PathMetadata metadata : expired) {', nan]\n",
      "['+        if (metadata.isDeleted()) {', nan]\n",
      "['+          ms.forgetMetadata(metadata.getFileStatus().getPath());', nan]\n",
      "['+        }', nan]\n",
      "['+      }', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['return dlm;', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java b/hado', 'p-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java']\n",
      "['index 6e89d0cd2da..f89777f7303 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java', nan]\n",
      "['@@ -30,12 +30,14 @@', nan]\n",
      "['import java.util.Date;', nan]\n",
      "['import java.util.HashMap;', nan]\n",
      "['import java.util.HashSet;', nan]\n",
      "['+import java.util.Iterator;', nan]\n",
      "['import java.util.List;', nan]\n",
      "['import java.util.Locale;', nan]\n",
      "['import java.util.Map;', nan]\n",
      "['import java.util.Scanner;', nan]\n",
      "['import java.util.Set;', nan]\n",
      "['import java.util.concurrent.TimeUnit;', nan]\n",
      "['+import java.util.stream.Collectors;', nan]\n",
      "[nan, nan]\n",
      "['import com.amazonaws.services.s3.model.MultipartUpload;', nan]\n",
      "['import com.google.common.annotations.VisibleForTesting;', nan]\n",
      "['@@ -44,12 +46,15 @@', nan]\n",
      "['import org.slf4j.LoggerFactory;', nan]\n",
      "[nan, nan]\n",
      "['import org.apache.commons.lang3.StringUtils;', nan]\n",
      "['+import org.apache.hadoop.classification.InterfaceAudience;', nan]\n",
      "['+import org.apache.hadoop.classification.InterfaceStability;', nan]\n",
      "['import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['import org.apache.hadoop.conf.Configured;', nan]\n",
      "['import org.apache.hadoop.fs.FileStatus;', nan]\n",
      "['import org.apache.hadoop.fs.FileSystem;', nan]\n",
      "['import org.apache.hadoop.fs.FilterFileSystem;', nan]\n",
      "['import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.StorageStatistics;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.MultipartUtils;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.S3AFileStatus;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['@@ -58,7 +63,10 @@', nan]\n",
      "['import org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.commit.CommitConstants;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.commit.InternalCommitterConstants;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.DirectoryPolicy;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.select.SelectTool;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.tools.MarkerTool;', nan]\n",
      "['import org.apache.hadoop.fs.shell.CommandFormat;', nan]\n",
      "['import org.apache.hadoop.io.IOUtils;', nan]\n",
      "['import org.apache.hadoop.security.UserGroupInformation;', nan]\n",
      "['@@ -79,7 +87,11 @@', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* CLI to manage S3Guard Metadata Store.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Some management tools invoke this class directly.', nan]\n",
      "['*/', nan]\n",
      "['+@InterfaceAudience.LimitedPrivate(\"management tools\")', nan]\n",
      "['+@InterfaceStability.Evolving', nan]\n",
      "['public abstract class S3GuardTool extends Configured implements Tool,', nan]\n",
      "['Closeable {', nan]\n",
      "['private static final Logger LOG = LoggerFactory.getLogger(S3GuardTool.class);', nan]\n",
      "['@@ -98,15 +110,17 @@', nan]\n",
      "['\"Commands: \\\\n\" +', nan]\n",
      "['\"\\\\t\" + Init.NAME + \" - \" + Init.PURPOSE + \"\\\\n\" +', nan]\n",
      "['\"\\\\t\" + Destroy.NAME + \" - \" + Destroy.PURPOSE + \"\\\\n\" +', nan]\n",
      "['-      \"\\\\t\" + Import.NAME + \" - \" + Import.PURPOSE + \"\\\\n\" +', nan]\n",
      "['+      \"\\\\t\" + Authoritative.NAME + \" - \" + Authoritative.PURPOSE + \"\\\\n\" +', nan]\n",
      "['\"\\\\t\" + BucketInfo.NAME + \" - \" + BucketInfo.PURPOSE + \"\\\\n\" +', nan]\n",
      "['-      \"\\\\t\" + Uploads.NAME + \" - \" + Uploads.PURPOSE + \"\\\\n\" +', nan]\n",
      "['\"\\\\t\" + Diff.NAME + \" - \" + Diff.PURPOSE + \"\\\\n\" +', nan]\n",
      "['+      \"\\\\t\" + Fsck.NAME + \" - \" + Fsck.PURPOSE + \"\\\\n\" +', nan]\n",
      "['+      \"\\\\t\" + Import.NAME + \" - \" + Import.PURPOSE + \"\\\\n\" +', nan]\n",
      "['+      \"\\\\t\" + MarkerTool.MARKERS + \" - \" + MarkerTool.PURPOSE + \"\\\\n\" +', nan]\n",
      "['\"\\\\t\" + Prune.NAME + \" - \" + Prune.PURPOSE + \"\\\\n\" +', nan]\n",
      "['\"\\\\t\" + SetCapacity.NAME + \" - \" + SetCapacity.PURPOSE + \"\\\\n\" +', nan]\n",
      "['\"\\\\t\" + SelectTool.NAME + \" - \" + SelectTool.PURPOSE + \"\\\\n\" +', nan]\n",
      "['-      \"\\\\t\" + Fsck.NAME + \" - \" + Fsck.PURPOSE + \"\\\\n\" +', nan]\n",
      "['-      \"\\\\t\" + Authoritative.NAME + \" - \" + Authoritative.PURPOSE + \"\\\\n\";', nan]\n",
      "['+      \"\\\\t\" + Uploads.NAME + \" - \" + Uploads.PURPOSE + \"\\\\n\";', nan]\n",
      "['+', nan]\n",
      "['private static final String DATA_IN_S3_IS_PRESERVED', nan]\n",
      "['= \"(all data in S3 is preserved)\";', nan]\n",
      "[nan, nan]\n",
      "['@@ -116,6 +130,7 @@', nan]\n",
      "['static final int SUCCESS = EXIT_SUCCESS;', nan]\n",
      "['static final int INVALID_ARGUMENT = EXIT_COMMAND_ARGUMENT_ERROR;', nan]\n",
      "['static final int E_USAGE = EXIT_USAGE;', nan]\n",
      "['+', nan]\n",
      "['static final int ERROR = EXIT_FAIL;', nan]\n",
      "['static final int E_BAD_STATE = EXIT_NOT_ACCEPTABLE;', nan]\n",
      "['static final int E_NOT_FOUND = EXIT_NOT_FOUND;', nan]\n",
      "['@@ -472,6 +487,14 @@ protected void setStore(MetadataStore store) {', nan]\n",
      "['this.store = store;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Reset the store and filesystem bindings.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected void resetBindings() {', nan]\n",
      "['+    store = null;', nan]\n",
      "['+    filesystem = null;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['protected CommandFormat getCommandFormat() {', nan]\n",
      "['return commandFormat;', nan]\n",
      "['}', nan]\n",
      "['@@ -497,6 +520,30 @@ public final int run(String[] args) throws Exception {', nan]\n",
      "['public abstract int run(String[] args, PrintStream out) throws Exception,', nan]\n",
      "['ExitUtil.ExitException;', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Dump the filesystem Storage Statistics if the FS is not null.', nan]\n",
      "['+   * Only non-zero statistics are printed.', nan]\n",
      "['+   * @param stream output stream', nan]\n",
      "['+   */', nan]\n",
      "['+  protected void dumpFileSystemStatistics(PrintStream stream) {', nan]\n",
      "['+    FileSystem fs = getFilesystem();', nan]\n",
      "['+    if (fs == null) {', nan]\n",
      "['+      return;', nan]\n",
      "['+    }', nan]\n",
      "['+    println(stream, \"%nStorage Statistics for %s%n\", fs.getUri());', nan]\n",
      "['+    StorageStatistics st = fs.getStorageStatistics();', nan]\n",
      "['+    Iterator<StorageStatistics.LongStatistic> it', nan]\n",
      "['+        = st.getLongStatistics();', nan]\n",
      "['+    while (it.hasNext()) {', nan]\n",
      "['+      StorageStatistics.LongStatistic next = it.next();', nan]\n",
      "['+      long value = next.getValue();', nan]\n",
      "['+      if (value != 0) {', nan]\n",
      "['+        println(stream, \"%s\\\\t%s\", next.getName(), value);', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+    println(stream, \"\");', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Create the metadata store.', nan]\n",
      "['*/', nan]\n",
      "['@@ -1167,16 +1214,20 @@ public int run(String[] args, PrintStream out) throws', nan]\n",
      "['* Get info about a bucket and its S3Guard integration status.', nan]\n",
      "['*/', nan]\n",
      "['public static class BucketInfo extends S3GuardTool {', nan]\n",
      "['-    public static final String NAME = \"bucket-info\";', nan]\n",
      "['+    public static final String BUCKET_INFO = \"bucket-info\";', nan]\n",
      "['+    public static final String NAME = BUCKET_INFO;', nan]\n",
      "['public static final String GUARDED_FLAG = \"guarded\";', nan]\n",
      "['public static final String UNGUARDED_FLAG = \"unguarded\";', nan]\n",
      "['public static final String AUTH_FLAG = \"auth\";', nan]\n",
      "['public static final String NONAUTH_FLAG = \"nonauth\";', nan]\n",
      "['public static final String ENCRYPTION_FLAG = \"encryption\";', nan]\n",
      "['public static final String MAGIC_FLAG = \"magic\";', nan]\n",
      "['+    public static final String MARKERS_FLAG = \"markers\";', nan]\n",
      "['+    public static final String MARKERS_AWARE = \"aware\";', nan]\n",
      "[nan, nan]\n",
      "['public static final String PURPOSE = \"provide/check S3Guard information\"', nan]\n",
      "['+ \" about a specific bucket\";', nan]\n",
      "['+', nan]\n",
      "['private static final String USAGE = NAME + \" [OPTIONS] s3a://BUCKET\\\\n\"', nan]\n",
      "['+ \"\\\\t\" + PURPOSE + \"\\\\n\\\\n\"', nan]\n",
      "['+ \"Common options:\\\\n\"', nan]\n",
      "['@@ -1186,7 +1237,9 @@ public int run(String[] args, PrintStream out) throws', nan]\n",
      "['+ \"  -\" + NONAUTH_FLAG + \" - Require the S3Guard mode to be \\\\\"non-authoritative\\\\\"\\\\n\"', nan]\n",
      "['+ \"  -\" + MAGIC_FLAG + \" - Require the S3 filesystem to be support the \\\\\"magic\\\\\" committer\\\\n\"', nan]\n",
      "['+ \"  -\" + ENCRYPTION_FLAG', nan]\n",
      "['-        + \" -require {none, sse-s3, sse-kms} - Require encryption policy\";', nan]\n",
      "['+        + \" (none, sse-s3, sse-kms) - Require encryption policy\\\\n\"', nan]\n",
      "['+        + \"  -\" + MARKERS_FLAG', nan]\n",
      "['+        + \" (aware, keep, delete, authoritative) - directory markers policy\\\\n\";', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* Output when the client cannot get the location of a bucket.', nan]\n",
      "['@@ -1196,10 +1249,17 @@ public int run(String[] args, PrintStream out) throws', nan]\n",
      "['\"Location unknown -caller lacks \"', nan]\n",
      "['+ RolePolicies.S3_GET_BUCKET_LOCATION + \" permission\";', nan]\n",
      "[nan, nan]\n",
      "['+', nan]\n",
      "['+    @VisibleForTesting', nan]\n",
      "['+    public static final String IS_MARKER_AWARE =', nan]\n",
      "['+        \"The S3A connector is compatible with buckets where\"', nan]\n",
      "['+            + \" directory markers are not deleted\";', nan]\n",
      "['+', nan]\n",
      "['public BucketInfo(Configuration conf) {', nan]\n",
      "['super(conf, GUARDED_FLAG, UNGUARDED_FLAG, AUTH_FLAG, NONAUTH_FLAG, MAGIC_FLAG);', nan]\n",
      "['CommandFormat format = getCommandFormat();', nan]\n",
      "['format.addOptionWithValue(ENCRYPTION_FLAG);', nan]\n",
      "['+      format.addOptionWithValue(MARKERS_FLAG);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Override', nan]\n",
      "['@@ -1384,10 +1444,57 @@ public int run(String[] args, PrintStream out)', nan]\n",
      "['fsUri, desiredEncryption, encryption);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+      // directory markers', nan]\n",
      "['+      processMarkerOption(out, fs,', nan]\n",
      "['+          getCommandFormat().getOptValue(MARKERS_FLAG));', nan]\n",
      "['+', nan]\n",
      "['+      // and finally flush the output and report a success.', nan]\n",
      "['out.flush();', nan]\n",
      "['return SUCCESS;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+    /**', nan]\n",
      "['+     * Validate the marker options.', nan]\n",
      "['+     * @param out output stream', nan]\n",
      "['+     * @param fs filesystem', nan]\n",
      "['+     * @param path test path', nan]\n",
      "['+     * @param marker desired marker option -may be null.', nan]\n",
      "['+     */', nan]\n",
      "['+    private void processMarkerOption(final PrintStream out,', nan]\n",
      "['+        final S3AFileSystem fs,', nan]\n",
      "['+        final String marker) {', nan]\n",
      "['+      DirectoryPolicy markerPolicy = fs.getDirectoryMarkerPolicy();', nan]\n",
      "['+      String desc = markerPolicy.describe();', nan]\n",
      "['+      println(out, \"%nThe directory marker policy is \\\\\"%s\\\\\"%n\", desc);', nan]\n",
      "['+', nan]\n",
      "['+      DirectoryPolicy.MarkerPolicy mp = markerPolicy.getMarkerPolicy();', nan]\n",
      "['+', nan]\n",
      "['+      String desiredMarker = marker == null', nan]\n",
      "['+          ? \"\"', nan]\n",
      "['+          : marker.trim();', nan]\n",
      "['+      final String optionName = mp.getOptionName();', nan]\n",
      "['+      if (!desiredMarker.isEmpty()) {', nan]\n",
      "['+        if (MARKERS_AWARE.equalsIgnoreCase(desiredMarker)) {', nan]\n",
      "['+          // simple awareness test -provides a way to validate compatibility', nan]\n",
      "['+          // on the command line', nan]\n",
      "['+          println(out, IS_MARKER_AWARE);', nan]\n",
      "['+          String pols = DirectoryPolicyImpl.availablePolicies()', nan]\n",
      "['+              .stream()', nan]\n",
      "['+              .map(DirectoryPolicy.MarkerPolicy::getOptionName)', nan]\n",
      "['+              .collect(Collectors.joining(\", \"));', nan]\n",
      "['+          println(out, \"Available Policies: %s\", pols);', nan]\n",
      "['+', nan]\n",
      "['+        } else {', nan]\n",
      "['+          // compare with current policy', nan]\n",
      "['+          if (!optionName.equalsIgnoreCase(desiredMarker)) {', nan]\n",
      "['+            throw badState(\"Bucket %s: required marker policy is \\\\\"%s\\\\\"\"', nan]\n",
      "['+                    + \" but actual policy is \\\\\"%s\\\\\"\",', nan]\n",
      "['+                fs.getUri(), desiredMarker, optionName);', nan]\n",
      "['+          }', nan]\n",
      "['+        }', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['private String printOption(PrintStream out,', nan]\n",
      "['String description, String key, String defVal) {', nan]\n",
      "['String t = getFilesystem().getConf().getTrimmed(key, defVal);', nan]\n",
      "['@@ -1991,6 +2098,9 @@ public static int run(Configuration conf, String...args) throws', nan]\n",
      "['case Diff.NAME:', nan]\n",
      "['command = new Diff(conf);', nan]\n",
      "['break;', nan]\n",
      "['+    case MarkerTool.MARKERS:', nan]\n",
      "['+      command = new MarkerTool(conf);', nan]\n",
      "['+      break;', nan]\n",
      "['case Prune.NAME:', nan]\n",
      "['command = new Prune(conf);', nan]\n",
      "['break;', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java b/hadoop-', 'ools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..6855c52edbb', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java', nan]\n",
      "['@@ -0,0 +1,723 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.tools;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.FileNotFoundException;', nan]\n",
      "['+import java.io.FileOutputStream;', nan]\n",
      "['+import java.io.IOException;', nan]\n",
      "['+import java.io.OutputStreamWriter;', nan]\n",
      "['+import java.io.PrintStream;', nan]\n",
      "['+import java.io.Writer;', nan]\n",
      "['+import java.net.URI;', nan]\n",
      "['+import java.nio.charset.StandardCharsets;', nan]\n",
      "['+import java.util.ArrayList;', nan]\n",
      "['+import java.util.Collections;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+import java.util.Map;', nan]\n",
      "['+import java.util.stream.Collectors;', nan]\n",
      "['+', nan]\n",
      "['+import com.amazonaws.AmazonClientException;', nan]\n",
      "['+import com.amazonaws.services.s3.model.DeleteObjectsRequest;', nan]\n",
      "['+import com.amazonaws.services.s3.model.MultiObjectDeleteException;', nan]\n",
      "['+import com.google.common.annotations.VisibleForTesting;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.commons.io.IOUtils;', nan]\n",
      "['+import org.apache.hadoop.classification.InterfaceAudience;', nan]\n",
      "['+import org.apache.hadoop.classification.InterfaceStability;', nan]\n",
      "['+import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['+import org.apache.hadoop.fs.FileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.RemoteIterator;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.Retries;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileStatus;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3ALocatedFileStatus;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.UnknownStoreException;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.DirMarkerTracker;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.DirectoryPolicy;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.StoreContext;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.s3guard.S3GuardTool;', nan]\n",
      "['+import org.apache.hadoop.fs.shell.CommandFormat;', nan]\n",
      "['+import org.apache.hadoop.util.DurationInfo;', nan]\n",
      "['+import org.apache.hadoop.util.ExitUtil;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.AUTHORITATIVE_PATH;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.BULK_DELETE_PAGE_SIZE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.BULK_DELETE_PAGE_SIZE_DEFAULT;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Invoker.once;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_INTERRUPTED;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_NOT_ACCEPTABLE;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_NOT_FOUND;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_SUCCESS;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_USAGE;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Audit and S3 bucket for directory markers.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * This tool does not go anywhere near S3Guard; its scan bypasses any', nan]\n",
      "['+ * metastore as we are explicitly looking for marker objects.', nan]\n",
      "['+ */', nan]\n",
      "['+@InterfaceAudience.LimitedPrivate(\"management tools\")', nan]\n",
      "['+@InterfaceStability.Unstable', nan]\n",
      "['+public final class MarkerTool extends S3GuardTool {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG = LoggerFactory.getLogger(MarkerTool.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Name of this tool: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String MARKERS = \"markers\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Purpose of this tool: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String PURPOSE =', nan]\n",
      "['+      \"View and manipulate S3 directory markers\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Audit sub-command: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String OPT_AUDIT = \"audit\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Clean Sub-command: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String OPT_CLEAN = \"clean\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Audit sub-command: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String AUDIT = \"-\" + OPT_AUDIT;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Clean Sub-command: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String CLEAN = \"-\" + OPT_CLEAN;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Expected number of markers to find: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String OPT_EXPECTED = \"expected\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Name of a file to save the list of markers to: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String OPT_OUT = \"out\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Limit of objects to scan: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String OPT_LIMIT = \"limit\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Only consider markers found in non-authoritative paths', nan]\n",
      "['+   * as failures: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final String OPT_NONAUTH = \"nonauth\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Error text when too few arguments are found.', nan]\n",
      "['+   */', nan]\n",
      "['+  @VisibleForTesting', nan]\n",
      "['+  static final String E_ARGUMENTS = \"Wrong number of arguments: %d\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Constant to use when there is no limit on the number of', nan]\n",
      "['+   * objects listed: {@value}.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * The value is 0 and not -1 because it allows for the limit to be', nan]\n",
      "['+   * set on the command line {@code -limit 0}.', nan]\n",
      "['+   * The command line parser rejects {@code -limit -1} as the -1', nan]\n",
      "['+   * is interpreted as the (unknown) option \"-1\".', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final int UNLIMITED_LISTING = 0;', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Usage string: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final String USAGE = MARKERS', nan]\n",
      "['+      + \" (-\" + OPT_AUDIT', nan]\n",
      "['+      + \" | -\" + OPT_CLEAN + \")\"', nan]\n",
      "['+      + \" [-\" + OPT_EXPECTED + \" <count>]\"', nan]\n",
      "['+      + \" [-\" + OPT_OUT + \" <filename>]\"', nan]\n",
      "['+      + \" [-\" + OPT_LIMIT + \" <limit>]\"', nan]\n",
      "['+      + \" [-\" + OPT_NONAUTH + \"]\"', nan]\n",
      "['+      + \" [-\" + VERBOSE + \"]\"', nan]\n",
      "['+', nan]\n",
      "['+      + \" <PATH>\\\\n\"', nan]\n",
      "['+      + \"\\\\t\" + PURPOSE + \"\\\\n\\\\n\";', nan]\n",
      "['+', nan]\n",
      "['+  /** Will be overridden in run(), but during tests needs to avoid NPEs. */', nan]\n",
      "['+  private PrintStream out = System.out;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Verbosity flag.', nan]\n",
      "['+   */', nan]\n",
      "['+  private boolean verbose;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Store context.', nan]\n",
      "['+   */', nan]\n",
      "['+  private StoreContext storeContext;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Operations during the scan.', nan]\n",
      "['+   */', nan]\n",
      "['+  private MarkerToolOperations operations;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Constructor.', nan]\n",
      "['+   * @param conf configuration', nan]\n",
      "['+   */', nan]\n",
      "['+  public MarkerTool(final Configuration conf) {', nan]\n",
      "['+    super(conf,', nan]\n",
      "['+        OPT_AUDIT,', nan]\n",
      "['+        OPT_CLEAN,', nan]\n",
      "['+        VERBOSE,', nan]\n",
      "['+        OPT_NONAUTH);', nan]\n",
      "['+    CommandFormat format = getCommandFormat();', nan]\n",
      "['+    format.addOptionWithValue(OPT_EXPECTED);', nan]\n",
      "['+    format.addOptionWithValue(OPT_LIMIT);', nan]\n",
      "['+    format.addOptionWithValue(OPT_OUT);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public String getUsage() {', nan]\n",
      "['+    return USAGE;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public String getName() {', nan]\n",
      "['+    return MARKERS;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void resetBindings() {', nan]\n",
      "['+    super.resetBindings();', nan]\n",
      "['+    storeContext = null;', nan]\n",
      "['+    operations = null;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public int run(final String[] args, final PrintStream stream)', nan]\n",
      "['+      throws ExitUtil.ExitException, Exception {', nan]\n",
      "['+    this.out = stream;', nan]\n",
      "['+    final List<String> parsedArgs;', nan]\n",
      "['+    try {', nan]\n",
      "['+      parsedArgs = parseArgs(args);', nan]\n",
      "['+    } catch (CommandFormat.UnknownOptionException e) {', nan]\n",
      "['+      errorln(getUsage());', nan]\n",
      "['+      throw new ExitUtil.ExitException(EXIT_USAGE, e.getMessage(), e);', nan]\n",
      "['+    }', nan]\n",
      "['+    if (parsedArgs.size() != 1) {', nan]\n",
      "['+      errorln(getUsage());', nan]\n",
      "['+      println(out, \"Supplied arguments: [\"', nan]\n",
      "['+          + parsedArgs.stream()', nan]\n",
      "['+          .collect(Collectors.joining(\", \"))', nan]\n",
      "['+          + \"]\");', nan]\n",
      "['+      throw new ExitUtil.ExitException(EXIT_USAGE,', nan]\n",
      "['+          String.format(E_ARGUMENTS, parsedArgs.size()));', nan]\n",
      "['+    }', nan]\n",
      "['+    // read arguments', nan]\n",
      "['+    CommandFormat command = getCommandFormat();', nan]\n",
      "['+    verbose = command.getOpt(VERBOSE);', nan]\n",
      "['+', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+    // How many markers are expected?', nan]\n",
      "['+    int expected = 0;', nan]\n",
      "['+    String value = command.getOptValue(OPT_EXPECTED);', nan]\n",
      "['+    if (value != null && !value.isEmpty()) {', nan]\n",
      "['+      expected = Integer.parseInt(value);', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    // determine the action', nan]\n",
      "['+    boolean audit = command.getOpt(OPT_AUDIT);', nan]\n",
      "['+    boolean clean = command.getOpt(OPT_CLEAN);', nan]\n",
      "['+    if (audit == clean) {', nan]\n",
      "['+      // either both are set or neither are set', nan]\n",
      "['+      // this is equivalent to (not audit xor clean)', nan]\n",
      "['+      errorln(getUsage());', nan]\n",
      "['+      throw new ExitUtil.ExitException(EXIT_USAGE,', nan]\n",
      "['+          \"Exactly one of \" + AUDIT + \" and \" + CLEAN);', nan]\n",
      "['+    }', nan]\n",
      "['+    int limit = UNLIMITED_LISTING;', nan]\n",
      "['+    value = command.getOptValue(OPT_LIMIT);', nan]\n",
      "['+    if (value != null && !value.isEmpty()) {', nan]\n",
      "['+      limit = Integer.parseInt(value);', nan]\n",
      "['+    }', nan]\n",
      "['+    final String dir = parsedArgs.get(0);', nan]\n",
      "['+    Path path = new Path(dir);', nan]\n",
      "['+    URI uri = path.toUri();', nan]\n",
      "['+    if (uri.getPath().isEmpty()) {', nan]\n",
      "['+      // fix up empty URI for better CLI experience', nan]\n",
      "['+      path = new Path(path, \"/\");', nan]\n",
      "['+    }', nan]\n",
      "['+    FileSystem fs = path.getFileSystem(getConf());', nan]\n",
      "['+    ScanResult result = execute(', nan]\n",
      "['+        fs,', nan]\n",
      "['+        path,', nan]\n",
      "['+        clean,', nan]\n",
      "['+        expected,', nan]\n",
      "['+        limit,', nan]\n",
      "['+        command.getOpt(OPT_NONAUTH));', nan]\n",
      "['+    if (verbose) {', nan]\n",
      "['+      dumpFileSystemStatistics(out);', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    // and finally see if the output should be saved to a file', nan]\n",
      "['+    String saveFile = command.getOptValue(OPT_OUT);', nan]\n",
      "['+    if (saveFile != null && !saveFile.isEmpty()) {', nan]\n",
      "['+      println(out, \"Saving result to %s\", saveFile);', nan]\n",
      "['+      try (Writer writer =', nan]\n",
      "['+               new OutputStreamWriter(', nan]\n",
      "['+                   new FileOutputStream(saveFile),', nan]\n",
      "['+                   StandardCharsets.UTF_8)) {', nan]\n",
      "['+        final List<String> surplus = result.getTracker()', nan]\n",
      "['+            .getSurplusMarkers()', nan]\n",
      "['+            .keySet()', nan]\n",
      "['+            .stream()', nan]\n",
      "['+            .map(p-> p.toString() + \"/\")', nan]\n",
      "['+            .sorted()', nan]\n",
      "['+            .collect(Collectors.toList());', nan]\n",
      "['+        IOUtils.writeLines(surplus, \"\\\\n\", writer);', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+    return result.exitCode;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute the scan/purge.', nan]\n",
      "['+   * @param sourceFS source FS; must be or wrap an S3A FS.', nan]\n",
      "['+   * @param path path to scan.', nan]\n",
      "['+   * @param doPurge purge?', nan]\n",
      "['+   * @param expectedMarkerCount expected marker count', nan]\n",
      "[\"+   * @param limit limit of files to scan; -1 for 'unlimited'\", nan]\n",
      "['+   * @param nonAuth consider only markers in nonauth paths as errors', nan]\n",
      "['+   * @return scan+purge result.', nan]\n",
      "['+   * @throws IOException failure', nan]\n",
      "['+   */', nan]\n",
      "['+  @VisibleForTesting', nan]\n",
      "['+  ScanResult execute(', nan]\n",
      "['+      final FileSystem sourceFS,', nan]\n",
      "['+      final Path path,', nan]\n",
      "['+      final boolean doPurge,', nan]\n",
      "['+      final int expectedMarkerCount,', nan]\n",
      "['+      final int limit,', nan]\n",
      "['+      final boolean nonAuth)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    S3AFileSystem fs = bindFilesystem(sourceFS);', nan]\n",
      "['+', nan]\n",
      "['+    // extract the callbacks needed for the rest of the work', nan]\n",
      "['+    storeContext = fs.createStoreContext();', nan]\n",
      "['+    operations = fs.createMarkerToolOperations();', nan]\n",
      "['+    // filesystem policy.', nan]\n",
      "['+    // if the -nonauth option is set, this is used to filter', nan]\n",
      "['+    // out surplus markers from the results.', nan]\n",
      "['+    DirectoryPolicy activePolicy = fs.getDirectoryMarkerPolicy();', nan]\n",
      "['+    DirectoryPolicy.MarkerPolicy policy = activePolicy', nan]\n",
      "['+        .getMarkerPolicy();', nan]\n",
      "['+    println(out, \"The directory marker policy of %s is \\\\\"%s\\\\\"\",', nan]\n",
      "['+        storeContext.getFsURI(),', nan]\n",
      "['+        policy);', nan]\n",
      "['+    String authPath = storeContext.getConfiguration()', nan]\n",
      "['+        .getTrimmed(AUTHORITATIVE_PATH, \"\");', nan]\n",
      "['+    if (policy == DirectoryPolicy.MarkerPolicy.Authoritative) {', nan]\n",
      "['+      // in auth mode, note the auth paths.', nan]\n",
      "['+      println(out, \"Authoritative path list is \\\\\"%s\\\\\"\", authPath);', nan]\n",
      "['+    }', nan]\n",
      "['+    // qualify the path', nan]\n",
      "['+    Path target = path.makeQualified(fs.getUri(), new Path(\"/\"));', nan]\n",
      "['+    // initial safety check: does the path exist?', nan]\n",
      "['+    try {', nan]\n",
      "['+      getFilesystem().getFileStatus(target);', nan]\n",
      "['+    } catch (UnknownStoreException ex) {', nan]\n",
      "[\"+      // bucket doesn't exist.\", nan]\n",
      "['+      // replace the stack trace with an error code.', nan]\n",
      "['+      throw new ExitUtil.ExitException(EXIT_NOT_FOUND,', nan]\n",
      "['+          ex.toString(), ex);', nan]\n",
      "['+', nan]\n",
      "['+    } catch (FileNotFoundException ex) {', nan]\n",
      "['+      throw new ExitUtil.ExitException(EXIT_NOT_FOUND,', nan]\n",
      "['+          \"Not found: \" + target, ex);', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    // the default filter policy is that all entries should be deleted', nan]\n",
      "['+    DirectoryPolicy filterPolicy = nonAuth', nan]\n",
      "['+        ? activePolicy', nan]\n",
      "['+        : null;', nan]\n",
      "['+    ScanResult result = scan(target, doPurge, expectedMarkerCount, limit,', nan]\n",
      "['+        filterPolicy);', nan]\n",
      "['+    return result;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Result of the scan operation.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final class ScanResult {', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Exit code to return if an exception was not raised.', nan]\n",
      "['+     */', nan]\n",
      "['+    private int exitCode;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * The tracker.', nan]\n",
      "['+     */', nan]\n",
      "['+    private DirMarkerTracker tracker;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Scan summary.', nan]\n",
      "['+     */', nan]\n",
      "['+    private MarkerPurgeSummary purgeSummary;', nan]\n",
      "['+', nan]\n",
      "['+    private ScanResult() {', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public String toString() {', nan]\n",
      "['+      return \"ScanResult{\" +', nan]\n",
      "['+          \"exitCode=\" + exitCode +', nan]\n",
      "['+          \", tracker=\" + tracker +', nan]\n",
      "['+          \", purgeSummary=\" + purgeSummary +', nan]\n",
      "[\"+          '}';\", nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /** Exit code to report. */', nan]\n",
      "['+    public int getExitCode() {', nan]\n",
      "['+      return exitCode;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /** Tracker which did the scan. */', nan]\n",
      "['+    public DirMarkerTracker getTracker() {', nan]\n",
      "['+      return tracker;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /** Summary of purge. Null if none took place. */', nan]\n",
      "['+    public MarkerPurgeSummary getPurgeSummary() {', nan]\n",
      "['+      return purgeSummary;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Do the scan/purge.', nan]\n",
      "['+   * @param path path to scan.', nan]\n",
      "['+   * @param clean purge?', nan]\n",
      "['+   * @param expectedMarkerCount expected marker count', nan]\n",
      "[\"+   * @param limit limit of files to scan; 0 for 'unlimited'\", nan]\n",
      "['+   * @param filterPolicy filter policy on a nonauth scan; may be null', nan]\n",
      "['+   * @return result.', nan]\n",
      "['+   * @throws IOException IO failure', nan]\n",
      "['+   * @throws ExitUtil.ExitException explicitly raised failure', nan]\n",
      "['+   */', nan]\n",
      "['+  @Retries.RetryTranslated', nan]\n",
      "['+  private ScanResult scan(', nan]\n",
      "['+      final Path path,', nan]\n",
      "['+      final boolean clean,', nan]\n",
      "['+      final int expectedMarkerCount,', nan]\n",
      "['+      final int limit,', nan]\n",
      "['+      final DirectoryPolicy filterPolicy)', nan]\n",
      "['+      throws IOException, ExitUtil.ExitException {', nan]\n",
      "['+', nan]\n",
      "['+    ScanResult result = new ScanResult();', nan]\n",
      "['+', nan]\n",
      "['+    // Mission Accomplished', nan]\n",
      "['+    result.exitCode = EXIT_SUCCESS;', nan]\n",
      "['+    // Now do the work.', nan]\n",
      "['+    DirMarkerTracker tracker = new DirMarkerTracker(path, true);', nan]\n",
      "['+    result.tracker = tracker;', nan]\n",
      "['+    boolean completed;', nan]\n",
      "['+    try (DurationInfo ignored =', nan]\n",
      "['+             new DurationInfo(LOG, \"marker scan %s\", path)) {', nan]\n",
      "['+      completed = scanDirectoryTree(path, tracker, limit);', nan]\n",
      "['+    }', nan]\n",
      "['+    int objectsFound = tracker.getObjectsFound();', nan]\n",
      "['+    println(out, \"Listed %d object%s under %s%n\",', nan]\n",
      "['+        objectsFound,', nan]\n",
      "['+        suffix(objectsFound),', nan]\n",
      "['+        path);', nan]\n",
      "['+    // scan done. what have we got?', nan]\n",
      "['+    Map<Path, DirMarkerTracker.Marker> surplusMarkers', nan]\n",
      "['+        = tracker.getSurplusMarkers();', nan]\n",
      "['+    Map<Path, DirMarkerTracker.Marker> leafMarkers', nan]\n",
      "['+        = tracker.getLeafMarkers();', nan]\n",
      "['+    int surplus = surplusMarkers.size();', nan]\n",
      "['+    if (surplus == 0) {', nan]\n",
      "['+      println(out, \"No surplus directory markers were found under %s\", path);', nan]\n",
      "['+    } else {', nan]\n",
      "['+      println(out, \"Found %d surplus directory marker%s under %s\",', nan]\n",
      "['+          surplus,', nan]\n",
      "['+          suffix(surplus),', nan]\n",
      "['+          path);', nan]\n",
      "['+', nan]\n",
      "['+      for (Path markers : surplusMarkers.keySet()) {', nan]\n",
      "['+        println(out, \"    %s/\", markers);', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+    if (!leafMarkers.isEmpty()) {', nan]\n",
      "['+      println(out, \"Found %d empty directory \\'leaf\\' marker%s under %s\",', nan]\n",
      "['+          leafMarkers.size(),', nan]\n",
      "['+          suffix(leafMarkers.size()),', nan]\n",
      "['+          path);', nan]\n",
      "['+      for (Path markers : leafMarkers.keySet()) {', nan]\n",
      "['+        println(out, \"    %s/\", markers);', nan]\n",
      "['+      }', nan]\n",
      "['+      println(out, \"These are required to indicate empty directories\");', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    if (clean) {', nan]\n",
      "['+      // clean: remove the markers, do not worry about their', nan]\n",
      "['+      // presence when reporting success/failiure', nan]\n",
      "['+      int deletePageSize = storeContext.getConfiguration()', nan]\n",
      "['+          .getInt(BULK_DELETE_PAGE_SIZE,', nan]\n",
      "['+              BULK_DELETE_PAGE_SIZE_DEFAULT);', nan]\n",
      "['+      result.purgeSummary = purgeMarkers(tracker, deletePageSize);', nan]\n",
      "['+    } else {', nan]\n",
      "['+      // this is an audit, so validate the marker count', nan]\n",
      "['+', nan]\n",
      "['+      if (filterPolicy != null) {', nan]\n",
      "['+        // if a filter policy is supplied, filter out all markers', nan]\n",
      "['+        // under the auth path', nan]\n",
      "['+        List<Path> allowed = tracker.removeAllowedMarkers(filterPolicy);', nan]\n",
      "['+        int allowedMarkers =  allowed.size();', nan]\n",
      "['+        println(out, \"%nIgnoring %d marker%s in authoritative paths\",', nan]\n",
      "['+            allowedMarkers, suffix(allowedMarkers));', nan]\n",
      "['+        if (verbose) {', nan]\n",
      "['+          allowed.forEach(p -> println(out, p.toString()));', nan]\n",
      "['+        }', nan]\n",
      "['+        // recalculate the marker size', nan]\n",
      "['+        surplus = surplusMarkers.size();', nan]\n",
      "['+      }', nan]\n",
      "['+      if (surplus > expectedMarkerCount) {', nan]\n",
      "['+        // failure', nan]\n",
      "['+        if (expectedMarkerCount > 0) {', nan]\n",
      "['+          println(out, \"Expected %d marker%s\", expectedMarkerCount,', nan]\n",
      "['+              suffix(surplus));', nan]\n",
      "['+        }', nan]\n",
      "['+        println(out, \"Surplus markers were found -failing audit\");', nan]\n",
      "['+', nan]\n",
      "['+        result.exitCode = EXIT_NOT_ACCEPTABLE;', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    // now one little check for whether a limit was reached.', nan]\n",
      "['+    if (!completed) {', nan]\n",
      "['+      println(out, \"Listing limit reached before completing the scan\");', nan]\n",
      "['+      result.exitCode = EXIT_INTERRUPTED;', nan]\n",
      "['+    }', nan]\n",
      "['+    return result;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Suffix for plurals.', nan]\n",
      "['+   * @param size size to generate a suffix for', nan]\n",
      "['+   * @return \"\" or \"s\", depending on size', nan]\n",
      "['+   */', nan]\n",
      "['+  private String suffix(final int size) {', nan]\n",
      "['+    return size == 1 ? \"\" : \"s\";', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Scan a directory tree.', nan]\n",
      "['+   * @param path path to scan', nan]\n",
      "['+   * @param tracker tracker to update', nan]\n",
      "[\"+   * @param limit limit of files to scan; -1 for 'unlimited'\", nan]\n",
      "['+   * @return true if the scan completedly scanned the entire tree', nan]\n",
      "['+   * @throws IOException IO failure', nan]\n",
      "['+   */', nan]\n",
      "['+  @Retries.RetryTranslated', nan]\n",
      "['+  private boolean scanDirectoryTree(', nan]\n",
      "['+      final Path path,', nan]\n",
      "['+      final DirMarkerTracker tracker,', nan]\n",
      "['+      final int limit) throws IOException {', nan]\n",
      "['+', nan]\n",
      "['+    int count = 0;', nan]\n",
      "['+    RemoteIterator<S3AFileStatus> listing = operations', nan]\n",
      "['+        .listObjects(path, storeContext.pathToKey(path));', nan]\n",
      "['+    while (listing.hasNext()) {', nan]\n",
      "['+      count++;', nan]\n",
      "['+      S3AFileStatus status = listing.next();', nan]\n",
      "['+      Path statusPath = status.getPath();', nan]\n",
      "['+      S3ALocatedFileStatus locatedStatus = new S3ALocatedFileStatus(', nan]\n",
      "['+          status, null);', nan]\n",
      "['+      String key = storeContext.pathToKey(statusPath);', nan]\n",
      "['+      if (status.isDirectory()) {', nan]\n",
      "['+        if (verbose) {', nan]\n",
      "['+          println(out, \"  Directory Marker %s/\", key);', nan]\n",
      "['+        }', nan]\n",
      "['+        LOG.debug(\"{}\", key);', nan]\n",
      "['+        tracker.markerFound(statusPath,', nan]\n",
      "['+            key + \"/\",', nan]\n",
      "['+            locatedStatus);', nan]\n",
      "['+      } else {', nan]\n",
      "['+        tracker.fileFound(statusPath,', nan]\n",
      "['+            key,', nan]\n",
      "['+            locatedStatus);', nan]\n",
      "['+      }', nan]\n",
      "['+      if ((count % 1000) == 0) {', nan]\n",
      "['+        println(out, \"Scanned %,d objects\", count);', nan]\n",
      "['+      }', nan]\n",
      "['+      if (limit > 0 && count >= limit) {', nan]\n",
      "['+        println(out, \"Limit of scan reached - %,d object%s\",', nan]\n",
      "['+            limit, suffix(limit));', nan]\n",
      "['+        return false;', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['+    return true;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Result of a call of {@link #purgeMarkers(DirMarkerTracker, int)};', nan]\n",
      "['+   * included in {@link ScanResult} so must share visibility.', nan]\n",
      "['+   */', nan]\n",
      "['+  static final class MarkerPurgeSummary {', nan]\n",
      "['+', nan]\n",
      "['+    /** Number of markers deleted. */', nan]\n",
      "['+    private int markersDeleted;', nan]\n",
      "['+', nan]\n",
      "['+    /** Number of delete requests issued. */', nan]\n",
      "['+    private int deleteRequests;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Total duration of delete requests.', nan]\n",
      "['+     * If this is ever parallelized, this will', nan]\n",
      "['+     * be greater than the elapsed time of the', nan]\n",
      "['+     * operation.', nan]\n",
      "['+     */', nan]\n",
      "['+    private long totalDeleteRequestDuration;', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public String toString() {', nan]\n",
      "['+      return \"MarkerPurgeSummary{\" +', nan]\n",
      "['+          \"markersDeleted=\" + markersDeleted +', nan]\n",
      "['+          \", deleteRequests=\" + deleteRequests +', nan]\n",
      "['+          \", totalDeleteRequestDuration=\" + totalDeleteRequestDuration +', nan]\n",
      "[\"+          '}';\", nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+    int getMarkersDeleted() {', nan]\n",
      "['+      return markersDeleted;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    int getDeleteRequests() {', nan]\n",
      "['+      return deleteRequests;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    long getTotalDeleteRequestDuration() {', nan]\n",
      "['+      return totalDeleteRequestDuration;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Purge the markers.', nan]\n",
      "['+   * @param tracker tracker with the details', nan]\n",
      "['+   * @param deletePageSize page size of deletes', nan]\n",
      "['+   * @return summary', nan]\n",
      "['+   * @throws IOException IO failure', nan]\n",
      "['+   */', nan]\n",
      "['+  @Retries.RetryTranslated', nan]\n",
      "['+  private MarkerPurgeSummary purgeMarkers(', nan]\n",
      "['+      final DirMarkerTracker tracker,', nan]\n",
      "['+      final int deletePageSize)', nan]\n",
      "['+      throws MultiObjectDeleteException, AmazonClientException, IOException {', nan]\n",
      "['+', nan]\n",
      "['+    MarkerPurgeSummary summary = new MarkerPurgeSummary();', nan]\n",
      "['+    // we get a map of surplus markers to delete.', nan]\n",
      "['+    Map<Path, DirMarkerTracker.Marker> markers', nan]\n",
      "['+        = tracker.getSurplusMarkers();', nan]\n",
      "['+    int size = markers.size();', nan]\n",
      "['+    // build a list from the strings in the map', nan]\n",
      "['+    List<DeleteObjectsRequest.KeyVersion> collect =', nan]\n",
      "['+        markers.values().stream()', nan]\n",
      "['+            .map(p -> new DeleteObjectsRequest.KeyVersion(p.getKey()))', nan]\n",
      "['+            .collect(Collectors.toList());', nan]\n",
      "['+    // build an array list for ease of creating the lists of', nan]\n",
      "['+    // keys in each page through the subList() method.', nan]\n",
      "['+    List<DeleteObjectsRequest.KeyVersion> markerKeys =', nan]\n",
      "['+        new ArrayList<>(collect);', nan]\n",
      "['+', nan]\n",
      "['+    // now randomize. Why so? if the list spans multiple S3 partitions,', nan]\n",
      "['+    // it should reduce the IO load on each part.', nan]\n",
      "['+    Collections.shuffle(markerKeys);', nan]\n",
      "['+    int pages = size / deletePageSize;', nan]\n",
      "['+    if (size % deletePageSize > 0) {', nan]\n",
      "['+      pages += 1;', nan]\n",
      "['+    }', nan]\n",
      "['+    if (verbose) {', nan]\n",
      "['+      println(out, \"%n%d marker%s to delete in %d page%s of %d keys/page\",', nan]\n",
      "['+          size, suffix(size),', nan]\n",
      "['+          pages, suffix(pages),', nan]\n",
      "['+          deletePageSize);', nan]\n",
      "['+    }', nan]\n",
      "['+    DurationInfo durationInfo = new DurationInfo(LOG, \"Deleting markers\");', nan]\n",
      "['+    int start = 0;', nan]\n",
      "['+    while (start < size) {', nan]\n",
      "['+      // end is one past the end of the page', nan]\n",
      "['+      int end = Math.min(start + deletePageSize, size);', nan]\n",
      "['+      List<DeleteObjectsRequest.KeyVersion> page = markerKeys.subList(start,', nan]\n",
      "['+          end);', nan]\n",
      "['+      List<Path> undeleted = new ArrayList<>();', nan]\n",
      "['+      once(\"Remove S3 Keys\",', nan]\n",
      "['+          tracker.getBasePath().toString(), () ->', nan]\n",
      "['+              operations.removeKeys(page, true, undeleted, null, false));', nan]\n",
      "['+      summary.deleteRequests++;', nan]\n",
      "['+      // and move to the start of the next page', nan]\n",
      "['+      start = end;', nan]\n",
      "['+    }', nan]\n",
      "['+    durationInfo.close();', nan]\n",
      "['+    summary.totalDeleteRequestDuration = durationInfo.value();', nan]\n",
      "['+    summary.markersDeleted = size;', nan]\n",
      "['+    return summary;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public boolean isVerbose() {', nan]\n",
      "['+    return verbose;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public void setVerbose(final boolean verbose) {', nan]\n",
      "['+    this.verbose = verbose;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute the marker tool, with no checks on return codes.', nan]\n",
      "['+   *', nan]\n",
      "['+   * @param sourceFS filesystem to use', nan]\n",
      "['+   * @param path path to scan', nan]\n",
      "['+   * @param doPurge should markers be purged', nan]\n",
      "['+   * @param expectedMarkers number of markers expected', nan]\n",
      "[\"+   * @param limit limit of files to scan; -1 for 'unlimited'\", nan]\n",
      "['+   * @param nonAuth only use nonauth path count for failure rules', nan]\n",
      "['+   * @return the result', nan]\n",
      "['+   */', nan]\n",
      "['+  @SuppressWarnings(\"IOResourceOpenedButNotSafelyClosed\")', nan]\n",
      "['+  public static MarkerTool.ScanResult execMarkerTool(', nan]\n",
      "['+      final FileSystem sourceFS,', nan]\n",
      "['+      final Path path,', nan]\n",
      "['+      final boolean doPurge,', nan]\n",
      "['+      final int expectedMarkers,', nan]\n",
      "['+      final int limit, boolean nonAuth) throws IOException {', nan]\n",
      "['+    MarkerTool tool = new MarkerTool(sourceFS.getConf());', nan]\n",
      "['+    tool.setVerbose(LOG.isDebugEnabled());', nan]\n",
      "['+', nan]\n",
      "['+    return tool.execute(sourceFS, path, doPurge,', nan]\n",
      "['+        expectedMarkers, limit, nonAuth);', nan]\n",
      "['+  }', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerToolOperations.java', 'b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerToolOperations.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..9ab7636d6c9', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerToolOperations.java', nan]\n",
      "['@@ -0,0 +1,91 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.tools;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.IOException;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+', nan]\n",
      "['+import com.amazonaws.AmazonClientException;', nan]\n",
      "['+import com.amazonaws.services.s3.model.DeleteObjectsRequest;', nan]\n",
      "['+import com.amazonaws.services.s3.model.DeleteObjectsResult;', nan]\n",
      "['+import com.amazonaws.services.s3.model.MultiObjectDeleteException;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.InvalidRequestException;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.RemoteIterator;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.Retries;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileStatus;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.s3guard.BulkOperationState;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Operations which must be offered by the store for {@link MarkerTool}.', nan]\n",
      "['+ * These are a proper subset of {@code OperationCallbacks}; this interface', nan]\n",
      "['+ * strips down those provided to the tool.', nan]\n",
      "['+ */', nan]\n",
      "['+public interface MarkerToolOperations {', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create an iterator over objects in S3 only; S3Guard', nan]\n",
      "['+   * is not involved.', nan]\n",
      "['+   * The listing includes the key itself, if found.', nan]\n",
      "['+   * @param path  path of the listing.', nan]\n",
      "['+   * @param key object key', nan]\n",
      "['+   * @return iterator with the first listing completed.', nan]\n",
      "['+   * @throws IOException failure.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Retries.RetryTranslated', nan]\n",
      "['+  RemoteIterator<S3AFileStatus> listObjects(', nan]\n",
      "['+      Path path,', nan]\n",
      "['+      String key)', nan]\n",
      "['+      throws IOException;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Remove keys from the store, updating the metastore on a', nan]\n",
      "['+   * partial delete represented as a MultiObjectDeleteException failure by', nan]\n",
      "['+   * deleting all those entries successfully deleted and then rethrowing', nan]\n",
      "['+   * the MultiObjectDeleteException.', nan]\n",
      "['+   * @param keysToDelete collection of keys to delete on the s3-backend.', nan]\n",
      "['+   *        if empty, no request is made of the object store.', nan]\n",
      "['+   * @param deleteFakeDir indicates whether this is for deleting fake dirs.', nan]\n",
      "['+   * @param undeletedObjectsOnFailure List which will be built up of all', nan]\n",
      "['+   * files that were not deleted. This happens even as an exception', nan]\n",
      "['+   * is raised.', nan]\n",
      "['+   * @param operationState bulk operation state', nan]\n",
      "['+   * @param quiet should a bulk query be quiet, or should its result list', nan]\n",
      "['+   * all deleted keys', nan]\n",
      "['+   * @return the deletion result if a multi object delete was invoked', nan]\n",
      "['+   * and it returned without a failure, else null.', nan]\n",
      "['+   * @throws InvalidRequestException if the request was rejected due to', nan]\n",
      "['+   * a mistaken attempt to delete the root directory.', nan]\n",
      "['+   * @throws MultiObjectDeleteException one or more of the keys could not', nan]\n",
      "['+   * be deleted in a multiple object delete operation.', nan]\n",
      "['+   * @throws AmazonClientException amazon-layer failure.', nan]\n",
      "['+   * @throws IOException other IO Exception.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Retries.RetryMixed', nan]\n",
      "['+  DeleteObjectsResult removeKeys(', nan]\n",
      "['+      List<DeleteObjectsRequest.KeyVersion> keysToDelete,', nan]\n",
      "['+      boolean deleteFakeDir,', nan]\n",
      "['+      List<Path> undeletedObjectsOnFailure,', nan]\n",
      "['+      BulkOperationState operationState,', nan]\n",
      "['+      boolean quiet)', nan]\n",
      "['+      throws MultiObjectDeleteException, AmazonClientException,', nan]\n",
      "['+             IOException;', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerToolOperationsImpl.', 'ava b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerToolOperationsI']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..d14bb6b1d8e', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerToolOperationsImpl.java', nan]\n",
      "['@@ -0,0 +1,70 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.tools;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.IOException;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+', nan]\n",
      "['+import com.amazonaws.AmazonClientException;', nan]\n",
      "['+import com.amazonaws.services.s3.model.DeleteObjectsRequest;', nan]\n",
      "['+import com.amazonaws.services.s3.model.DeleteObjectsResult;', nan]\n",
      "['+import com.amazonaws.services.s3.model.MultiObjectDeleteException;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.RemoteIterator;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileStatus;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.OperationCallbacks;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.s3guard.BulkOperationState;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Implement the marker tool operations by forwarding to the', nan]\n",
      "['+ * {@link OperationCallbacks} instance provided in the constructor.', nan]\n",
      "['+ */', nan]\n",
      "['+public class MarkerToolOperationsImpl implements MarkerToolOperations {', nan]\n",
      "['+', nan]\n",
      "['+  private final OperationCallbacks operationCallbacks;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Constructor.', nan]\n",
      "['+   * @param operations implementation of the operations', nan]\n",
      "['+   */', nan]\n",
      "['+  public MarkerToolOperationsImpl(final OperationCallbacks operations) {', nan]\n",
      "['+    this.operationCallbacks = operations;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public RemoteIterator<S3AFileStatus> listObjects(final Path path,', nan]\n",
      "['+      final String key)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    return operationCallbacks.listObjects(path, key);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public DeleteObjectsResult removeKeys(', nan]\n",
      "['+      final List<DeleteObjectsRequest.KeyVersion> keysToDelete,', nan]\n",
      "['+      final boolean deleteFakeDir,', nan]\n",
      "['+      final List<Path> undeletedObjectsOnFailure,', nan]\n",
      "['+      final BulkOperationState operationState,', nan]\n",
      "['+      final boolean quiet)', nan]\n",
      "['+      throws MultiObjectDeleteException, AmazonClientException, IOException {', nan]\n",
      "['+    return operationCallbacks.removeKeys(keysToDelete, deleteFakeDir,', nan]\n",
      "['+        undeletedObjectsOnFailure, operationState, quiet);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/package-info.java b/hadoo', '-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/package-info.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..cb3a3749b65', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/package-info.java', nan]\n",
      "['@@ -0,0 +1,27 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * S3A Command line tools independent of S3Guard.', nan]\n",
      "['+ */', nan]\n",
      "['+@InterfaceAudience.Private', nan]\n",
      "['+@InterfaceStability.Unstable', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.tools;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.classification.InterfaceAudience;', nan]\n",
      "['+import org.apache.hadoop.classification.InterfaceStability;', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/directory_markers.md b/hadoop-tools', 'hadoop-aws/src/site/markdown/tools/hadoop-aws/directory_markers.md']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..e9622ce906d', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/directory_markers.md', nan]\n",
      "['@@ -0,0 +1,694 @@', nan]\n",
      "['+<!---', nan]\n",
      "['+  Licensed under the Apache License, Version 2.0 (the \"License\");', nan]\n",
      "['+  you may not use this file except in compliance with the License.', nan]\n",
      "['+  You may obtain a copy of the License at', nan]\n",
      "['+', nan]\n",
      "['+   http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+', nan]\n",
      "['+  Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+  distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+  See the License for the specific language governing permissions and', nan]\n",
      "['+  limitations under the License. See accompanying LICENSE file.', nan]\n",
      "['+-->', nan]\n",
      "['+', nan]\n",
      "['+# Controlling the S3A Directory Marker Behavior', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"compatibility\"></a> Critical: this is not backwards compatible!', nan]\n",
      "['+', nan]\n",
      "['+This document shows how the performance of S3 I/O, especially applications', nan]\n",
      "['+creating many files (for example Apache Hive) or working with versioned S3 buckets can', nan]\n",
      "['+increase performance by changing the S3A directory marker retention policy.', nan]\n",
      "['+', nan]\n",
      "['+Changing the policy from the default value, `\"delete\"` _is not backwards compatible_.', nan]\n",
      "['+', nan]\n",
      "['+Versions of Hadoop which are incompatible with other marker retention policies,', nan]\n",
      "['+as of August 2020.', nan]\n",
      "['+', nan]\n",
      "['+-------------------------------------------------------', nan]\n",
      "['+|  Branch    | Compatible Since | Future Fix Planned? |', nan]\n",
      "['+|------------|------------------|---------------------|', nan]\n",
      "['+| Hadoop 2.x |                  | NO                  |', nan]\n",
      "['+| Hadoop 3.0 |                  | NO                  |', nan]\n",
      "['+| Hadoop 3.1 |      check       | Yes                 |', nan]\n",
      "['+| Hadoop 3.2 |      check       | Yes                 |', nan]\n",
      "['+| Hadoop 3.3 |      3.3.1       | Done                |', nan]\n",
      "['+-------------------------------------------------------', nan]\n",
      "['+', nan]\n",
      "['+The `s3guard bucket-info` tool [can be used to verify support](#bucket-info).', nan]\n",
      "['+This allows for a command line check of compatibility, including', nan]\n",
      "['+in scripts.', nan]\n",
      "['+', nan]\n",
      "['+External Hadoop-based applications should also be assumed to be incompatible', nan]\n",
      "['+unless otherwise stated/known.', nan]\n",
      "['+', nan]\n",
      "['+It is only safe change the directory marker policy if the following', nan]\n",
      "['+ conditions are met:', nan]\n",
      "['+', nan]\n",
      "['+1. You know exactly which applications are writing to and reading from', nan]\n",
      "['+   (including backing up) an S3 bucket.', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+2. You know all applications which read data from the bucket are compatible.', nan]\n",
      "['+', nan]\n",
      "['+### <a name=\"backups\"></a> Applications backing up data.', nan]\n",
      "['+', nan]\n",
      "['+It is not enough to have a version of Apache Hadoop which is compatible, any', nan]\n",
      "['+application which backs up an S3 bucket or copies elsewhere must have an S3', nan]\n",
      "['+connector which is compatible. For the Hadoop codebase, that means that if', nan]\n",
      "['+distcp is used, it _must_ be from a compatible hadoop version.', nan]\n",
      "['+', nan]\n",
      "['+### <a name=\"fallure-mode\"></a> How will incompatible applications/versions fail?', nan]\n",
      "['+', nan]\n",
      "['+Applications using an incompatible version of the S3A connector will mistake', nan]\n",
      "['+directories containing data for empty directories. This means that:', nan]\n",
      "['+', nan]\n",
      "['+* Listing directories/directory trees may exclude files which exist.', nan]\n",
      "['+* Queries across the data will miss data files.', nan]\n",
      "['+* Renaming a directory to a new location may exclude files underneath.', nan]\n",
      "['+', nan]\n",
      "['+The failures are silent: there is no error message, stack trace or', nan]\n",
      "[\"+other warning that files may have been missed. They simply aren't\", nan]\n",
      "['+found.', nan]\n",
      "['+', nan]\n",
      "['+### <a name=\"recovery\"></a> If an application has updated a directory tree incompatibly-- what can be done', nan]\n",
      "['+', nan]\n",
      "[\"+There's a tool on the hadoop command line, [marker tool](#marker-tool) which can audit\", nan]\n",
      "['+a bucket/path for markers, and clean up any markers which were found.', nan]\n",
      "['+It can be used to make a bucket compatible with older applications.', nan]\n",
      "['+', nan]\n",
      "[\"+Now that this is all clear, let's explain the problem.\", nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"background\"></a> Background: Directory Markers: what and why?', nan]\n",
      "['+', nan]\n",
      "['+Amazon S3 is not a filesystem, it is an object store.', nan]\n",
      "['+', nan]\n",
      "['+The S3A connector not only provides a hadoop-compatible API to interact with', nan]\n",
      "['+data in S3, it tries to maintain the filesystem metaphor.', nan]\n",
      "['+', nan]\n",
      "['+One key aspect of the metaphor of a file system is \"directories\"', nan]\n",
      "['+', nan]\n",
      "['+#### The directory concept', nan]\n",
      "['+', nan]\n",
      "['+In normal Unix-style filesystems, the \"filesystem\" is really a \"directory and', nan]\n",
      "['+file tree\" in which files are always stored in \"directories\"', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+* A directory may contain zero or more files.', nan]\n",
      "['+* A directory may contain zero or more directories \"subdirectories\"', nan]\n",
      "['+* At the base of a filesystem is the \"root directory\"', nan]\n",
      "['+* All files MUST be in a directory \"the parent directory\"', nan]\n",
      "['+* All directories other than the root directory must be in another directory.', nan]\n",
      "['+* If a directory contains no files or directories, it is \"empty\"', nan]\n",
      "['+* When a directory is _listed_, all files and directories in it are enumerated', nan]\n",
      "['+  and returned to the caller', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+The S3A connector emulates this metaphor by grouping all objects which have', nan]\n",
      "['+the same prefix as if they are in the same directory tree.', nan]\n",
      "['+', nan]\n",
      "['+If there are two objects `a/b/file1` and `a/b/file2` then S3A pretends that there is a', nan]\n",
      "['+directory `/a/b` containing two files `file1`  and `file2`.', nan]\n",
      "['+', nan]\n",
      "['+The directory itself does not exist.', nan]\n",
      "['+', nan]\n",
      "[\"+There's a bit of a complication here.\", nan]\n",
      "['+', nan]\n",
      "['+#### What does `mkdirs()` do?', nan]\n",
      "['+', nan]\n",
      "['+1. In HDFS and other \"real\" filesystems, when `mkdirs()` is invoked on a path', nan]\n",
      "['+whose parents are all directories, then an _empty directory_ is created.', nan]\n",
      "['+', nan]\n",
      "['+1. This directory can be probed for \"it exists\" and listed (an empty list is', nan]\n",
      "['+returned)', nan]\n",
      "['+', nan]\n",
      "['+1. Files and other directories can be created in it.', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+Lots of code contains a big assumption here: after you create a directory it', nan]\n",
      "['+exists. They also assume that after files in a directory are deleted, the', nan]\n",
      "['+directory still exists.', nan]\n",
      "['+', nan]\n",
      "['+Given the S3A connector mimics directories just by aggregating objects which share a', nan]\n",
      "['+prefix, how can you have empty directories?', nan]\n",
      "['+', nan]\n",
      "['+The original Hadoop `s3n://` connector created a Directory Marker -any path ending', nan]\n",
      "['+in `_$folder$` was considered to be a sign that a directory existed. A call to', nan]\n",
      "['+`mkdir(s3n://bucket/a/b)` would create a new marker object `a/b_$folder$` .', nan]\n",
      "['+', nan]\n",
      "['+The S3A also has directory markers, but it just appends a \"/\" to the directory', nan]\n",
      "['+name, so `mkdir(s3a://bucket/a/b)` will create a new marker object `a/b/` .', nan]\n",
      "['+', nan]\n",
      "['+When a file is created under a path, the directory marker is deleted. And when a', nan]\n",
      "['+file is deleted, if it was the last file in the directory, the marker is', nan]\n",
      "['+recreated.', nan]\n",
      "['+', nan]\n",
      "['+And, historically, When a path is listed, if a marker to that path is found, *it', nan]\n",
      "['+has been interpreted as an empty directory.*', nan]\n",
      "['+', nan]\n",
      "['+It is that little detail which is the cause of the incompatibility issues.', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"problem\"></a> The Problem with Directory Markers', nan]\n",
      "['+', nan]\n",
      "['+Creating, deleting and the listing directory markers adds overhead and can slow', nan]\n",
      "['+down applications.', nan]\n",
      "['+', nan]\n",
      "['+Whenever a file is created we have to delete any marker which could exist in', nan]\n",
      "['+parent directory _or any parent paths_. Rather than do a sequence of probes for', nan]\n",
      "['+parent markers existing, the connector issues a single request to S3 to delete', nan]\n",
      "['+all parents. For example, if a file `/a/b/file1` is created, a multi-object', nan]\n",
      "['+`DELETE` request containing the keys `/a/` and `/a/b/` is issued.', nan]\n",
      "['+If no markers exists, this is harmless.', nan]\n",
      "['+', nan]\n",
      "['+When a file is deleted, a check for the parent directory continuing to exist', nan]\n",
      "['+(i.e. are there sibling files/directories?), and if not a marker is created.', nan]\n",
      "['+', nan]\n",
      "['+This all works well and has worked well for many years.', nan]\n",
      "['+', nan]\n",
      "['+However, it turns out to have some scale problems, especially from the delete', nan]\n",
      "['+call made whenever a file is created.', nan]\n",
      "['+', nan]\n",
      "['+1. The number of the objects listed in each request is that of the number of', nan]\n",
      "['+parent directories: deeper trees create longer requests.', nan]\n",
      "['+', nan]\n",
      "['+2. Every single object listed in the delete request is considered to be a write', nan]\n",
      "['+operation.', nan]\n",
      "['+', nan]\n",
      "['+3. In versioned S3 buckets, tombstone markers are added to the S3 indices even', nan]\n",
      "['+if no object was deleted.', nan]\n",
      "['+', nan]\n",
      "[\"+4. There's also the overhead of actually issuing the request and awaiting the\", nan]\n",
      "['+response.', nan]\n",
      "['+', nan]\n",
      "['+Issue #2 has turned out to cause significant problems on some interactions with', nan]\n",
      "['+large hive tables:', nan]\n",
      "['+', nan]\n",
      "['+Because each object listed in a DELETE call is treated as one operation, and', nan]\n",
      "['+there is (as of summer 2020) a limit of 3500 write requests/second in a directory', nan]\n",
      "['+tree.', nan]\n",
      "['+When writing many files to a deep directory tree, it is the delete calls which', nan]\n",
      "['+create throttling problems.', nan]\n",
      "['+', nan]\n",
      "['+The tombstone markers have follow-on consequences -it makes listings against', nan]\n",
      "['+S3 versioned buckets slower.', nan]\n",
      "['+This can have adverse effects on those large directories, again.', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"solutions\"></a> Strategies to avoid marker-related problems.', nan]\n",
      "['+', nan]\n",
      "['+###  Presto: every path is a directory', nan]\n",
      "['+', nan]\n",
      "['+In the Presto [S3 connector](https://prestodb.io/docs/current/connector/hive.html#amazon-s3-configuration)', nan]\n",
      "['+`mkdirs()` is a no-op.', nan]\n",
      "[\"+Whenever it lists any path which isn't an object or a prefix of one more more objects, it returns an\", nan]\n",
      "['+empty listing. That is:;  by default, every path is an empty directory.', nan]\n",
      "['+', nan]\n",
      "['+Provided no code probes for a directory existing and fails if it is there, this', nan]\n",
      "[\"+is very efficient. That's a big requirement however, -one Presto can pull off\", nan]\n",
      "['+because they know how their file uses data in S3.', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+###  Hadoop 3.3.1+: marker deletion is now optional', nan]\n",
      "['+', nan]\n",
      "['+From Hadoop 3.3.1 onwards, the S3A client can be configured to skip deleting', nan]\n",
      "['+directory markers when creating files under paths. This removes all scalability', nan]\n",
      "['+problems caused by deleting these markers -however, it is achieved at the expense', nan]\n",
      "['+of backwards compatibility.', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"marker-retention\"></a> Controlling marker retention with `fs.s3a.directory.marker.retention`', nan]\n",
      "['+', nan]\n",
      "['+There is now an option `fs.s3a.directory.marker.retention` which controls how', nan]\n",
      "['+markers are managed when new files are created', nan]\n",
      "['+', nan]\n",
      "['+*Default* `delete`: a request is issued to delete any parental directory markers', nan]\n",
      "['+whenever a file or directory is created.', nan]\n",
      "['+', nan]\n",
      "['+*New* `keep`: No delete request is issued.', nan]\n",
      "['+Any directory markers which exist are not deleted.', nan]\n",
      "['+This is *not* backwards compatible', nan]\n",
      "['+', nan]\n",
      "['+*New* `authoritative`: directory markers are deleted _except for files created', nan]\n",
      "['+in \"authoritative\" directories_.', nan]\n",
      "['+This is backwards compatible _outside authoritative directories_.', nan]\n",
      "['+', nan]\n",
      "['+Until now, the notion of an \"authoritative\"', nan]\n",
      "['+directory has only been used as a performance optimization for deployments', nan]\n",
      "['+where it is known that all Applications are using the same S3Guard metastore', nan]\n",
      "['+when writing and reading data.', nan]\n",
      "['+In such a deployment, if it is also known that all applications are using a', nan]\n",
      "['+compatible version of the s3a connector, then they', nan]\n",
      "['+can switch to the higher-performance mode for those specific directories.', nan]\n",
      "['+', nan]\n",
      "['+Only the default setting, `fs.s3a.directory.marker.retention = delete` is compatible with', nan]\n",
      "['+every shipping Hadoop releases.', nan]\n",
      "['+', nan]\n",
      "['+##  <a name=\"authoritative\"></a> Directory Markers and S3Guard', nan]\n",
      "['+', nan]\n",
      "['+Applications which interact with S3A in S3A clients with S3Guard enabled still', nan]\n",
      "[\"+create and delete markers. There's no attempt to skip operations, such as by having\", nan]\n",
      "['+`mkdirs() `create entries in the DynamoDB table but not the store.', nan]\n",
      "['+Having the client always update S3 ensures that other applications and clients', nan]\n",
      "['+do (eventually) see the changes made by the \"guarded\" application.', nan]\n",
      "['+', nan]\n",
      "['+When S3Guard is configured to treat some directories as  [Authoritative](s3guard.html#authoritative)', nan]\n",
      "['+then an S3A connector with a retention policy of `fs.s3a.directory.marker.retention` of', nan]\n",
      "['+`authoritative` will omit deleting markers in authoritative directories.', nan]\n",
      "['+', nan]\n",
      "['+*Note* there may be further changes in directory semantics in \"authoritative mode\";', nan]\n",
      "['+only use in managed applications where all clients are using the same version of', nan]\n",
      "['+hadoop, and configured consistently.', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"bucket-info\"></a> Verifying marker policy with `s3guard bucket-info`', nan]\n",
      "['+', nan]\n",
      "['+The `bucket-info` command has been enhanced to support verification from the command', nan]\n",
      "['+line of bucket policies via the `-marker` option', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+| option | verifies |', nan]\n",
      "['+|--------|--------|', nan]\n",
      "['+| `-markers aware` | the hadoop release is \"aware\" of directory markers |', nan]\n",
      "['+| `-markers delete` | directory markers are deleted |', nan]\n",
      "['+| `-markers keep` | directory markers are kept (not backwards compatible) |', nan]\n",
      "['+| `-markers authoritative` | directory markers are kept in authoritative paths |', nan]\n",
      "['+', nan]\n",
      "['+All releases of Hadoop which have been updated to be marker aware will support the `-markers aware` option', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+1. Updated releases which do not support switching marker retention policy will also support the', nan]\n",
      "['+`-markers delete` option.', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+Example: `s3guard bucket-info -markers aware` on a compatible release.', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop s3guard bucket-info -markers aware s3a://landsat-pds/', nan]\n",
      "['+ Filesystem s3a://landsat-pds', nan]\n",
      "['+ Location: us-west-2', nan]\n",
      "['+ Filesystem s3a://landsat-pds is not using S3Guard', nan]\n",
      "['+', nan]\n",
      "['+...', nan]\n",
      "['+', nan]\n",
      "['+ Security', nan]\n",
      "['+    Delegation token support is disabled', nan]\n",
      "['+', nan]\n",
      "['+ The directory marker policy is \"delete\"', nan]\n",
      "['+', nan]\n",
      "['+ The S3A connector is compatible with buckets where directory markers are not deleted', nan]\n",
      "['+ Available Policies: delete, keep, authoritative', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+The same command will fail on older releases, because the `-markers` option', nan]\n",
      "['+is unknown', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop s3guard bucket-info -markers aware s3a://landsat-pds/', nan]\n",
      "['+Illegal option -markers', nan]\n",
      "['+Usage: hadoop bucket-info [OPTIONS] s3a://BUCKET', nan]\n",
      "['+    provide/check S3Guard information about a specific bucket', nan]\n",
      "['+', nan]\n",
      "['+Common options:', nan]\n",
      "['+  -guarded - Require S3Guard', nan]\n",
      "['+  -unguarded - Force S3Guard to be disabled', nan]\n",
      "['+  -auth - Require the S3Guard mode to be \"authoritative\"', nan]\n",
      "['+  -nonauth - Require the S3Guard mode to be \"non-authoritative\"', nan]\n",
      "['+  -magic - Require the S3 filesystem to be support the \"magic\" committer', nan]\n",
      "['+  -encryption -require {none, sse-s3, sse-kms} - Require encryption policy', nan]\n",
      "['+', nan]\n",
      "['+When possible and not overridden by more specific options, metadata', nan]\n",
      "['+repository information will be inferred from the S3A URL (if provided)', nan]\n",
      "['+', nan]\n",
      "['+Generic options supported are:', nan]\n",
      "['+  -conf <config file> - specify an application configuration file', nan]\n",
      "['+  -D <property=value> - define a value for a given property', nan]\n",
      "['+', nan]\n",
      "['+2020-08-12 16:47:16,579 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 42', 'Illegal option -markers']\n",
      "['+````', nan]\n",
      "['+', nan]\n",
      "['+A specific policy check verifies that the connector is configured as desired', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop s3guard bucket-info -markers delete s3a://landsat-pds/', nan]\n",
      "['+Filesystem s3a://landsat-pds', nan]\n",
      "['+Location: us-west-2', nan]\n",
      "['+Filesystem s3a://landsat-pds is not using S3Guard', nan]\n",
      "['+', nan]\n",
      "['+...', nan]\n",
      "['+', nan]\n",
      "['+The directory marker policy is \"delete\"', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+When probing for a specific policy, the error code \"46\" is returned if the active policy', nan]\n",
      "['+does not match that requested:', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop s3guard bucket-info -markers keep s3a://landsat-pds/', nan]\n",
      "['+Filesystem s3a://landsat-pds', nan]\n",
      "['+Location: us-west-2', nan]\n",
      "['+Filesystem s3a://landsat-pds is not using S3Guard', nan]\n",
      "['+', nan]\n",
      "['+...', nan]\n",
      "['+', nan]\n",
      "['+Security', nan]\n",
      "['+    Delegation token support is disabled', nan]\n",
      "['+', nan]\n",
      "['+The directory marker policy is \"delete\"', nan]\n",
      "['+', nan]\n",
      "['+2020-08-12 17:14:30,563 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 46', '46: Bucket s3a://landsat-pds: required marker policy is \"keep\" but actual policy is \"delete\"']\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+##  <a name=\"marker-tool\"></a> The marker tool:`hadoop s3guard markers`', nan]\n",
      "['+', nan]\n",
      "['+The marker tool aims to help migration by scanning/auditing directory trees', nan]\n",
      "['+for surplus markers, and for optionally deleting them.', nan]\n",
      "['+Leaf-node markers for empty directories are not considered surplus and', nan]\n",
      "['+will be retained.', nan]\n",
      "['+', nan]\n",
      "['+Syntax', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop s3guard markers -verbose -nonauth', nan]\n",
      "['+markers (-audit | -clean) [-expected <count>] [-out <filename>] [-limit <limit>] [-nonauth] [-verbose] <PA', 'H>']\n",
      "['+        View and manipulate S3 directory markers', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+*Options*', nan]\n",
      "['+', nan]\n",
      "['+| Option                  | Meaning                 |', nan]\n",
      "['+|-------------------------|-------------------------|', nan]\n",
      "['+|  `-audit`               | Audit the path for surplus markers |', nan]\n",
      "['+|  `-clean`               | Clean all surplus markers under a path |', nan]\n",
      "['+|  `-expected <count>]`   | Expected number of markers to find (primarily for testing)  |', nan]\n",
      "['+|  `-limit <count>]`      | Limit the number of objects to scan |', nan]\n",
      "['+|  `-nonauth`             | Only consider markers in non-authoritative paths as errors  |', nan]\n",
      "['+|  `-out <filename>`      | Save a list of all markers found to the nominated file  |', nan]\n",
      "['+|  `-verbose`             | Verbose output  |', nan]\n",
      "['+', nan]\n",
      "['+*Exit Codes*', nan]\n",
      "['+', nan]\n",
      "['+| Code  | Meaning |', nan]\n",
      "['+|-------|---------|', nan]\n",
      "['+| 0     | Success |', nan]\n",
      "['+| 3     | interrupted -the value of `-limit` was reached |', nan]\n",
      "['+| 42    | Usage   |', nan]\n",
      "['+| 46    | Markers were found (see HTTP \"406\", \"unacceptable\") |', nan]\n",
      "['+', nan]\n",
      "['+All other non-zero status code also indicate errors of some form or other.', nan]\n",
      "['+', nan]\n",
      "['+###  <a name=\"marker-tool-report\"></a>`markers -audit`', nan]\n",
      "['+', nan]\n",
      "['+Audit the path and fail if any markers were found.', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop s3guard markers -limit 8000 -audit s3a://landsat-pds/', nan]\n",
      "['+', nan]\n",
      "['+The directory marker policy of s3a://landsat-pds is \"Delete\"', nan]\n",
      "['+2020-08-05 13:42:56,079 [main] INFO  tools.MarkerTool (DurationInfo.java:<init>(77)) - Starting: marker sc', 'n s3a://landsat-pds/']\n",
      "['+Scanned 1,000 objects', nan]\n",
      "['+Scanned 2,000 objects', nan]\n",
      "['+Scanned 3,000 objects', nan]\n",
      "['+Scanned 4,000 objects', nan]\n",
      "['+Scanned 5,000 objects', nan]\n",
      "['+Scanned 6,000 objects', nan]\n",
      "['+Scanned 7,000 objects', nan]\n",
      "['+Scanned 8,000 objects', nan]\n",
      "['+Limit of scan reached - 8,000 objects', nan]\n",
      "['+2020-08-05 13:43:01,184 [main] INFO  tools.MarkerTool (DurationInfo.java:close(98)) - marker scan s3a://la', 'dsat-pds/: duration 0:05.107s']\n",
      "['+No surplus directory markers were found under s3a://landsat-pds/', nan]\n",
      "['+Listing limit reached before completing the scan', nan]\n",
      "['+2020-08-05 13:43:01,187 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 3:', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+Here the scan reached its object limit before completing the audit; the exit code of 3, \"interrupted\" indi', 'ates this.']\n",
      "['+', nan]\n",
      "['+Example: a verbose audit of a bucket whose policy if authoritative -it is not an error if markers', nan]\n",
      "['+are found under the path `/tables`.', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> bin/hadoop s3guard markers -audit s3a://london/', nan]\n",
      "['+', nan]\n",
      "['+  2020-08-05 18:29:16,473 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPoli', 'y(143)) - Directory markers will be kept on authoritative paths']\n",
      "['+  The directory marker policy of s3a://london is \"Authoritative\"', nan]\n",
      "['+  Authoritative path list is \"/tables\"', nan]\n",
      "['+  2020-08-05 18:29:19,186 [main] INFO  tools.MarkerTool (DurationInfo.java:<init>(77)) - Starting: marker', 'can s3a://london/']\n",
      "['+  2020-08-05 18:29:21,610 [main] INFO  tools.MarkerTool (DurationInfo.java:close(98)) - marker scan s3a://', 'ondon/: duration 0:02.425s']\n",
      "['+  Listed 8 objects under s3a://london/', nan]\n",
      "['+', nan]\n",
      "['+Found 3 surplus directory markers under s3a://london/', nan]\n",
      "['+    s3a://london/tables', nan]\n",
      "['+    s3a://london/tables/tables-4', nan]\n",
      "['+    s3a://london/tables/tables-4/tables-5', nan]\n",
      "[\"+Found 5 empty directory 'leaf' markers under s3a://london/\", nan]\n",
      "['+    s3a://london/tables/tables-2', nan]\n",
      "['+    s3a://london/tables/tables-3', nan]\n",
      "['+    s3a://london/tables/tables-4/tables-5/06', nan]\n",
      "['+    s3a://london/tables2', nan]\n",
      "['+    s3a://london/tables3', nan]\n",
      "['+  These are required to indicate empty directories', nan]\n",
      "['+  Surplus markers were found -failing audit', nan]\n",
      "['+  2020-08-05 18:29:21,614 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status', '6:']\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+This fails because surplus markers were found. This S3A bucket would *NOT* be safe for older Hadoop versio', 's']\n",
      "['+to use.', nan]\n",
      "['+', nan]\n",
      "['+The `-nonauth` option does not treat markers under authoritative paths as errors:', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+bin/hadoop s3guard markers -nonauth -audit s3a://london/', nan]\n",
      "['+', nan]\n",
      "['+2020-08-05 18:31:16,255 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPolicy', '143)) - Directory markers will be kept on authoritative paths']\n",
      "['+The directory marker policy of s3a://london is \"Authoritative\"', nan]\n",
      "['+Authoritative path list is \"/tables\"', nan]\n",
      "['+2020-08-05 18:31:19,210 [main] INFO  tools.MarkerTool (DurationInfo.java:<init>(77)) - Starting: marker sc', 'n s3a://london/']\n",
      "['+2020-08-05 18:31:22,240 [main] INFO  tools.MarkerTool (DurationInfo.java:close(98)) - marker scan s3a://lo', 'don/: duration 0:03.031s']\n",
      "['+Listed 8 objects under s3a://london/', nan]\n",
      "['+', nan]\n",
      "['+Found 3 surplus directory markers under s3a://london/', nan]\n",
      "['+    s3a://london/tables/', nan]\n",
      "['+    s3a://london/tables/tables-4/', nan]\n",
      "['+    s3a://london/tables/tables-4/tables-5/', nan]\n",
      "[\"+Found 5 empty directory 'leaf' markers under s3a://london/\", nan]\n",
      "['+    s3a://london/tables/tables-2/', nan]\n",
      "['+    s3a://london/tables/tables-3/', nan]\n",
      "['+    s3a://london/tables/tables-4/tables-5/06/', nan]\n",
      "['+    s3a://london/tables2/', nan]\n",
      "['+    s3a://london/tables3/', nan]\n",
      "['+These are required to indicate empty directories', nan]\n",
      "['+', nan]\n",
      "['+Ignoring 3 markers in authoritative paths', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+All of this S3A bucket _other_ than the authoritative path `/tables` will be safe for', nan]\n",
      "['+incompatible Hadoop releases to to use.', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+###  <a name=\"marker-tool-clean\"></a>`markers clean`', nan]\n",
      "['+', nan]\n",
      "['+The `markers clean` command will clean the directory tree of all surplus markers.', nan]\n",
      "['+The `-verbose` option prints more detail on the operation as well as some IO statistics', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop s3guard markers -verbose -clean s3a://london/', nan]\n",
      "['+', nan]\n",
      "['+2020-08-05 18:33:25,303 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPolicy', '143)) - Directory markers will be kept on authoritative paths']\n",
      "['+The directory marker policy of s3a://london is \"Authoritative\"', nan]\n",
      "['+Authoritative path list is \"/tables\"', nan]\n",
      "['+2020-08-05 18:33:28,511 [main] INFO  tools.MarkerTool (DurationInfo.java:<init>(77)) - Starting: marker sc', 'n s3a://london/']\n",
      "['+  Directory Marker tables', nan]\n",
      "['+  Directory Marker tables/tables-2/', nan]\n",
      "['+  Directory Marker tables/tables-3/', nan]\n",
      "['+  Directory Marker tables/tables-4/', nan]\n",
      "['+  Directory Marker tables/tables-4/tables-5/', nan]\n",
      "['+  Directory Marker tables/tables-4/tables-5/06/', nan]\n",
      "['+  Directory Marker tables2/', nan]\n",
      "['+  Directory Marker tables3/', nan]\n",
      "['+2020-08-05 18:33:31,685 [main] INFO  tools.MarkerTool (DurationInfo.java:close(98)) - marker scan s3a://lo', 'don/: duration 0:03.175s']\n",
      "['+Listed 8 objects under s3a://london/', nan]\n",
      "['+', nan]\n",
      "['+Found 3 surplus directory markers under s3a://london/', nan]\n",
      "['+    s3a://london/tables/', nan]\n",
      "['+    s3a://london/tables/tables-4/', nan]\n",
      "['+    s3a://london/tables/tables-4/tables-5/', nan]\n",
      "[\"+Found 5 empty directory 'leaf' markers under s3a://london/\", nan]\n",
      "['+    s3a://london/tables/tables-2/', nan]\n",
      "['+    s3a://london/tables/tables-3/', nan]\n",
      "['+    s3a://london/tables/tables-4/tables-5/06/', nan]\n",
      "['+    s3a://london/tables2/', nan]\n",
      "['+    s3a://london/tables3/', nan]\n",
      "['+These are required to indicate empty directories', nan]\n",
      "['+', nan]\n",
      "['+3 markers to delete in 1 page of 250 keys/page', nan]\n",
      "['+2020-08-05 18:33:31,688 [main] INFO  tools.MarkerTool (DurationInfo.java:<init>(77)) - Starting: Deleting', 'arkers']\n",
      "['+2020-08-05 18:33:31,812 [main] INFO  tools.MarkerTool (DurationInfo.java:close(98)) - Deleting markers: du', 'ation 0:00.124s']\n",
      "['+', nan]\n",
      "['+Storage Statistics for s3a://london', nan]\n",
      "['+', nan]\n",
      "['+op_get_file_status\\t1', nan]\n",
      "['+object_delete_requests\\t1', nan]\n",
      "['+object_list_requests\\t2', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+The `markers -clean` command _does not_ delete markers above empty directories -only those which have', nan]\n",
      "['+files underneath. If invoked on a path, it will clean up the directory tree into a state', nan]\n",
      "['+where it is safe for older versions of Hadoop to interact with.', nan]\n",
      "['+', nan]\n",
      "['+Note that if invoked with a `-limit` value, surplus markers found during the scan will be removed,', nan]\n",
      "['+even though the scan will be considered a failure due to the limit being reached.', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"advanced-topics\"></a> Advanced Topics', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+### <a name=\"pathcapabilities\"></a> Probing for retention via `PathCapabilities` and `StreamCapabilities`', nan]\n",
      "['+', nan]\n",
      "['+An instance of the filesystem can be probed for its directory marker retention ability/', nan]\n",
      "['+policy can be probed for through the `org.apache.hadoop.fs.PathCapabilities` interface,', nan]\n",
      "['+which all FileSystem classes have supported since Hadoop 3.3.', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+| Probe                   | Meaning                 |', nan]\n",
      "['+|-------------------------|-------------------------|', nan]\n",
      "['+| `fs.s3a.capability.directory.marker.aware`  | Does the filesystem support surplus directory markers? |', nan]\n",
      "['+| `fs.s3a.capability.directory.marker.policy.delete` | Is the bucket policy \"delete\"? |', nan]\n",
      "['+| `fs.s3a.capability.directory.marker.policy.keep`   | Is the bucket policy \"keep\"? |', nan]\n",
      "['+| `fs.s3a.capability.directory.marker.policy.authoritative` | Is the bucket policy \"authoritative\"? |', nan]\n",
      "['+| `fs.s3a.capability.directory.marker.action.delete` | If a file was created at this path, would directory', 'markers be deleted? |']\n",
      "['+| `fs.s3a.capability.directory.marker.action.keep`   | If a file was created at this path, would directory', 'markers be retained? |']\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+The probe `fs.s3a.capability.directory.marker.aware` allows for a filesystem to be', nan]\n",
      "['+probed to determine if its file listing policy is \"aware\" of directory marker retention', nan]\n",
      "['+-that is: can this s3a client safely work with S3 buckets where markers have not been deleted.', nan]\n",
      "['+', nan]\n",
      "['+The `fs.s3a.capability.directory.marker.policy.` probes return the active policy for the bucket.', nan]\n",
      "['+', nan]\n",
      "['+The two `fs.s3a.capability.directory.marker.action.` probes dynamically query the marker', nan]\n",
      "['+retention behavior of a specific path.', nan]\n",
      "['+That is: if a file was created at that location, would ancestor directory markers', nan]\n",
      "['+be kept or deleted?', nan]\n",
      "['+', nan]\n",
      "['+The `S3AFileSystem` class also implements the `org.apache.hadoop.fs.StreamCapabilities` interface, which', nan]\n",
      "['+can be used to probe for marker awareness via the `fs.s3a.capability.directory.marker.aware` capability.', nan]\n",
      "['+', nan]\n",
      "['+Again, this will be true if-and-only-if the S3A connector is safe to work with S3A buckets/paths where', nan]\n",
      "['+directories are retained.', nan]\n",
      "['+', nan]\n",
      "['+*If an S3A instance, probed by `PathCapabilities` or `StreamCapabilities` for the capability', nan]\n",
      "['+`fs.s3a.capability.directory.marker.aware` and it returns false, *it is not safe to be used with', nan]\n",
      "['+S3A paths where markers have been retained*.', nan]\n",
      "['+', nan]\n",
      "['+This is programmatic probe -however it can be accessed on the command line via the', nan]\n",
      "['+external [`cloudstore`](https://github.com/steveloughran/cloudstore) tool:', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+> hadoop jar cloudstore-1.0.jar pathcapability fs.s3a.capability.directory.marker.aware  s3a://london/', nan]\n",
      "['+', nan]\n",
      "['+Probing s3a://london/ for capability fs.s3a.capability.directory.marker.aware', nan]\n",
      "['+', nan]\n",
      "['+Using filesystem s3a://london', nan]\n",
      "['+Path s3a://london/ has capability fs.s3a.capability.directory.marker.aware', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+If the exit code of the command is `0`, then the S3A is safe to work with buckets', nan]\n",
      "['+where markers have not been deleted.', nan]\n",
      "['+', nan]\n",
      "['+The same tool can be used to dynamically probe for the policy.', nan]\n",
      "['+', nan]\n",
      "['+Take a bucket with a retention policy of \"authoritative\" -only paths under `/tables` will have markers ret', 'ined.']\n",
      "['+', nan]\n",
      "['+```xml', nan]\n",
      "['+  <property>', nan]\n",
      "['+    <name>fs.s3a.bucket.london.directory.marker.retention</name>', nan]\n",
      "['+    <value>authoritative</value>', nan]\n",
      "['+  </property>', nan]\n",
      "['+  <property>', nan]\n",
      "['+    <name>fs.s3a.bucket.london.authoritative.path</name>', nan]\n",
      "['+    <value>/tables</value>', nan]\n",
      "['+  </property>```', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+With this policy the path capability `fs.s3a.capability.directory.marker.action.keep` will hold under', nan]\n",
      "['+the path `s3a://london/tables`', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+bin/hadoop jar cloudstore-1.0.jar pathcapability fs.s3a.capability.directory.marker.action.keep s3a://lond', 'n/tables']\n",
      "['+Probing s3a://london/tables for capability fs.s3a.capability.directory.marker.action.keep', nan]\n",
      "['+2020-08-11 22:03:31,658 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPolicy', '143))']\n",
      "['+ - Directory markers will be kept on authoritative paths', nan]\n",
      "['+Using filesystem s3a://london', nan]\n",
      "['+Path s3a://london/tables has capability fs.s3a.capability.directory.marker.action.keep', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+However it will not hold for other paths, so indicating that older Hadoop versions will be safe', nan]\n",
      "['+to work with data written there by this S3A client.', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+bin/hadoop jar cloudstore-1.0.jar pathcapability fs.s3a.capability.directory.marker.action.keep s3a://lond', 'n/tempdir']\n",
      "['+Probing s3a://london/tempdir for capability fs.s3a.capability.directory.marker.action.keep', nan]\n",
      "['+2020-08-11 22:06:56,300 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPolicy', '143))']\n",
      "['+ - Directory markers will be kept on authoritative paths', nan]\n",
      "['+Using filesystem s3a://london', nan]\n",
      "['+Path s3a://london/tempdir lacks capability fs.s3a.capability.directory.marker.action.keep', nan]\n",
      "['+2020-08-11 22:06:56,308 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status -1', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"glossary\"></a> Glossary', nan]\n",
      "['+', nan]\n",
      "['+#### Directory Marker', nan]\n",
      "['+', nan]\n",
      "['+An object in an S3 bucket with a trailing \"/\", used to indicate that there is a directory at that location', nan]\n",
      "['+These are necessary to maintain expectations about directories in an object store:', nan]\n",
      "['+', nan]\n",
      "['+1. After `mkdirs(path)`, `exists(path)` holds.', nan]\n",
      "['+1. After `rm(path/*)`, `exists(path)` holds.', nan]\n",
      "['+', nan]\n",
      "['+In previous releases of Hadoop, the marker created by a `mkdirs()` operation was deleted after a file was', 'reated.']\n",
      "['+Rather than make a slow HEAD probe + optional marker DELETE of every parent path element, HADOOP-13164 swi', 'ched']\n",
      "['+to enumerating all parent paths and issuing a single bulk DELETE request.', nan]\n",
      "['+This is faster under light load, but', nan]\n",
      "['+as each row in the delete consumes one write operation on the allocated IOPs of that bucket partition, cre', 'tes']\n",
      "['+load issues when many worker threads/processes are writing to files.', nan]\n",
      "['+This problem is bad on Apache Hive as:', nan]\n",
      "['+* The hive partition structure places all files within the same S3 partition.', nan]\n",
      "['+* As they are deep structures, there are many parent entries to include in the bulk delete calls.', nan]\n",
      "[\"+* It's creating a lot temporary files, and still uses rename to commit output.\", nan]\n",
      "['+', nan]\n",
      "['+Apache Spark has less of an issue when an S3A committer is used -although the partition structure', nan]\n",
      "['+is the same, the delayed manifestation of output files reduces load.', nan]\n",
      "['+', nan]\n",
      "['+#### Leaf Marker', nan]\n",
      "['+', nan]\n",
      "['+A directory marker which has not files or directory marker objects underneath.', nan]\n",
      "['+It genuinely represents an empty directory.', nan]\n",
      "['+', nan]\n",
      "['+#### Surplus Marker', nan]\n",
      "['+', nan]\n",
      "['+A directory marker which is above one or more files, and so is superfluous.', nan]\n",
      "['+These are the markers which were traditionally deleted; now it is optional.', nan]\n",
      "['+', nan]\n",
      "['+Older versions of Hadoop mistake such surplus markers as Leaf Markers.', nan]\n",
      "['+', nan]\n",
      "['+#### Versioned Bucket', nan]\n",
      "['+', nan]\n",
      "['+An S3 Bucket which has Object Versioning enabled.', nan]\n",
      "['+', nan]\n",
      "['+This provides a backup and recovery mechanism for data within the same', nan]\n",
      "['+bucket: older objects can be listed and restored through the AWS S3 console', nan]\n",
      "['+and some applications.', nan]\n",
      "['+', nan]\n",
      "['+## References', nan]\n",
      "['+', nan]\n",
      "['+<!-- if extending, keep JIRAs separate, have them in numerical order; the rest in lexical.` -->', nan]\n",
      "['+', nan]\n",
      "['+* [HADOOP-13164](https://issues.apache.org/jira/browse/HADOOP-13164). _Optimize S3AFileSystem::deleteUnnec', 'ssaryFakeDirectories._']\n",
      "['+', nan]\n",
      "['+* [HADOOP-13230](https://issues.apache.org/jira/browse/HADOOP-13230). _S3A to optionally retain directory', 'arkers_']\n",
      "['+', nan]\n",
      "['+* [HADOOP-16090](https://issues.apache.org/jira/browse/HADOOP-16090). _S3A Client to add explicit support', 'or versioned stores._']\n",
      "['+', nan]\n",
      "['+* [HADOOP-16823](https://issues.apache.org/jira/browse/HADOOP-16823). _Large DeleteObject requests are the', 'r own Thundering Herd_']\n",
      "['+', nan]\n",
      "['+* [Object Versioning](https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html). _Using versioning_', nan]\n",
      "['+', nan]\n",
      "['+* [Optimizing Performance](https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html). _', 'est Practices Design Patterns: Optimizing Amazon S3 Performance_']\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md b/hadoop-tools/hadoop-aws/', 'rc/site/markdown/tools/hadoop-aws/index.md']\n",
      "['index 964bda49dd0..861da4d82ee 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md', nan]\n",
      "['@@ -16,18 +16,29 @@', nan]\n",
      "[nan, nan]\n",
      "['<!-- MACRO{toc|fromDepth=0|toDepth=2} -->', nan]\n",
      "[nan, nan]\n",
      "[\"-**NOTE:  Hadoop's `s3:` and `s3n:` connectors have been removed.\", nan]\n",
      "['-Please use `s3a:` as the connector to data hosted in S3 with Apache Hadoop.**', nan]\n",
      "[nan, nan]\n",
      "['-**Consult the [s3n documentation](./s3n.html) for migration instructions.**', nan]\n",
      "[nan, nan]\n",
      "['+## <a name=\"compatibility\"></a> Compatibility', nan]\n",
      "[nan, nan]\n",
      "['-See also:', nan]\n",
      "['+', nan]\n",
      "['+###  <a name=\"directory-marker-compatibility\"></a> Directory Marker Compatibility', nan]\n",
      "['+', nan]\n",
      "['+1. This release can safely list/index/read S3 buckets where \"empty directory\"', nan]\n",
      "['+markers are retained.', nan]\n",
      "['+', nan]\n",
      "['+1. This release can be configured to retain these directory makers at the', nan]\n",
      "['+expense of being backwards incompatible.', nan]\n",
      "['+', nan]\n",
      "['+Consult [Controlling the S3A Directory Marker Behavior](directory_markers.html) for', nan]\n",
      "['+full details.', nan]\n",
      "['+', nan]\n",
      "['+## <a name=\"documents\"></a> Documents', nan]\n",
      "[nan, nan]\n",
      "['* [Encryption](./encryption.html)', nan]\n",
      "['* [Performance](./performance.html)', nan]\n",
      "['* [S3Guard](./s3guard.html)', nan]\n",
      "['* [Troubleshooting](./troubleshooting_s3a.html)', nan]\n",
      "['+* [Controlling the S3A Directory Marker Behavior](directory_markers.html).', nan]\n",
      "['* [Committing work to S3 with the \"S3A Committers\"](./committers.html)', nan]\n",
      "['* [S3A Committers Architecture](./committer_architecture.html)', nan]\n",
      "['* [Working with IAM Assumed Roles](./assumed_roles.html)', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3guard.md b/hadoop-tools/hadoop-aw', '/src/site/markdown/tools/hadoop-aws/s3guard.md']\n",
      "['index 5754f0b5dfd..b60d54622ed 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3guard.md', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3guard.md', nan]\n",
      "['@@ -113,7 +113,19 @@ Currently the only Metadata Store-independent setting, besides the', nan]\n",
      "['implementation class above, are the *allow authoritative* and *fail-on-error*', nan]\n",
      "['flags.', nan]\n",
      "[nan, nan]\n",
      "['-#### Allow Authoritative', nan]\n",
      "['+#### <a name=\"authoritative\"></a>  Authoritative S3Guard', nan]\n",
      "['+', nan]\n",
      "['+Authoritative S3Guard is a complicated configuration which delivers performance', nan]\n",
      "['+at the expense of being unsafe for other applications to use the same directory', nan]\n",
      "['+tree/bucket unless configured consistently.', nan]\n",
      "['+', nan]\n",
      "['+It can also be used to support [directory marker retention](directory_markers.html)', nan]\n",
      "['+in higher-performance but non-backwards-compatible modes.', nan]\n",
      "['+', nan]\n",
      "['+Most deployments do not use this setting -it is ony used in deployments where', nan]\n",
      "['+specific parts of a bucket (e.g. Apache Hive managed tables) are known to', nan]\n",
      "['+have exclusive access by a single application (Hive) and other tools/applications', nan]\n",
      "['+from exactly the same Hadoop release.', nan]\n",
      "[nan, nan]\n",
      "['The _authoritative_ expression in S3Guard is present in two different layers, for', nan]\n",
      "['two different reasons:', nan]\n",
      "['@@ -178,7 +190,7 @@ recommended that you leave the default setting here:', nan]\n",
      "['<value>false</value>', nan]\n",
      "['</property>', nan]\n",
      "['```', nan]\n",
      "['-.', nan]\n",
      "['+', nan]\n",
      "['Note that a MetadataStore MAY persist this bit in the directory listings. (Not', nan]\n",
      "['MUST).', nan]\n",
      "[nan, nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/testing.md b/hadoop-tools/hadoop-aw', '/src/site/markdown/tools/hadoop-aws/testing.md']\n",
      "['index 5629dab21ff..e9730444f3a 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/testing.md', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/testing.md', nan]\n",
      "['@@ -324,6 +324,49 @@ Once a bucket is converted to being versioned, it cannot be converted back', nan]\n",
      "['to being unversioned.', nan]\n",
      "[nan, nan]\n",
      "[nan, nan]\n",
      "['+## <a name=\"marker\"></a> Testing Different Marker Retention Policy', nan]\n",
      "['+', nan]\n",
      "['+Hadoop supports [different policies for directory marker retention](directory_markers.html)', nan]\n",
      "['+-essentially the classic \"delete\" and the higher-performance \"keep\" options; \"authoritative\"', nan]\n",
      "['+is just \"keep\" restricted to a part of the bucket.', nan]\n",
      "['+', nan]\n",
      "['+Example: test with `markers=delete`', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+mvn verify -Dparallel-tests -DtestsThreadCount=4 -Dmarkers=delete', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+Example: test with `markers=keep`', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+mvn verify -Dparallel-tests -DtestsThreadCount=4 -Dmarkers=keep', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+Example: test with `markers=authoritative`', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+mvn verify -Dparallel-tests -DtestsThreadCount=4 -Dmarkers=authoritative', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+This final option is of limited use unless paths in the bucket have actually been configured to be', nan]\n",
      "['+of mixed status; unless anything is set up then the outcome should equal that of \"delete\"', nan]\n",
      "['+', nan]\n",
      "['+### Enabling auditing of markers', nan]\n",
      "['+', nan]\n",
      "['+To enable an audit of the output directory of every test suite,', nan]\n",
      "['+enable the option `fs.s3a.directory.marker.audit`', nan]\n",
      "['+', nan]\n",
      "['+```', nan]\n",
      "['+-Dfs.s3a.directory.marker.audit=true', nan]\n",
      "['+```', nan]\n",
      "['+', nan]\n",
      "['+When set, if the marker policy is to delete markers under the test output directory, then', nan]\n",
      "['+the marker tool audit command will be run. This will fail if a marker was found.', nan]\n",
      "['+', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+This adds extra overhead to every operation, but helps verify that the connector is', nan]\n",
      "['+not keeping markers where it needs to be deleting them -and hence backwards compatibility', nan]\n",
      "['+is maintained.', nan]\n",
      "['+', nan]\n",
      "['## <a name=\"scale\"></a> Scale Tests', nan]\n",
      "[nan, nan]\n",
      "['There are a set of tests designed to measure the scalability and performance', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3ATestBase.java b/hado', 'p-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3ATestBase.java']\n",
      "['index d94288dfc30..a2ee9ea5f7b 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3ATestBase.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3ATestBase.java', nan]\n",
      "['@@ -25,16 +25,20 @@', nan]\n",
      "['import org.apache.hadoop.fs.contract.AbstractFSContractTestBase;', nan]\n",
      "['import org.apache.hadoop.fs.contract.ContractTestUtils;', nan]\n",
      "['import org.apache.hadoop.fs.contract.s3a.S3AContract;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.tools.MarkerTool;', nan]\n",
      "['import org.apache.hadoop.io.IOUtils;', nan]\n",
      "['-import org.junit.Before;', nan]\n",
      "['import org.slf4j.Logger;', nan]\n",
      "['import org.slf4j.LoggerFactory;', nan]\n",
      "[nan, nan]\n",
      "['+import java.io.FileNotFoundException;', nan]\n",
      "['import java.io.IOException;', nan]\n",
      "[nan, nan]\n",
      "['import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;', nan]\n",
      "['import static org.apache.hadoop.fs.contract.ContractTestUtils.writeDataset;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestDynamoTablePrefix;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestPropertyBool;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3AUtils.E_FS_CLOSED;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.UNLIMITED_LISTING;', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* An extension of the contract test base set up for S3A tests.', nan]\n",
      "['@@ -62,18 +66,43 @@ public void setup() throws Exception {', nan]\n",
      "['@Override', nan]\n",
      "['public void teardown() throws Exception {', nan]\n",
      "['Thread.currentThread().setName(\"teardown\");', nan]\n",
      "['+', nan]\n",
      "['+    maybeAuditTestPath();', nan]\n",
      "['+', nan]\n",
      "['super.teardown();', nan]\n",
      "['describe(\"closing file system\");', nan]\n",
      "['IOUtils.closeStream(getFileSystem());', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-  @Before', nan]\n",
      "['-  public void nameThread() {', nan]\n",
      "['-    Thread.currentThread().setName(\"JUnit-\" + getMethodName());', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-  protected String getMethodName() {', nan]\n",
      "['-    return methodName.getMethodName();', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Audit the FS under {@link #methodPath()} if', nan]\n",
      "['+   * the test option {@link #DIRECTORY_MARKER_AUDIT} is', nan]\n",
      "['+   * true.', nan]\n",
      "['+   */', nan]\n",
      "['+  public void maybeAuditTestPath() {', nan]\n",
      "['+    final S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    if (fs != null) {', nan]\n",
      "['+      try {', nan]\n",
      "['+        boolean audit = getTestPropertyBool(fs.getConf(),', nan]\n",
      "['+            DIRECTORY_MARKER_AUDIT, false);', nan]\n",
      "['+        Path methodPath = methodPath();', nan]\n",
      "['+        if (audit', nan]\n",
      "['+            && !fs.getDirectoryMarkerPolicy()', nan]\n",
      "['+            .keepDirectoryMarkers(methodPath)', nan]\n",
      "['+            && fs.isDirectory(methodPath)) {', nan]\n",
      "['+          MarkerTool.ScanResult result = MarkerTool.execMarkerTool(fs,', nan]\n",
      "['+              methodPath, true, 0, UNLIMITED_LISTING, false);', nan]\n",
      "['+          assertEquals(\"Audit of \" + methodPath + \" failed: \" + result,', nan]\n",
      "['+              0, result.getExitCode());', nan]\n",
      "['+        }', nan]\n",
      "['+      } catch (FileNotFoundException ignored) {', nan]\n",
      "['+      } catch (Exception e) {', nan]\n",
      "['+        // If is this is not due to the FS being closed: log.', nan]\n",
      "['+        if (!e.toString().contains(E_FS_CLOSED)) {', nan]\n",
      "['+          LOG.warn(\"Marker Tool Failure\", e);', nan]\n",
      "['+        }', nan]\n",
      "['+      }', nan]\n",
      "['+    }', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Override', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestAuthoritativePath.java b/h', 'doop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestAuthoritativePath.java']\n",
      "['index 0a91102bf5a..b1d742a4005 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestAuthoritativePath.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestAuthoritativePath.java', nan]\n",
      "['@@ -109,7 +109,8 @@ private S3AFileSystem createFullyAuthFS()', nan]\n",
      "['URI uri = testFS.getUri();', nan]\n",
      "[nan, nan]\n",
      "['removeBaseAndBucketOverrides(uri.getHost(), config,', nan]\n",
      "['-        METADATASTORE_AUTHORITATIVE);', nan]\n",
      "['+        METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['config.setBoolean(METADATASTORE_AUTHORITATIVE, true);', nan]\n",
      "['final S3AFileSystem newFS = createFS(uri, config);', nan]\n",
      "['// set back the same metadata store instance', nan]\n",
      "['@@ -124,7 +125,8 @@ private S3AFileSystem createSinglePathAuthFS(String authPath)', nan]\n",
      "['URI uri = testFS.getUri();', nan]\n",
      "[nan, nan]\n",
      "['removeBaseAndBucketOverrides(uri.getHost(), config,', nan]\n",
      "['-        METADATASTORE_AUTHORITATIVE);', nan]\n",
      "['+        METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['config.set(AUTHORITATIVE_PATH, authPath.toString());', nan]\n",
      "['final S3AFileSystem newFS = createFS(uri, config);', nan]\n",
      "['// set back the same metadata store instance', nan]\n",
      "['@@ -139,7 +141,8 @@ private S3AFileSystem createMultiPathAuthFS(String first, String middle, String', nan]\n",
      "['URI uri = testFS.getUri();', nan]\n",
      "[nan, nan]\n",
      "['removeBaseAndBucketOverrides(uri.getHost(), config,', nan]\n",
      "['-          METADATASTORE_AUTHORITATIVE);', nan]\n",
      "['+        METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['config.set(AUTHORITATIVE_PATH, first + \",\" + middle + \",\" + last);', nan]\n",
      "['final S3AFileSystem newFS = createFS(uri, config);', nan]\n",
      "['// set back the same metadata store instance', nan]\n",
      "['@@ -155,7 +158,8 @@ private S3AFileSystem createRawFS() throws Exception {', nan]\n",
      "['removeBaseAndBucketOverrides(uri.getHost(), config,', nan]\n",
      "['S3_METADATA_STORE_IMPL);', nan]\n",
      "['removeBaseAndBucketOverrides(uri.getHost(), config,', nan]\n",
      "['-        METADATASTORE_AUTHORITATIVE);', nan]\n",
      "['+        METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['return createFS(uri, config);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABucketExistence.java b/', 'adoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABucketExistence.java']\n",
      "['index 6be9003e4ec..8c215d79ea6 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABucketExistence.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABucketExistence.java', nan]\n",
      "['@@ -75,7 +75,11 @@ public void testNoBucketProbing() throws Exception {', nan]\n",
      "[nan, nan]\n",
      "['// the exception must not be caught and marked down to an FNFE', nan]\n",
      "['expectUnknownStore(() -> fs.exists(src));', nan]\n",
      "['-    expectUnknownStore(() -> fs.isFile(src));', nan]\n",
      "['+    // now that isFile() only does a HEAD, it will get a 404 without', nan]\n",
      "['+    // the no-such-bucket error.', nan]\n",
      "['+    assertFalse(\"isFile(\" + src + \")\"', nan]\n",
      "['+            + \" was expected to complete by returning false\",', nan]\n",
      "['+        fs.isFile(src));', nan]\n",
      "['expectUnknownStore(() -> fs.isDirectory(src));', nan]\n",
      "['expectUnknownStore(() -> fs.mkdirs(src));', nan]\n",
      "['expectUnknownStore(() -> fs.delete(src));', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEC.java b/h', 'doop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEC.java']\n",
      "['index f086a08201c..1c395b2adcf 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEC.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEC.java', nan]\n",
      "['@@ -20,8 +20,13 @@', nan]\n",
      "[nan, nan]\n",
      "['import java.io.IOException;', nan]\n",
      "['import java.nio.file.AccessDeniedException;', nan]\n",
      "['+import java.util.Arrays;', nan]\n",
      "['+import java.util.Collection;', nan]\n",
      "[nan, nan]\n",
      "['+import org.assertj.core.api.Assertions;', nan]\n",
      "['import org.junit.Test;', nan]\n",
      "['+import org.junit.runner.RunWith;', nan]\n",
      "['+import org.junit.runners.Parameterized;', nan]\n",
      "[nan, nan]\n",
      "['import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['import org.apache.hadoop.fs.FileSystem;', nan]\n",
      "['@@ -31,13 +36,26 @@', nan]\n",
      "['import org.apache.hadoop.io.IOUtils;', nan]\n",
      "[nan, nan]\n",
      "['import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;', nan]\n",
      "['+import static org.apache.hadoop.fs.contract.ContractTestUtils.touch;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_KEEP;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.ETAG_CHECKSUM_ENABLED;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.S3_METADATA_STORE_IMPL;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.SERVER_SIDE_ENCRYPTION_KEY;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3ATestUtils.*;', nan]\n",
      "['import static org.apache.hadoop.test.LambdaTestUtils.intercept;', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* Concrete class that extends {@link AbstractTestS3AEncryption}', nan]\n",
      "['* and tests SSE-C encryption.', nan]\n",
      "['+ * HEAD requests against SSE-C-encrypted data will fail if the wrong key', nan]\n",
      "['+ * is presented, so the tests are very brittle to S3Guard being on vs. off.', nan]\n",
      "['+ * Equally \"vexing\" has been the optimizations of getFileStatus(), wherein', nan]\n",
      "['+ * LIST comes before HEAD path + /', nan]\n",
      "['*/', nan]\n",
      "['+@RunWith(Parameterized.class)', nan]\n",
      "['public class ITestS3AEncryptionSSEC extends AbstractTestS3AEncryption {', nan]\n",
      "[nan, nan]\n",
      "['private static final String SERVICE_AMAZON_S3_STATUS_CODE_403', nan]\n",
      "['@@ -52,18 +70,67 @@', nan]\n",
      "['= \"msdo3VvvZznp66Gth58a91Hxe/UpExMkwU9BHkIjfW8=\";', nan]\n",
      "['private static final int TEST_FILE_LEN = 2048;', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameterization.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Parameterized.Parameters(name = \"{0}\")', nan]\n",
      "['+  public static Collection<Object[]> params() {', nan]\n",
      "['+    return Arrays.asList(new Object[][]{', nan]\n",
      "['+        {\"raw-keep-markers\", false, true},', nan]\n",
      "['+        {\"raw-delete-markers\", false, false},', nan]\n",
      "['+        {\"guarded-keep-markers\", true, true},', nan]\n",
      "['+        {\"guarded-delete-markers\", true, false}', nan]\n",
      "['+    });', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameter: should the stores be guarded?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean s3guard;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameter: should directory markers be retained?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean keepMarkers;', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Filesystem created with a different key.', nan]\n",
      "['*/', nan]\n",
      "['-  private FileSystem fsKeyB;', nan]\n",
      "['+  private S3AFileSystem fsKeyB;', nan]\n",
      "['+', nan]\n",
      "['+  public ITestS3AEncryptionSSEC(final String name,', nan]\n",
      "['+      final boolean s3guard,', nan]\n",
      "['+      final boolean keepMarkers) {', nan]\n",
      "['+    this.s3guard = s3guard;', nan]\n",
      "['+    this.keepMarkers = keepMarkers;', nan]\n",
      "['+  }', nan]\n",
      "[nan, nan]\n",
      "['@Override', nan]\n",
      "['protected Configuration createConfiguration() {', nan]\n",
      "['Configuration conf = super.createConfiguration();', nan]\n",
      "['disableFilesystemCaching(conf);', nan]\n",
      "['-    conf.set(Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM,', nan]\n",
      "['+    String bucketName = getTestBucketName(conf);', nan]\n",
      "['+    removeBucketOverrides(bucketName, conf,', nan]\n",
      "['+        S3_METADATA_STORE_IMPL);', nan]\n",
      "['+    if (!s3guard) {', nan]\n",
      "['+      // in a raw run remove all s3guard settings', nan]\n",
      "['+      removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['+          S3_METADATA_STORE_IMPL);', nan]\n",
      "['+    }', nan]\n",
      "['+    // directory marker options', nan]\n",
      "['+    removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        ETAG_CHECKSUM_ENABLED,', nan]\n",
      "['+        SERVER_SIDE_ENCRYPTION_ALGORITHM,', nan]\n",
      "['+        SERVER_SIDE_ENCRYPTION_KEY);', nan]\n",
      "['+    conf.set(DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        keepMarkers', nan]\n",
      "['+            ? DIRECTORY_MARKER_POLICY_KEEP', nan]\n",
      "['+            : DIRECTORY_MARKER_POLICY_DELETE);', nan]\n",
      "['+    conf.set(SERVER_SIDE_ENCRYPTION_ALGORITHM,', nan]\n",
      "['getSSEAlgorithm().getMethod());', nan]\n",
      "['-    conf.set(Constants.SERVER_SIDE_ENCRYPTION_KEY, KEY_1);', nan]\n",
      "['+    conf.set(SERVER_SIDE_ENCRYPTION_KEY, KEY_1);', nan]\n",
      "['+    conf.setBoolean(ETAG_CHECKSUM_ENABLED, true);', nan]\n",
      "['return conf;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@@ -109,31 +176,19 @@ public void testCreateFileAndReadWithDifferentEncryptionKey() throws', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['-   * While each object has its own key and should be distinct, this verifies', nan]\n",
      "['-   * that hadoop treats object keys as a filesystem path.  So if a top level', nan]\n",
      "['-   * dir is encrypted with keyA, a sublevel dir cannot be accessed with a', nan]\n",
      "['-   * different keyB.', nan]\n",
      "['-   *', nan]\n",
      "['-   * This is expected AWS S3 SSE-C behavior.', nan]\n",
      "['*', nan]\n",
      "['+   * You can use a different key under a sub directory, even if you', nan]\n",
      "['+   * do not have permissions to read the marker.', nan]\n",
      "['* @throws Exception', nan]\n",
      "['*/', nan]\n",
      "['@Test', nan]\n",
      "['public void testCreateSubdirWithDifferentKey() throws Exception {', nan]\n",
      "['-    requireUnguardedFilesystem();', nan]\n",
      "['-', nan]\n",
      "['-    intercept(AccessDeniedException.class,', nan]\n",
      "['-        SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          Path base = path(\"testCreateSubdirWithDifferentKey\");', nan]\n",
      "['-          Path nestedDirectory = new Path(base, \"nestedDir\");', nan]\n",
      "['-          fsKeyB = createNewFileSystemWithSSECKey(', nan]\n",
      "['-              KEY_2);', nan]\n",
      "['-          getFileSystem().mkdirs(base);', nan]\n",
      "['-          fsKeyB.mkdirs(nestedDirectory);', nan]\n",
      "['-          // expected to fail', nan]\n",
      "['-          return fsKeyB.getFileStatus(nestedDirectory);', nan]\n",
      "['-        });', nan]\n",
      "['+    Path base = path(\"testCreateSubdirWithDifferentKey\");', nan]\n",
      "['+    Path nestedDirectory = new Path(base, \"nestedDir\");', nan]\n",
      "['+    fsKeyB = createNewFileSystemWithSSECKey(', nan]\n",
      "['+        KEY_2);', nan]\n",
      "['+    getFileSystem().mkdirs(base);', nan]\n",
      "['+    fsKeyB.mkdirs(nestedDirectory);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -176,14 +231,11 @@ public void testRenameFile() throws Exception {', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['-   * It is possible to list the contents of a directory up to the actual', nan]\n",
      "['-   * end of the nested directories.  This is due to how S3A mocks the', nan]\n",
      "['-   * directories and how prefixes work in S3.', nan]\n",
      "['+   * Directory listings always work.', nan]\n",
      "['* @throws Exception', nan]\n",
      "['*/', nan]\n",
      "['@Test', nan]\n",
      "['public void testListEncryptedDir() throws Exception {', nan]\n",
      "['-    requireUnguardedFilesystem();', nan]\n",
      "[nan, nan]\n",
      "['Path pathABC = path(\"testListEncryptedDir/a/b/c/\");', nan]\n",
      "['Path pathAB = pathABC.getParent();', nan]\n",
      "['@@ -196,17 +248,11 @@ public void testListEncryptedDir() throws Exception {', nan]\n",
      "[nan, nan]\n",
      "['fsKeyB.listFiles(pathA, true);', nan]\n",
      "['fsKeyB.listFiles(pathAB, true);', nan]\n",
      "['-', nan]\n",
      "['-    //Until this point, no exception is thrown about access', nan]\n",
      "['-    intercept(AccessDeniedException.class,', nan]\n",
      "['-        SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          fsKeyB.listFiles(pathABC, false);', nan]\n",
      "['-        });', nan]\n",
      "['+    fsKeyB.listFiles(pathABC, false);', nan]\n",
      "[nan, nan]\n",
      "['Configuration conf = this.createConfiguration();', nan]\n",
      "['-    conf.unset(Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM);', nan]\n",
      "['-    conf.unset(Constants.SERVER_SIDE_ENCRYPTION_KEY);', nan]\n",
      "['+    conf.unset(SERVER_SIDE_ENCRYPTION_ALGORITHM);', nan]\n",
      "['+    conf.unset(SERVER_SIDE_ENCRYPTION_KEY);', nan]\n",
      "[nan, nan]\n",
      "['S3AContract contract = (S3AContract) createContract(conf);', nan]\n",
      "['contract.init();', nan]\n",
      "['@@ -215,20 +261,14 @@ public void testListEncryptedDir() throws Exception {', nan]\n",
      "['//unencrypted can access until the final directory', nan]\n",
      "['unencryptedFileSystem.listFiles(pathA, true);', nan]\n",
      "['unencryptedFileSystem.listFiles(pathAB, true);', nan]\n",
      "['-    AWSBadRequestException ex = intercept(AWSBadRequestException.class,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          unencryptedFileSystem.listFiles(pathABC, false);', nan]\n",
      "['-        });', nan]\n",
      "['+    unencryptedFileSystem.listFiles(pathABC, false);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['-   * Much like the above list encrypted directory test, you cannot get the', nan]\n",
      "['-   * metadata of an object without the correct encryption key.', nan]\n",
      "['-   * @throws Exception', nan]\n",
      "['+   * listStatus also works with encrypted directories and key mismatch.', nan]\n",
      "['*/', nan]\n",
      "['@Test', nan]\n",
      "['public void testListStatusEncryptedDir() throws Exception {', nan]\n",
      "['-    requireUnguardedFilesystem();', nan]\n",
      "[nan, nan]\n",
      "['Path pathABC = path(\"testListStatusEncryptedDir/a/b/c/\");', nan]\n",
      "['Path pathAB = pathABC.getParent();', nan]\n",
      "['@@ -240,17 +280,14 @@ public void testListStatusEncryptedDir() throws Exception {', nan]\n",
      "['fsKeyB.listStatus(pathA);', nan]\n",
      "['fsKeyB.listStatus(pathAB);', nan]\n",
      "[nan, nan]\n",
      "['-    //Until this point, no exception is thrown about access', nan]\n",
      "['-    intercept(AccessDeniedException.class,', nan]\n",
      "['-        SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          fsKeyB.listStatus(pathABC);', nan]\n",
      "['-        });', nan]\n",
      "['+    // this used to raise 403, but with LIST before HEAD,', nan]\n",
      "['+    // no longer true.', nan]\n",
      "['+    fsKeyB.listStatus(pathABC);', nan]\n",
      "[nan, nan]\n",
      "['//Now try it with an unencrypted filesystem.', nan]\n",
      "['Configuration conf = createConfiguration();', nan]\n",
      "['-    conf.unset(Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM);', nan]\n",
      "['-    conf.unset(Constants.SERVER_SIDE_ENCRYPTION_KEY);', nan]\n",
      "['+    conf.unset(SERVER_SIDE_ENCRYPTION_ALGORITHM);', nan]\n",
      "['+    conf.unset(SERVER_SIDE_ENCRYPTION_KEY);', nan]\n",
      "[nan, nan]\n",
      "['S3AContract contract = (S3AContract) createContract(conf);', nan]\n",
      "['contract.init();', nan]\n",
      "['@@ -259,21 +296,15 @@ public void testListStatusEncryptedDir() throws Exception {', nan]\n",
      "['//unencrypted can access until the final directory', nan]\n",
      "['unencryptedFileSystem.listStatus(pathA);', nan]\n",
      "['unencryptedFileSystem.listStatus(pathAB);', nan]\n",
      "['-', nan]\n",
      "['-    intercept(AWSBadRequestException.class,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          unencryptedFileSystem.listStatus(pathABC);', nan]\n",
      "['-        });', nan]\n",
      "['+    unencryptedFileSystem.listStatus(pathABC);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['-   * Much like trying to access a encrypted directory, an encrypted file cannot', nan]\n",
      "['-   * have its metadata read, since both are technically an object.', nan]\n",
      "['+   * An encrypted file cannot have its metadata read.', nan]\n",
      "['* @throws Exception', nan]\n",
      "['*/', nan]\n",
      "['@Test', nan]\n",
      "['public void testListStatusEncryptedFile() throws Exception {', nan]\n",
      "['-    requireUnguardedFilesystem();', nan]\n",
      "['Path pathABC = path(\"testListStatusEncryptedFile/a/b/c/\");', nan]\n",
      "['assertTrue(\"mkdirs failed\", getFileSystem().mkdirs(pathABC));', nan]\n",
      "[nan, nan]\n",
      "['@@ -283,23 +314,15 @@ public void testListStatusEncryptedFile() throws Exception {', nan]\n",
      "['fsKeyB = createNewFileSystemWithSSECKey(KEY_4);', nan]\n",
      "[nan, nan]\n",
      "['//Until this point, no exception is thrown about access', nan]\n",
      "['-    intercept(AccessDeniedException.class,', nan]\n",
      "['-        SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          fsKeyB.listStatus(fileToStat);', nan]\n",
      "['-        });', nan]\n",
      "['+    if (!fsKeyB.hasMetadataStore()) {', nan]\n",
      "['+      intercept(AccessDeniedException.class,', nan]\n",
      "['+          SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['+          () -> fsKeyB.listStatus(fileToStat));', nan]\n",
      "['+    } else {', nan]\n",
      "['+      fsKeyB.listStatus(fileToStat);', nan]\n",
      "['+    }', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-  /**', nan]\n",
      "['-   * Skip the test case if S3Guard is enabled; generally this is because', nan]\n",
      "['-   * list and GetFileStatus calls can succeed even with different keys.', nan]\n",
      "['-   */', nan]\n",
      "['-  protected void requireUnguardedFilesystem() {', nan]\n",
      "['-    assume(\"Filesystem has a metastore\",', nan]\n",
      "['-        !getFileSystem().hasMetadataStore());', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-', nan]\n",
      "['/**', nan]\n",
      "['* It is possible to delete directories without the proper encryption key and', nan]\n",
      "['* the hierarchy above it.', nan]\n",
      "['@@ -308,7 +331,7 @@ protected void requireUnguardedFilesystem() {', nan]\n",
      "['*/', nan]\n",
      "['@Test', nan]\n",
      "['public void testDeleteEncryptedObjectWithDifferentKey() throws Exception {', nan]\n",
      "['-    requireUnguardedFilesystem();', nan]\n",
      "['+    //requireUnguardedFilesystem();', nan]\n",
      "['Path pathABC = path(\"testDeleteEncryptedObjectWithDifferentKey/a/b/c/\");', nan]\n",
      "[nan, nan]\n",
      "['Path pathAB = pathABC.getParent();', nan]\n",
      "['@@ -317,12 +340,13 @@ public void testDeleteEncryptedObjectWithDifferentKey() throws Exception {', nan]\n",
      "['Path fileToDelete = new Path(pathABC, \"filetobedeleted.txt\");', nan]\n",
      "['writeThenReadFile(fileToDelete, TEST_FILE_LEN);', nan]\n",
      "['fsKeyB = createNewFileSystemWithSSECKey(KEY_4);', nan]\n",
      "['-    intercept(AccessDeniedException.class,', nan]\n",
      "['-        SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['-        () -> {', nan]\n",
      "['-          fsKeyB.delete(fileToDelete, false);', nan]\n",
      "['-        });', nan]\n",
      "['-', nan]\n",
      "['+    if (!fsKeyB.hasMetadataStore()) {', nan]\n",
      "['+      intercept(AccessDeniedException.class,', nan]\n",
      "['+          SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['+          () -> fsKeyB.delete(fileToDelete, false));', nan]\n",
      "['+    } else {', nan]\n",
      "['+      fsKeyB.delete(fileToDelete, false);', nan]\n",
      "['+    }', nan]\n",
      "['//This is possible', nan]\n",
      "['fsKeyB.delete(pathABC, true);', nan]\n",
      "['fsKeyB.delete(pathAB, true);', nan]\n",
      "['@@ -330,15 +354,33 @@ public void testDeleteEncryptedObjectWithDifferentKey() throws Exception {', nan]\n",
      "['assertPathDoesNotExist(\"expected recursive delete\", fileToDelete);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-  private FileSystem createNewFileSystemWithSSECKey(String sseCKey) throws', nan]\n",
      "['+  /**', nan]\n",
      "['+   * getFileChecksum always goes to S3, so when', nan]\n",
      "['+   * the caller lacks permissions, it fails irrespective', nan]\n",
      "['+   * of guard.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testChecksumRequiresReadAccess() throws Throwable {', nan]\n",
      "['+    Path path = path(\"tagged-file\");', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    touch(fs, path);', nan]\n",
      "['+    Assertions.assertThat(fs.getFileChecksum(path))', nan]\n",
      "['+        .isNotNull();', nan]\n",
      "['+    fsKeyB = createNewFileSystemWithSSECKey(KEY_4);', nan]\n",
      "['+    intercept(AccessDeniedException.class,', nan]\n",
      "['+        SERVICE_AMAZON_S3_STATUS_CODE_403,', nan]\n",
      "['+        () -> fsKeyB.getFileChecksum(path));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  private S3AFileSystem createNewFileSystemWithSSECKey(String sseCKey) throws', nan]\n",
      "['IOException {', nan]\n",
      "['Configuration conf = this.createConfiguration();', nan]\n",
      "['-    conf.set(Constants.SERVER_SIDE_ENCRYPTION_KEY, sseCKey);', nan]\n",
      "['+    conf.set(SERVER_SIDE_ENCRYPTION_KEY, sseCKey);', nan]\n",
      "[nan, nan]\n",
      "['S3AContract contract = (S3AContract) createContract(conf);', nan]\n",
      "['contract.init();', nan]\n",
      "['FileSystem fileSystem = contract.getTestFileSystem();', nan]\n",
      "['-    return fileSystem;', nan]\n",
      "['+    return (S3AFileSystem) fileSystem;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Override', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileOperationCost.java', '/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileOperationCost.java']\n",
      "['index e54fd97a6af..46e6f5fcea7 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileOperationCost.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileOperationCost.java', nan]\n",
      "['@@ -18,15 +18,14 @@', nan]\n",
      "[nan, nan]\n",
      "['package org.apache.hadoop.fs.s3a;', nan]\n",
      "[nan, nan]\n",
      "['-import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['-import org.apache.hadoop.fs.FSDataOutputStream;', nan]\n",
      "['-import org.apache.hadoop.fs.FileStatus;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.FileAlreadyExistsException;', nan]\n",
      "['import org.apache.hadoop.fs.FileSystem;', nan]\n",
      "['import org.apache.hadoop.fs.Path;', nan]\n",
      "['-import org.apache.hadoop.fs.contract.ContractTestUtils;', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.StatusProbeEnum;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.performance.AbstractS3ACostTest;', nan]\n",
      "['+', nan]\n",
      "[nan, nan]\n",
      "['-import org.assertj.core.api.Assertions;', nan]\n",
      "['import org.junit.Test;', nan]\n",
      "['import org.junit.runner.RunWith;', nan]\n",
      "['import org.junit.runners.Parameterized;', nan]\n",
      "['@@ -39,26 +38,21 @@', nan]\n",
      "['import java.util.Arrays;', nan]\n",
      "['import java.util.Collection;', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import java.util.EnumSet;', nan]\n",
      "['-import java.util.UUID;', nan]\n",
      "['-import java.util.concurrent.Callable;', nan]\n",
      "['+', nan]\n",
      "[nan, nan]\n",
      "['import static org.apache.hadoop.fs.contract.ContractTestUtils.*;', nan]\n",
      "['-import static org.apache.hadoop.fs.s3a.Constants.S3_METADATA_STORE_IMPL;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Statistic.*;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3ATestUtils.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.performance.OperationCost.*;', nan]\n",
      "['import static org.apache.hadoop.test.GenericTestUtils.getTestDir;', nan]\n",
      "['import static org.apache.hadoop.test.LambdaTestUtils.intercept;', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['- * Use metrics to assert about the cost of file status queries.', nan]\n",
      "['- * {@link S3AFileSystem#getFileStatus(Path)}.', nan]\n",
      "['- * Parameterized on guarded vs raw.', nan]\n",
      "['+ * Use metrics to assert about the cost of file API calls.', nan]\n",
      "['+ * Parameterized on guarded vs raw. and directory marker keep vs delete', nan]\n",
      "['*/', nan]\n",
      "['@RunWith(Parameterized.class)', nan]\n",
      "['-public class ITestS3AFileOperationCost extends AbstractS3ATestBase {', nan]\n",
      "['-', nan]\n",
      "['-  private MetricDiff metadataRequests;', nan]\n",
      "['-  private MetricDiff listRequests;', nan]\n",
      "['+public class ITestS3AFileOperationCost extends AbstractS3ACostTest {', nan]\n",
      "[nan, nan]\n",
      "['private static final Logger LOG =', nan]\n",
      "['LoggerFactory.getLogger(ITestS3AFileOperationCost.class);', nan]\n",
      "['@@ -69,103 +63,62 @@', nan]\n",
      "['@Parameterized.Parameters(name = \"{0}\")', nan]\n",
      "['public static Collection<Object[]> params() {', nan]\n",
      "['return Arrays.asList(new Object[][]{', nan]\n",
      "['-        {\"raw\", false},', nan]\n",
      "['-        {\"guarded\", true}', nan]\n",
      "['+        {\"raw-keep-markers\", false, true, false},', nan]\n",
      "['+        {\"raw-delete-markers\", false, false, false},', nan]\n",
      "['+        {\"nonauth-keep-markers\", true, true, false},', nan]\n",
      "['+        {\"auth-delete-markers\", true, false, true}', nan]\n",
      "['});', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-  private final String name;', nan]\n",
      "['-', nan]\n",
      "['-  private final boolean s3guard;', nan]\n",
      "['-', nan]\n",
      "['-  public ITestS3AFileOperationCost(final String name, final boolean s3guard) {', nan]\n",
      "['-    this.name = name;', nan]\n",
      "['-    this.s3guard = s3guard;', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-  @Override', nan]\n",
      "['-  public Configuration createConfiguration() {', nan]\n",
      "['-    Configuration conf = super.createConfiguration();', nan]\n",
      "['-    String bucketName = getTestBucketName(conf);', nan]\n",
      "['-    removeBucketOverrides(bucketName, conf,', nan]\n",
      "['-        S3_METADATA_STORE_IMPL);', nan]\n",
      "['-    if (!s3guard) {', nan]\n",
      "['-      // in a raw run remove all s3guard settings', nan]\n",
      "['-      removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['-          S3_METADATA_STORE_IMPL);', nan]\n",
      "['-    }', nan]\n",
      "['-    disableFilesystemCaching(conf);', nan]\n",
      "['-    return conf;', nan]\n",
      "['-  }', nan]\n",
      "['-  @Override', nan]\n",
      "['-  public void setup() throws Exception {', nan]\n",
      "['-    super.setup();', nan]\n",
      "['-    if (s3guard) {', nan]\n",
      "['-      // s3guard is required for those test runs where any of the', nan]\n",
      "['-      // guard options are set', nan]\n",
      "['-      assumeS3GuardState(true, getConfiguration());', nan]\n",
      "['-    }', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    metadataRequests = new MetricDiff(fs, OBJECT_METADATA_REQUESTS);', nan]\n",
      "['-    listRequests = new MetricDiff(fs, OBJECT_LIST_REQUESTS);', nan]\n",
      "['-    skipDuringFaultInjection(fs);', nan]\n",
      "['+  public ITestS3AFileOperationCost(final String name,', nan]\n",
      "['+      final boolean s3guard,', nan]\n",
      "['+      final boolean keepMarkers,', nan]\n",
      "['+      final boolean authoritative) {', nan]\n",
      "['+    super(s3guard, keepMarkers, authoritative);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Test the cost of {@code listLocatedStatus(file)}.', nan]\n",
      "[\"+   * There's a minor inefficiency in that calling this on\", nan]\n",
      "['+   * a file in S3Guard still executes a LIST call, even', nan]\n",
      "['+   * though the file record is in the store.', nan]\n",
      "['+   */', nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfLocatedFileStatusOnFile() throws Throwable {', nan]\n",
      "['describe(\"performing listLocatedStatus on a file\");', nan]\n",
      "['-    Path file = path(getMethodName() + \".txt\");', nan]\n",
      "['+    Path file = file(methodPath());', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    touch(fs, file);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    fs.listLocatedStatus(file);', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      // Unguarded FS.', nan]\n",
      "['-      metadataRequests.assertDiffEquals(1);', nan]\n",
      "['-    }', nan]\n",
      "['-    listRequests.assertDiffEquals(1);', nan]\n",
      "['+    verifyMetrics(() -> fs.listLocatedStatus(file),', nan]\n",
      "['+        whenRaw(FILE_STATUS_FILE_PROBE', nan]\n",
      "['+            .plus(LIST_LOCATED_STATUS_LIST_OP)),', nan]\n",
      "['+        whenAuthoritative(LIST_LOCATED_STATUS_LIST_OP),', nan]\n",
      "['+        whenNonauth(LIST_LOCATED_STATUS_LIST_OP));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfListLocatedStatusOnEmptyDir() throws Throwable {', nan]\n",
      "['describe(\"performing listLocatedStatus on an empty dir\");', nan]\n",
      "['-    Path dir = path(getMethodName());', nan]\n",
      "['+    Path dir = dir(methodPath());', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    fs.mkdirs(dir);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    fs.listLocatedStatus(dir);', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      // Unguarded FS.', nan]\n",
      "['-      verifyOperationCount(2, 1);', nan]\n",
      "['-    } else {', nan]\n",
      "['-      if (fs.allowAuthoritative(dir)) {', nan]\n",
      "['-        verifyOperationCount(0, 0);', nan]\n",
      "['-      } else {', nan]\n",
      "['-        verifyOperationCount(0, 1);', nan]\n",
      "['-      }', nan]\n",
      "['-    }', nan]\n",
      "['+    verifyMetrics(() ->', nan]\n",
      "['+            fs.listLocatedStatus(dir),', nan]\n",
      "['+        whenRaw(LIST_LOCATED_STATUS_LIST_OP', nan]\n",
      "['+            .plus(GET_FILE_STATUS_ON_EMPTY_DIR)),', nan]\n",
      "['+        whenAuthoritative(NO_IO),', nan]\n",
      "['+        whenNonauth(LIST_LOCATED_STATUS_LIST_OP));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfListLocatedStatusOnNonEmptyDir() throws Throwable {', nan]\n",
      "['describe(\"performing listLocatedStatus on a non empty dir\");', nan]\n",
      "['-    Path dir = path(getMethodName() + \"dir\");', nan]\n",
      "['+    Path dir = dir(methodPath());', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    fs.mkdirs(dir);', nan]\n",
      "['-    Path file = new Path(dir, \"file.txt\");', nan]\n",
      "['-    touch(fs, file);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    fs.listLocatedStatus(dir);', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      // Unguarded FS.', nan]\n",
      "['-      verifyOperationCount(0, 1);', nan]\n",
      "['-    } else {', nan]\n",
      "['-      if(fs.allowAuthoritative(dir)) {', nan]\n",
      "['-        verifyOperationCount(0, 0);', nan]\n",
      "['-      } else {', nan]\n",
      "['-        verifyOperationCount(0, 1);', nan]\n",
      "['-      }', nan]\n",
      "['-    }', nan]\n",
      "['+    Path file = file(new Path(dir, \"file.txt\"));', nan]\n",
      "['+    verifyMetrics(() ->', nan]\n",
      "['+          fs.listLocatedStatus(dir),', nan]\n",
      "['+        whenRaw(LIST_LOCATED_STATUS_LIST_OP),', nan]\n",
      "['+        whenAuthoritative(NO_IO),', nan]\n",
      "['+        whenNonauth(LIST_LOCATED_STATUS_LIST_OP));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['@@ -174,36 +127,27 @@ public void testCostOfListFilesOnFile() throws Throwable {', nan]\n",
      "['Path file = path(getMethodName() + \".txt\");', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['touch(fs, file);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    fs.listFiles(file, true);', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      metadataRequests.assertDiffEquals(1);', nan]\n",
      "['-    } else {', nan]\n",
      "['-      if (fs.allowAuthoritative(file)) {', nan]\n",
      "['-        listRequests.assertDiffEquals(0);', nan]\n",
      "['-      } else {', nan]\n",
      "['-        listRequests.assertDiffEquals(1);', nan]\n",
      "['-      }', nan]\n",
      "['-    }', nan]\n",
      "['+    verifyMetrics(() ->', nan]\n",
      "['+            fs.listFiles(file, true),', nan]\n",
      "['+        whenRaw(LIST_LOCATED_STATUS_LIST_OP', nan]\n",
      "['+            .plus(GET_FILE_STATUS_ON_FILE)),', nan]\n",
      "['+        whenAuthoritative(NO_IO),', nan]\n",
      "['+        whenNonauth(LIST_LOCATED_STATUS_LIST_OP));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfListFilesOnEmptyDir() throws Throwable {', nan]\n",
      "['-    describe(\"Performing listFiles() on an empty dir\");', nan]\n",
      "['+    describe(\"Perpforming listFiles() on an empty dir with marker\");', nan]\n",
      "['+    // this attem', nan]\n",
      "['Path dir = path(getMethodName());', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['fs.mkdirs(dir);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    fs.listFiles(dir, true);', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      verifyOperationCount(2, 1);', nan]\n",
      "['-    } else {', nan]\n",
      "['-      if (fs.allowAuthoritative(dir)) {', nan]\n",
      "['-        verifyOperationCount(0, 0);', nan]\n",
      "['-      } else {', nan]\n",
      "['-        verifyOperationCount(0, 1);', nan]\n",
      "['-      }', nan]\n",
      "['-    }', nan]\n",
      "['+    verifyMetrics(() ->', nan]\n",
      "['+            fs.listFiles(dir, true),', nan]\n",
      "['+        whenRaw(LIST_FILES_LIST_OP', nan]\n",
      "['+            .plus(GET_FILE_STATUS_ON_EMPTY_DIR)),', nan]\n",
      "['+        whenAuthoritative(NO_IO),', nan]\n",
      "['+        whenNonauth(LIST_FILES_LIST_OP));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['@@ -214,17 +158,11 @@ public void testCostOfListFilesOnNonEmptyDir() throws Throwable {', nan]\n",
      "['fs.mkdirs(dir);', nan]\n",
      "['Path file = new Path(dir, \"file.txt\");', nan]\n",
      "['touch(fs, file);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    fs.listFiles(dir, true);', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      verifyOperationCount(0, 1);', nan]\n",
      "['-    } else {', nan]\n",
      "['-      if (fs.allowAuthoritative(dir)) {', nan]\n",
      "['-        verifyOperationCount(0, 0);', nan]\n",
      "['-      } else {', nan]\n",
      "['-        verifyOperationCount(0, 1);', nan]\n",
      "['-      }', nan]\n",
      "['-    }', nan]\n",
      "['+    verifyMetrics(() ->', nan]\n",
      "['+            fs.listFiles(dir, true),', nan]\n",
      "['+        whenRaw(LIST_FILES_LIST_OP),', nan]\n",
      "['+        whenAuthoritative(NO_IO),', nan]\n",
      "['+        whenNonauth(LIST_FILES_LIST_OP));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['@@ -232,118 +170,70 @@ public void testCostOfListFilesOnNonExistingDir() throws Throwable {', nan]\n",
      "['describe(\"Performing listFiles() on a non existing dir\");', nan]\n",
      "['Path dir = path(getMethodName());', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    intercept(FileNotFoundException.class,', nan]\n",
      "['-        () -> fs.listFiles(dir, true));', nan]\n",
      "['-    verifyOperationCount(2, 2);', nan]\n",
      "['+    verifyMetricsIntercepting(FileNotFoundException.class, \"\",', nan]\n",
      "['+        () -> fs.listFiles(dir, true),', nan]\n",
      "['+        whenRaw(LIST_FILES_LIST_OP', nan]\n",
      "['+            .plus(GET_FILE_STATUS_FNFE)));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfGetFileStatusOnFile() throws Throwable {', nan]\n",
      "['describe(\"performing getFileStatus on a file\");', nan]\n",
      "['-    Path simpleFile = path(\"simple.txt\");', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    touch(fs, simpleFile);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    FileStatus status = fs.getFileStatus(simpleFile);', nan]\n",
      "['+    Path simpleFile = file(methodPath());', nan]\n",
      "['+    S3AFileStatus status = verifyRawInnerGetFileStatus(simpleFile, true,', nan]\n",
      "['+        StatusProbeEnum.ALL,', nan]\n",
      "['+        GET_FILE_STATUS_ON_FILE);', nan]\n",
      "['assertTrue(\"not a file: \" + status, status.isFile());', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      metadataRequests.assertDiffEquals(1);', nan]\n",
      "['-    }', nan]\n",
      "['-    listRequests.assertDiffEquals(0);', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-  private void resetMetricDiffs() {', nan]\n",
      "['-    reset(metadataRequests, listRequests);', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-  /**', nan]\n",
      "['-   * Verify that the head and list calls match expectations,', nan]\n",
      "['-   * then reset the counters ready for the next operation.', nan]\n",
      "['-   * @param head expected HEAD count', nan]\n",
      "['-   * @param list expected LIST count', nan]\n",
      "['-   */', nan]\n",
      "['-  private void verifyOperationCount(int head, int list) {', nan]\n",
      "['-    metadataRequests.assertDiffEquals(head);', nan]\n",
      "['-    listRequests.assertDiffEquals(list);', nan]\n",
      "['-    metadataRequests.reset();', nan]\n",
      "['-    listRequests.reset();', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfGetFileStatusOnEmptyDir() throws Throwable {', nan]\n",
      "['describe(\"performing getFileStatus on an empty directory\");', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    Path dir = path(\"empty\");', nan]\n",
      "['-    fs.mkdirs(dir);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    S3AFileStatus status = fs.innerGetFileStatus(dir, true,', nan]\n",
      "['-        StatusProbeEnum.ALL);', nan]\n",
      "['+    Path dir = dir(methodPath());', nan]\n",
      "['+    S3AFileStatus status = verifyRawInnerGetFileStatus(dir, true,', nan]\n",
      "['+        StatusProbeEnum.ALL,', nan]\n",
      "['+        GET_FILE_STATUS_ON_DIR_MARKER);', nan]\n",
      "['assertSame(\"not empty: \" + status, Tristate.TRUE,', nan]\n",
      "['status.isEmptyDirectory());', nan]\n",
      "['-', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      metadataRequests.assertDiffEquals(2);', nan]\n",
      "['-    }', nan]\n",
      "['-    listRequests.assertDiffEquals(0);', nan]\n",
      "['-', nan]\n",
      "['// but now only ask for the directories and the file check is skipped.', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    fs.innerGetFileStatus(dir, false,', nan]\n",
      "['-        StatusProbeEnum.DIRECTORIES);', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      metadataRequests.assertDiffEquals(1);', nan]\n",
      "['-    }', nan]\n",
      "['+    verifyRawInnerGetFileStatus(dir, false,', nan]\n",
      "['+        StatusProbeEnum.DIRECTORIES,', nan]\n",
      "['+        FILE_STATUS_DIR_PROBE);', nan]\n",
      "['+', nan]\n",
      "['+    // now look at isFile/isDir against the same entry', nan]\n",
      "['+    isDir(dir, true, FILE_STATUS_DIR_PROBE);', nan]\n",
      "['+    isFile(dir, false, FILE_STATUS_FILE_PROBE);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfGetFileStatusOnMissingFile() throws Throwable {', nan]\n",
      "['describe(\"performing getFileStatus on a missing file\");', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    Path path = path(\"missing\");', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    intercept(FileNotFoundException.class,', nan]\n",
      "['-        () -> fs.getFileStatus(path));', nan]\n",
      "['-    metadataRequests.assertDiffEquals(2);', nan]\n",
      "['-    listRequests.assertDiffEquals(1);', nan]\n",
      "['+    interceptRawGetFileStatusFNFE(methodPath(), false,', nan]\n",
      "['+        StatusProbeEnum.ALL,', nan]\n",
      "['+        GET_FILE_STATUS_FNFE);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['-  public void testCostOfGetFileStatusOnMissingSubPath() throws Throwable {', nan]\n",
      "['-    describe(\"performing getFileStatus on a missing file\");', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    Path path = path(\"missingdir/missingpath\");', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    intercept(FileNotFoundException.class,', nan]\n",
      "['-        () -> fs.getFileStatus(path));', nan]\n",
      "['-    metadataRequests.assertDiffEquals(2);', nan]\n",
      "['-    listRequests.assertDiffEquals(1);', nan]\n",
      "['+  public void testIsDirIsFileMissingPath() throws Throwable {', nan]\n",
      "['+    describe(\"performing isDir and isFile on a missing file\");', nan]\n",
      "['+    Path path = methodPath();', nan]\n",
      "['+    // now look at isFile/isDir against the same entry', nan]\n",
      "['+    isDir(path, false,', nan]\n",
      "['+        FILE_STATUS_DIR_PROBE);', nan]\n",
      "['+    isFile(path, false,', nan]\n",
      "['+        FILE_STATUS_FILE_PROBE);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfGetFileStatusOnNonEmptyDir() throws Throwable {', nan]\n",
      "['describe(\"performing getFileStatus on a non-empty directory\");', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    Path dir = path(\"empty\");', nan]\n",
      "['-    fs.mkdirs(dir);', nan]\n",
      "['-    Path simpleFile = new Path(dir, \"simple.txt\");', nan]\n",
      "['-    touch(fs, simpleFile);', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    S3AFileStatus status = fs.innerGetFileStatus(dir, true,', nan]\n",
      "['-        StatusProbeEnum.ALL);', nan]\n",
      "['-    if (status.isEmptyDirectory() == Tristate.TRUE) {', nan]\n",
      "['-      // erroneous state', nan]\n",
      "['-      String fsState = fs.toString();', nan]\n",
      "['-      fail(\"FileStatus says directory isempty: \" + status', nan]\n",
      "['-          + \"\\\\n\" + ContractTestUtils.ls(fs, dir)', nan]\n",
      "['-          + \"\\\\n\" + fsState);', nan]\n",
      "['-    }', nan]\n",
      "['-    if (!fs.hasMetadataStore()) {', nan]\n",
      "['-      metadataRequests.assertDiffEquals(2);', nan]\n",
      "['-      listRequests.assertDiffEquals(1);', nan]\n",
      "['-    }', nan]\n",
      "['+    Path dir = dir(methodPath());', nan]\n",
      "['+    file(new Path(dir, \"simple.txt\"));', nan]\n",
      "['+    S3AFileStatus status = verifyRawInnerGetFileStatus(dir, true,', nan]\n",
      "['+        StatusProbeEnum.ALL,', nan]\n",
      "['+        GET_FILE_STATUS_ON_DIR);', nan]\n",
      "['+    assertEmptyDirStatus(status, Tristate.FALSE);', nan]\n",
      "['}', nan]\n",
      "['-', nan]\n",
      "['@Test', nan]\n",
      "['public void testCostOfCopyFromLocalFile() throws Throwable {', nan]\n",
      "['describe(\"testCostOfCopyFromLocalFile\");', nan]\n",
      "['@@ -361,19 +251,18 @@ public void testCostOfCopyFromLocalFile() throws Throwable {', nan]\n",
      "[\"byte[] data = dataset(len, 'A', 'Z');\", nan]\n",
      "['writeDataset(localFS, localPath, data, len, 1024, true);', nan]\n",
      "['S3AFileSystem s3a = getFileSystem();', nan]\n",
      "['-      MetricDiff copyLocalOps = new MetricDiff(s3a,', nan]\n",
      "['-          INVOCATION_COPY_FROM_LOCAL_FILE);', nan]\n",
      "['-      MetricDiff putRequests = new MetricDiff(s3a,', nan]\n",
      "['-          OBJECT_PUT_REQUESTS);', nan]\n",
      "['-      MetricDiff putBytes = new MetricDiff(s3a,', nan]\n",
      "['-          OBJECT_PUT_BYTES);', nan]\n",
      "['-', nan]\n",
      "['-      Path remotePath = path(\"copied\");', nan]\n",
      "['-      s3a.copyFromLocalFile(false, true, localPath, remotePath);', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+      Path remotePath = methodPath();', nan]\n",
      "['+', nan]\n",
      "['+      verifyMetrics(() -> {', nan]\n",
      "['+        s3a.copyFromLocalFile(false, true, localPath, remotePath);', nan]\n",
      "['+        return \"copy\";', nan]\n",
      "['+      },', nan]\n",
      "['+          with(INVOCATION_COPY_FROM_LOCAL_FILE, 1),', nan]\n",
      "['+          with(OBJECT_PUT_REQUESTS, 1),', nan]\n",
      "['+          with(OBJECT_PUT_BYTES, len));', nan]\n",
      "['verifyFileContents(s3a, remotePath, data);', nan]\n",
      "['-      copyLocalOps.assertDiffEquals(1);', nan]\n",
      "['-      putRequests.assertDiffEquals(1);', nan]\n",
      "['-      putBytes.assertDiffEquals(len);', nan]\n",
      "['// print final stats', nan]\n",
      "['LOG.info(\"Filesystem {}\", s3a);', nan]\n",
      "['} finally {', nan]\n",
      "['@@ -381,268 +270,123 @@ public void testCostOfCopyFromLocalFile() throws Throwable {', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-  private boolean reset(MetricDiff... diffs) {', nan]\n",
      "['-    for (MetricDiff diff : diffs) {', nan]\n",
      "['-      diff.reset();', nan]\n",
      "['-    }', nan]\n",
      "['-    return true;', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-  @Test', nan]\n",
      "['-  public void testFakeDirectoryDeletion() throws Throwable {', nan]\n",
      "['-    describe(\"Verify whether create file works after renaming a file. \"', nan]\n",
      "['-        + \"In S3, rename deletes any fake directories as a part of \"', nan]\n",
      "['-        + \"clean up activity\");', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-', nan]\n",
      "['-    Path srcBaseDir = path(\"src\");', nan]\n",
      "['-    mkdirs(srcBaseDir);', nan]\n",
      "['-    MetricDiff deleteRequests =', nan]\n",
      "['-        new MetricDiff(fs, Statistic.OBJECT_DELETE_REQUESTS);', nan]\n",
      "['-    MetricDiff directoriesDeleted =', nan]\n",
      "['-        new MetricDiff(fs, Statistic.DIRECTORIES_DELETED);', nan]\n",
      "['-    MetricDiff fakeDirectoriesDeleted =', nan]\n",
      "['-        new MetricDiff(fs, Statistic.FAKE_DIRECTORIES_DELETED);', nan]\n",
      "['-    MetricDiff directoriesCreated =', nan]\n",
      "['-        new MetricDiff(fs, Statistic.DIRECTORIES_CREATED);', nan]\n",
      "['-', nan]\n",
      "['-    // when you call toString() on this, you get the stats', nan]\n",
      "['-    // so it gets auto-evaluated in log calls.', nan]\n",
      "['-    Object summary = new Object() {', nan]\n",
      "['-      @Override', nan]\n",
      "['-      public String toString() {', nan]\n",
      "['-        return String.format(\"[%s, %s, %s, %s]\",', nan]\n",
      "['-            directoriesCreated, directoriesDeleted,', nan]\n",
      "['-            deleteRequests, fakeDirectoriesDeleted);', nan]\n",
      "['-      }', nan]\n",
      "['-    };', nan]\n",
      "['-', nan]\n",
      "['-    // reset operation to invoke', nan]\n",
      "['-    Callable<Boolean> reset = () ->', nan]\n",
      "['-        reset(deleteRequests, directoriesCreated, directoriesDeleted,', nan]\n",
      "['-          fakeDirectoriesDeleted);', nan]\n",
      "['-', nan]\n",
      "['-    Path srcDir = new Path(srcBaseDir, \"1/2/3/4/5/6\");', nan]\n",
      "['-    int srcDirDepth = directoriesInPath(srcDir);', nan]\n",
      "['-    // one dir created, one removed', nan]\n",
      "['-    mkdirs(srcDir);', nan]\n",
      "['-    String state = \"after mkdir(srcDir) \" + summary;', nan]\n",
      "['-    directoriesCreated.assertDiffEquals(state, 1);', nan]\n",
      "['-    deleteRequests.assertDiffEquals(state, 1);', nan]\n",
      "['-    directoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-    // HADOOP-14255 deletes unnecessary fake directory objects in mkdirs()', nan]\n",
      "['-    fakeDirectoriesDeleted.assertDiffEquals(state, srcDirDepth - 1);', nan]\n",
      "['-    reset.call();', nan]\n",
      "['-', nan]\n",
      "['-    // creating a file should trigger demise of the src dir', nan]\n",
      "['-    final Path srcFilePath = new Path(srcDir, \"source.txt\");', nan]\n",
      "['-    touch(fs, srcFilePath);', nan]\n",
      "['-    state = \"after touch(fs, srcFilePath) \" + summary;', nan]\n",
      "['-    deleteRequests.assertDiffEquals(state, 1);', nan]\n",
      "['-    directoriesCreated.assertDiffEquals(state, 0);', nan]\n",
      "['-    directoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-    fakeDirectoriesDeleted.assertDiffEquals(state, srcDirDepth);', nan]\n",
      "['-', nan]\n",
      "['-    reset.call();', nan]\n",
      "['-', nan]\n",
      "['-    // create a directory tree, expect the dir to be created and', nan]\n",
      "['-    // a request to delete all parent directories made.', nan]\n",
      "['-    Path destBaseDir = path(\"dest\");', nan]\n",
      "['-    Path destDir = new Path(destBaseDir, \"1/2/3/4/5/6\");', nan]\n",
      "['-    Path destFilePath = new Path(destDir, \"dest.txt\");', nan]\n",
      "['-    mkdirs(destDir);', nan]\n",
      "['-    state = \"after mkdir(destDir) \" + summary;', nan]\n",
      "['-', nan]\n",
      "['-    int destDirDepth = directoriesInPath(destDir);', nan]\n",
      "['-    directoriesCreated.assertDiffEquals(state, 1);', nan]\n",
      "['-    deleteRequests.assertDiffEquals(state, 1);', nan]\n",
      "['-    directoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-    fakeDirectoriesDeleted.assertDiffEquals(state, destDirDepth - 1);', nan]\n",
      "['-', nan]\n",
      "['-    // create a new source file.', nan]\n",
      "['-    // Explicitly use a new path object to guarantee that the parent paths', nan]\n",
      "['-    // are different object instances', nan]\n",
      "['-    final Path srcFile2 = new Path(srcDir.toUri() + \"/source2.txt\");', nan]\n",
      "['-    touch(fs, srcFile2);', nan]\n",
      "['-', nan]\n",
      "['-    reset.call();', nan]\n",
      "['-', nan]\n",
      "['-    // rename the source file to the destination file.', nan]\n",
      "['-    // this tests the file rename path, not the dir rename path', nan]\n",
      "['-    // as srcFile2 exists, the parent dir of srcFilePath must not be created.', nan]\n",
      "['-    fs.rename(srcFilePath, destFilePath);', nan]\n",
      "['-    state = String.format(\"after rename(srcFilePath, destFilePath)\"', nan]\n",
      "['-            + \" %s dest dir depth=%d\",', nan]\n",
      "['-        summary,', nan]\n",
      "['-        destDirDepth);', nan]\n",
      "['-', nan]\n",
      "['-    directoriesCreated.assertDiffEquals(state, 0);', nan]\n",
      "['-    // one for the renamed file, one for the parent of the dest dir', nan]\n",
      "['-    deleteRequests.assertDiffEquals(state, 2);', nan]\n",
      "['-    directoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-    fakeDirectoriesDeleted.assertDiffEquals(state, destDirDepth);', nan]\n",
      "['-', nan]\n",
      "[\"-    // these asserts come after the checks on iop counts, so they don't\", nan]\n",
      "['-    // interfere', nan]\n",
      "['-    assertIsFile(destFilePath);', nan]\n",
      "['-    assertIsDirectory(srcDir);', nan]\n",
      "['-    assertPathDoesNotExist(\"should have gone in the rename\", srcFilePath);', nan]\n",
      "['-    reset.call();', nan]\n",
      "['-', nan]\n",
      "['-    // rename the source file2 to the (no longer existing', nan]\n",
      "['-    // this tests the file rename path, not the dir rename path', nan]\n",
      "['-    // as srcFile2 exists, the parent dir of srcFilePath must not be created.', nan]\n",
      "['-    fs.rename(srcFile2, srcFilePath);', nan]\n",
      "['-    state = String.format(\"after rename(%s, %s) %s dest dir depth=%d\",', nan]\n",
      "['-        srcFile2, srcFilePath,', nan]\n",
      "['-        summary,', nan]\n",
      "['-        destDirDepth);', nan]\n",
      "['-', nan]\n",
      "['-    // here we expect there to be no fake directories', nan]\n",
      "['-    directoriesCreated.assertDiffEquals(state, 0);', nan]\n",
      "['-    // one for the renamed file only', nan]\n",
      "['-    deleteRequests.assertDiffEquals(state, 1);', nan]\n",
      "['-    directoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-    fakeDirectoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['-  private int directoriesInPath(Path path) {', nan]\n",
      "['-    return path.isRoot() ? 0 : 1 + directoriesInPath(path.getParent());', nan]\n",
      "['-  }', nan]\n",
      "['-', nan]\n",
      "['@Test', nan]\n",
      "['-  public void testCostOfRootRename() throws Throwable {', nan]\n",
      "['-    describe(\"assert that a root directory rename doesn\\'t\"', nan]\n",
      "['-        + \" do much in terms of parent dir operations\");', nan]\n",
      "['+  public void testDirProbes() throws Throwable {', nan]\n",
      "['+    describe(\"Test directory probe cost\");', nan]\n",
      "['+    assumeUnguarded();', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    // Create the empty directory.', nan]\n",
      "['+    Path emptydir = dir(methodPath());', nan]\n",
      "[nan, nan]\n",
      "[\"-    // unique name, so that even when run in parallel tests, there's no conflict\", nan]\n",
      "['-    String uuid = UUID.randomUUID().toString();', nan]\n",
      "['-    Path src = new Path(\"/src-\" + uuid);', nan]\n",
      "['-    Path dest = new Path(\"/dest-\" + uuid);', nan]\n",
      "['+    // head probe fails', nan]\n",
      "['+    interceptRawGetFileStatusFNFE(emptydir, false,', nan]\n",
      "['+        StatusProbeEnum.HEAD_ONLY,', nan]\n",
      "['+        FILE_STATUS_FILE_PROBE);', nan]\n",
      "[nan, nan]\n",
      "['-    try {', nan]\n",
      "['-      MetricDiff deleteRequests =', nan]\n",
      "['-          new MetricDiff(fs, Statistic.OBJECT_DELETE_REQUESTS);', nan]\n",
      "['-      MetricDiff directoriesDeleted =', nan]\n",
      "['-          new MetricDiff(fs, Statistic.DIRECTORIES_DELETED);', nan]\n",
      "['-      MetricDiff fakeDirectoriesDeleted =', nan]\n",
      "['-          new MetricDiff(fs, Statistic.FAKE_DIRECTORIES_DELETED);', nan]\n",
      "['-      MetricDiff directoriesCreated =', nan]\n",
      "['-          new MetricDiff(fs, Statistic.DIRECTORIES_CREATED);', nan]\n",
      "['-      touch(fs, src);', nan]\n",
      "['-      fs.rename(src, dest);', nan]\n",
      "['-      Object summary = new Object() {', nan]\n",
      "['-        @Override', nan]\n",
      "['-        public String toString() {', nan]\n",
      "['-          return String.format(\"[%s, %s, %s, %s]\",', nan]\n",
      "['-              directoriesCreated, directoriesDeleted,', nan]\n",
      "['-              deleteRequests, fakeDirectoriesDeleted);', nan]\n",
      "['-        }', nan]\n",
      "['-      };', nan]\n",
      "['-', nan]\n",
      "['-      String state = String.format(\"after touch(%s) %s\",', nan]\n",
      "['-          src, summary);', nan]\n",
      "['-      touch(fs, src);', nan]\n",
      "['-      fs.rename(src, dest);', nan]\n",
      "['-      directoriesCreated.assertDiffEquals(state, 0);', nan]\n",
      "['-', nan]\n",
      "['-', nan]\n",
      "['-      state = String.format(\"after rename(%s, %s) %s\",', nan]\n",
      "['-          src, dest, summary);', nan]\n",
      "['-      // here we expect there to be no fake directories', nan]\n",
      "['-      directoriesCreated.assertDiffEquals(state, 0);', nan]\n",
      "['-      // one for the renamed file only', nan]\n",
      "['-      deleteRequests.assertDiffEquals(state, 1);', nan]\n",
      "['-      directoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-      fakeDirectoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-', nan]\n",
      "['-      // delete that destination file, assert only the file delete was issued', nan]\n",
      "['-      reset(deleteRequests, directoriesCreated, directoriesDeleted,', nan]\n",
      "['-          fakeDirectoriesDeleted);', nan]\n",
      "['-', nan]\n",
      "['-      fs.delete(dest, false);', nan]\n",
      "['-      // here we expect there to be no fake directories', nan]\n",
      "['-      directoriesCreated.assertDiffEquals(state, 0);', nan]\n",
      "['-      // one for the deleted file', nan]\n",
      "['-      deleteRequests.assertDiffEquals(state, 1);', nan]\n",
      "['-      directoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-      fakeDirectoriesDeleted.assertDiffEquals(state, 0);', nan]\n",
      "['-    } finally {', nan]\n",
      "['-      fs.delete(src, false);', nan]\n",
      "['-      fs.delete(dest, false);', nan]\n",
      "['-    }', nan]\n",
      "['-  }', nan]\n",
      "['+    // a LIST will find it and declare as empty', nan]\n",
      "['+    S3AFileStatus status = verifyRawInnerGetFileStatus(emptydir, true,', nan]\n",
      "['+        StatusProbeEnum.LIST_ONLY,', nan]\n",
      "['+        FILE_STATUS_DIR_PROBE);', nan]\n",
      "['+    assertEmptyDirStatus(status, Tristate.TRUE);', nan]\n",
      "[nan, nan]\n",
      "['-  @Test', nan]\n",
      "['-  public void testDirProbes() throws Throwable {', nan]\n",
      "['-    describe(\"Test directory probe cost -raw only\");', nan]\n",
      "['-    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    assume(\"Unguarded FS only\", !fs.hasMetadataStore());', nan]\n",
      "['-    String dir = \"testEmptyDirHeadProbe\";', nan]\n",
      "['-    Path emptydir = path(dir);', nan]\n",
      "['-    // Create the empty directory.', nan]\n",
      "['-    fs.mkdirs(emptydir);', nan]\n",
      "['-', nan]\n",
      "['-    // metrics and assertions.', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-', nan]\n",
      "['-    intercept(FileNotFoundException.class, () ->', nan]\n",
      "['-        fs.innerGetFileStatus(emptydir, false,', nan]\n",
      "['-            StatusProbeEnum.HEAD_ONLY));', nan]\n",
      "['-    verifyOperationCount(1, 0);', nan]\n",
      "['-', nan]\n",
      "[\"-    // a LIST will find it -but it doesn't consider it an empty dir.\", nan]\n",
      "['-    S3AFileStatus status = fs.innerGetFileStatus(emptydir, true,', nan]\n",
      "['-        StatusProbeEnum.LIST_ONLY);', nan]\n",
      "['-    verifyOperationCount(0, 1);', nan]\n",
      "['-    Assertions.assertThat(status)', nan]\n",
      "['-        .describedAs(\"LIST output is not considered empty\")', nan]\n",
      "['-        .matches(s -> !s.isEmptyDirectory().equals(Tristate.TRUE),  \"is empty\");', nan]\n",
      "['-', nan]\n",
      "['-    // finally, skip all probes and expect no operations toThere are', nan]\n",
      "['-    // take place', nan]\n",
      "['-    intercept(FileNotFoundException.class, () ->', nan]\n",
      "['-        fs.innerGetFileStatus(emptydir, false,', nan]\n",
      "['-            EnumSet.noneOf(StatusProbeEnum.class)));', nan]\n",
      "['-    verifyOperationCount(0, 0);', nan]\n",
      "['+    // skip all probes and expect no operations to take place', nan]\n",
      "['+    interceptRawGetFileStatusFNFE(emptydir, false,', nan]\n",
      "['+        EnumSet.noneOf(StatusProbeEnum.class),', nan]\n",
      "['+        NO_IO);', nan]\n",
      "[nan, nan]\n",
      "['// now add a trailing slash to the key and use the', nan]\n",
      "['// deep internal s3GetFileStatus method call.', nan]\n",
      "['String emptyDirTrailingSlash = fs.pathToKey(emptydir.getParent())', nan]\n",
      "['-        + \"/\" + dir +  \"/\";', nan]\n",
      "['+        + \"/\" + emptydir.getName() +  \"/\";', nan]\n",
      "['// A HEAD request does not probe for keys with a trailing /', nan]\n",
      "['-    intercept(FileNotFoundException.class, () ->', nan]\n",
      "['+    interceptRaw(FileNotFoundException.class, \"\",', nan]\n",
      "['+        NO_IO, () ->', nan]\n",
      "['fs.s3GetFileStatus(emptydir, emptyDirTrailingSlash,', nan]\n",
      "['-            StatusProbeEnum.HEAD_ONLY, null));', nan]\n",
      "['-    verifyOperationCount(0, 0);', nan]\n",
      "['+            StatusProbeEnum.HEAD_ONLY, null, false));', nan]\n",
      "[nan, nan]\n",
      "['// but ask for a directory marker and you get the entry', nan]\n",
      "['-    status = fs.s3GetFileStatus(emptydir,', nan]\n",
      "['-        emptyDirTrailingSlash,', nan]\n",
      "['-        StatusProbeEnum.DIR_MARKER_ONLY, null);', nan]\n",
      "['-    verifyOperationCount(1, 0);', nan]\n",
      "['+    status = verifyRaw(FILE_STATUS_DIR_PROBE, () ->', nan]\n",
      "['+        fs.s3GetFileStatus(emptydir,', nan]\n",
      "['+            emptyDirTrailingSlash,', nan]\n",
      "['+            StatusProbeEnum.LIST_ONLY,', nan]\n",
      "['+            null,', nan]\n",
      "['+            true));', nan]\n",
      "['assertEquals(emptydir, status.getPath());', nan]\n",
      "['+    assertEmptyDirStatus(status, Tristate.TRUE);', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['-  public void testCreateCost() throws Throwable {', nan]\n",
      "['-    describe(\"Test file creation cost -raw only\");', nan]\n",
      "['+  public void testNeedEmptyDirectoryProbeRequiresList() throws Throwable {', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['-    assume(\"Unguarded FS only\", !fs.hasMetadataStore());', nan]\n",
      "['-    resetMetricDiffs();', nan]\n",
      "['-    Path testFile = path(\"testCreateCost\");', nan]\n",
      "[nan, nan]\n",
      "['+    intercept(IllegalArgumentException.class, \"\", () ->', nan]\n",
      "['+            fs.s3GetFileStatus(new Path(\"/something\"), \"/something\",', nan]\n",
      "['+                StatusProbeEnum.HEAD_ONLY, null, true));', nan]\n",
      "['+  }', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCreateCost() throws Throwable {', nan]\n",
      "['+    describe(\"Test file creation cost -raw only\");', nan]\n",
      "['+    assumeUnguarded();', nan]\n",
      "['+    Path testFile = methodPath();', nan]\n",
      "['// when overwrite is false, the path is checked for existence.', nan]\n",
      "['-    try (FSDataOutputStream out = fs.create(testFile, false)) {', nan]\n",
      "['-      verifyOperationCount(2, 1);', nan]\n",
      "['-    }', nan]\n",
      "['-', nan]\n",
      "['+    create(testFile, false,', nan]\n",
      "['+        CREATE_FILE_NO_OVERWRITE);', nan]\n",
      "['// but when true: only the directory checks take place.', nan]\n",
      "['-    try (FSDataOutputStream out = fs.create(testFile, true)) {', nan]\n",
      "['-      verifyOperationCount(1, 1);', nan]\n",
      "['-    }', nan]\n",
      "['+    create(testFile, true, CREATE_FILE_OVERWRITE);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCreateCostFileExists() throws Throwable {', nan]\n",
      "['+    describe(\"Test cost of create file failing with existing file\");', nan]\n",
      "['+    assumeUnguarded();', nan]\n",
      "['+    Path testFile = file(methodPath());', nan]\n",
      "['+', nan]\n",
      "['+    // now there is a file there, an attempt with overwrite == false will', nan]\n",
      "['+    // fail on the first HEAD.', nan]\n",
      "['+    interceptRaw(FileAlreadyExistsException.class, \"\",', nan]\n",
      "['+        FILE_STATUS_FILE_PROBE,', nan]\n",
      "['+        () -> file(testFile, false));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCreateCostDirExists() throws Throwable {', nan]\n",
      "['+    describe(\"Test cost of create file failing with existing dir\");', nan]\n",
      "['+    assumeUnguarded();', nan]\n",
      "['+    Path testFile = dir(methodPath());', nan]\n",
      "['+', nan]\n",
      "['+    // now there is a file there, an attempt with overwrite == false will', nan]\n",
      "['+    // fail on the first HEAD.', nan]\n",
      "['+    interceptRaw(FileAlreadyExistsException.class, \"\",', nan]\n",
      "['+        GET_FILE_STATUS_ON_DIR_MARKER,', nan]\n",
      "['+        () -> file(testFile, false));', nan]\n",
      "['+  }', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Use the builder API.', nan]\n",
      "['+   * This always looks for a parent unless the caller says otherwise.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCreateBuilder() throws Throwable {', nan]\n",
      "['+    describe(\"Test builder file creation cost -raw only\");', nan]\n",
      "['+    assumeUnguarded();', nan]\n",
      "['+    Path testFile = methodPath();', nan]\n",
      "['+    dir(testFile.getParent());', nan]\n",
      "['+', nan]\n",
      "['+    // builder defaults to looking for parent existence (non-recursive)', nan]\n",
      "['+    buildFile(testFile, false,  false,', nan]\n",
      "['+        GET_FILE_STATUS_FNFE                // destination file', nan]\n",
      "['+            .plus(FILE_STATUS_DIR_PROBE));  // parent dir', nan]\n",
      "['+    // recursive = false and overwrite=true:', nan]\n",
      "[\"+    // only make sure the dest path isn't a directory.\", nan]\n",
      "['+    buildFile(testFile, true, true,', nan]\n",
      "['+        FILE_STATUS_DIR_PROBE);', nan]\n",
      "['+', nan]\n",
      "['+    // now there is a file there, an attempt with overwrite == false will', nan]\n",
      "['+    // fail on the first HEAD.', nan]\n",
      "['+    interceptRaw(FileAlreadyExistsException.class, \"\",', nan]\n",
      "['+        GET_FILE_STATUS_ON_FILE,', nan]\n",
      "['+        () -> buildFile(testFile, false, true,', nan]\n",
      "['+            GET_FILE_STATUS_ON_FILE));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['@@ -656,15 +400,15 @@ public void testCostOfGlobStatus() throws Throwable {', nan]\n",
      "['// create a bunch of files', nan]\n",
      "['int filesToCreate = 10;', nan]\n",
      "['for (int i = 0; i < filesToCreate; i++) {', nan]\n",
      "['-      try (FSDataOutputStream out = fs.create(basePath.suffix(\"/\" + i))) {', nan]\n",
      "['-        verifyOperationCount(1, 1);', nan]\n",
      "['-      }', nan]\n",
      "['+      create(basePath.suffix(\"/\" + i));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['fs.globStatus(basePath.suffix(\"/*\"));', nan]\n",
      "['// 2 head + 1 list from getFileStatus on path,', nan]\n",
      "['// plus 1 list to match the glob pattern', nan]\n",
      "['-    verifyOperationCount(2, 2);', nan]\n",
      "['+    verifyRaw(GET_FILE_STATUS_ON_DIR', nan]\n",
      "['+        .plus(LIST_OPERATION),', nan]\n",
      "['+        () -> fs.globStatus(basePath.suffix(\"/*\")));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@Test', nan]\n",
      "['@@ -678,14 +422,14 @@ public void testCostOfGlobStatusNoSymlinkResolution() throws Throwable {', nan]\n",
      "['// create a single file, globStatus returning a single file on a pattern', nan]\n",
      "['// triggers attempts at symlinks resolution if configured', nan]\n",
      "['String fileName = \"/notASymlinkDOntResolveMeLikeOne\";', nan]\n",
      "['-    try (FSDataOutputStream out = fs.create(basePath.suffix(fileName))) {', nan]\n",
      "['-      verifyOperationCount(1, 1);', nan]\n",
      "['-    }', nan]\n",
      "['-', nan]\n",
      "['-    fs.globStatus(basePath.suffix(\"/*\"));', nan]\n",
      "['+    create(basePath.suffix(fileName));', nan]\n",
      "['// unguarded: 2 head + 1 list from getFileStatus on path,', nan]\n",
      "['// plus 1 list to match the glob pattern', nan]\n",
      "['// no additional operations from symlink resolution', nan]\n",
      "['-    verifyOperationCount(2, 2);', nan]\n",
      "['+    verifyRaw(GET_FILE_STATUS_ON_DIR', nan]\n",
      "['+        .plus(LIST_OPERATION),', nan]\n",
      "['+        () -> fs.globStatus(basePath.suffix(\"/*\")));', nan]\n",
      "['}', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ARemoteFileChanged.java', '/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ARemoteFileChanged.java']\n",
      "['index 3fd70be9319..01a14ef8e93 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ARemoteFileChanged.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ARemoteFileChanged.java', nan]\n",
      "['@@ -326,7 +326,7 @@ protected Path path() throws IOException {', nan]\n",
      "['* @return a number >= 0.', nan]\n",
      "['*/', nan]\n",
      "['private int getFileStatusHeadCount() {', nan]\n",
      "['-    return authMode ? 0 : 1;', nan]\n",
      "['+    return authMode ? 0 : 0;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardEmptyDirs.java b/ha', 'oop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardEmptyDirs.java']\n",
      "['index ab81491c4cf..db3c2b6c274 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardEmptyDirs.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardEmptyDirs.java', nan]\n",
      "['@@ -26,6 +26,7 @@', nan]\n",
      "['import com.amazonaws.services.s3.AmazonS3;', nan]\n",
      "['import com.amazonaws.services.s3.model.ListObjectsV2Request;', nan]\n",
      "['import com.amazonaws.services.s3.model.ListObjectsV2Result;', nan]\n",
      "['+import com.amazonaws.services.s3.model.ObjectMetadata;', nan]\n",
      "['import com.amazonaws.services.s3.model.PutObjectRequest;', nan]\n",
      "['import com.amazonaws.services.s3.model.S3ObjectSummary;', nan]\n",
      "['import org.assertj.core.api.Assertions;', nan]\n",
      "['@@ -57,6 +58,10 @@', nan]\n",
      "['*/', nan]\n",
      "['public class ITestS3GuardEmptyDirs extends AbstractS3ATestBase {', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Rename an empty directory, verify that the empty dir', nan]\n",
      "['+   * marker moves in both S3Guard and in the S3A FS.', nan]\n",
      "['+   */', nan]\n",
      "['@Test', nan]\n",
      "['public void testRenameEmptyDir() throws Throwable {', nan]\n",
      "['S3AFileSystem fs = getFileSystem();', nan]\n",
      "['@@ -67,7 +72,7 @@ public void testRenameEmptyDir() throws Throwable {', nan]\n",
      "['String destDirMarker = fs.pathToKey(destDir) + \"/\";', nan]\n",
      "['// set things up.', nan]\n",
      "['mkdirs(sourceDir);', nan]\n",
      "[\"-    // there'a source directory marker\", nan]\n",
      "[\"+    // there's source directory marker\", nan]\n",
      "['fs.getObjectMetadata(sourceDirMarker);', nan]\n",
      "['S3AFileStatus srcStatus = getEmptyDirStatus(sourceDir);', nan]\n",
      "['assertEquals(\"Must be an empty dir: \" + srcStatus, Tristate.TRUE,', nan]\n",
      "['@@ -82,8 +87,12 @@ public void testRenameEmptyDir() throws Throwable {', nan]\n",
      "['() -> getEmptyDirStatus(sourceDir));', nan]\n",
      "[\"// and verify that there's no dir marker hidden under a tombstone\", nan]\n",
      "['intercept(FileNotFoundException.class,', nan]\n",
      "['-        () -> Invoker.once(\"HEAD\", sourceDirMarker,', nan]\n",
      "['-            () -> fs.getObjectMetadata(sourceDirMarker)));', nan]\n",
      "['+        () -> Invoker.once(\"HEAD\", sourceDirMarker, () -> {', nan]\n",
      "['+          ObjectMetadata md = fs.getObjectMetadata(sourceDirMarker);', nan]\n",
      "['+          return String.format(\"Object %s of length %d\",', nan]\n",
      "['+              sourceDirMarker, md.getInstanceLength());', nan]\n",
      "['+        }));', nan]\n",
      "['+', nan]\n",
      "[\"// the parent dir mustn't be confused\", nan]\n",
      "['S3AFileStatus baseStatus = getEmptyDirStatus(basePath);', nan]\n",
      "['assertEquals(\"Must not be an empty dir: \" + baseStatus, Tristate.FALSE,', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardListConsistency.jav', 'b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardListConsistency.ja']\n",
      "['index 3c67e252e6e..0246b5415f1 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardListConsistency.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardListConsistency.java', nan]\n",
      "['@@ -31,6 +31,7 @@', nan]\n",
      "['import org.apache.hadoop.fs.contract.ContractTestUtils;', nan]\n",
      "['import org.apache.hadoop.fs.contract.s3a.S3AContract;', nan]\n",
      "[nan, nan]\n",
      "['+import com.amazonaws.services.s3.model.S3ObjectSummary;', nan]\n",
      "['import com.google.common.collect.Lists;', nan]\n",
      "['import org.assertj.core.api.Assertions;', nan]\n",
      "['import org.junit.Assume;', nan]\n",
      "['@@ -560,24 +561,23 @@ public void testInconsistentS3ClientDeletes() throws Throwable {', nan]\n",
      "['+ \" paths\");', nan]\n",
      "[nan, nan]\n",
      "['ListObjectsV2Result postDeleteDelimited = listObjectsV2(fs, key, \"/\");', nan]\n",
      "['-    assertListSizeEqual(', nan]\n",
      "['+    boolean stripTombstones = false;', nan]\n",
      "['+    assertObjectSummariesEqual(', nan]\n",
      "['\"InconsistentAmazonS3Client added back objects incorrectly \" +', nan]\n",
      "['\"in a non-recursive listing\",', nan]\n",
      "['-        preDeleteDelimited.getObjectSummaries(),', nan]\n",
      "['-        postDeleteDelimited.getObjectSummaries());', nan]\n",
      "['+        preDeleteDelimited, postDeleteDelimited,', nan]\n",
      "['+        stripTombstones);', nan]\n",
      "[nan, nan]\n",
      "['assertListSizeEqual(\"InconsistentAmazonS3Client added back prefixes incorrectly \" +', nan]\n",
      "['\"in a non-recursive listing\",', nan]\n",
      "['preDeleteDelimited.getCommonPrefixes(),', nan]\n",
      "['-        postDeleteDelimited.getCommonPrefixes()', nan]\n",
      "['-    );', nan]\n",
      "['+        postDeleteDelimited.getCommonPrefixes());', nan]\n",
      "['LOG.info(\"Executing Deep listing\");', nan]\n",
      "['ListObjectsV2Result postDeleteUndelimited = listObjectsV2(fs, key, null);', nan]\n",
      "['-    assertListSizeEqual(\"InconsistentAmazonS3Client added back objects incorrectly \" +', nan]\n",
      "['-            \"in a recursive listing\",', nan]\n",
      "['-        preDeleteUndelimited.getObjectSummaries(),', nan]\n",
      "['-        postDeleteUndelimited.getObjectSummaries()', nan]\n",
      "['-    );', nan]\n",
      "['+    assertObjectSummariesEqual(\"InconsistentAmazonS3Client added back objects\"', nan]\n",
      "['+            + \" incorrectly in a recursive listing\",', nan]\n",
      "['+        preDeleteUndelimited, postDeleteUndelimited,', nan]\n",
      "['+        stripTombstones);', nan]\n",
      "[nan, nan]\n",
      "['assertListSizeEqual(\"InconsistentAmazonS3Client added back prefixes incorrectly \" +', nan]\n",
      "['\"in a recursive listing\",', nan]\n",
      "['@@ -586,6 +586,24 @@ public void testInconsistentS3ClientDeletes() throws Throwable {', nan]\n",
      "[');', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  private void assertObjectSummariesEqual(final String message,', nan]\n",
      "['+      final ListObjectsV2Result expected,', nan]\n",
      "['+      final ListObjectsV2Result actual,', nan]\n",
      "['+      final boolean stripTombstones) {', nan]\n",
      "['+    assertCollectionsEqual(', nan]\n",
      "['+        message,', nan]\n",
      "['+        stringify(expected.getObjectSummaries(), stripTombstones),', nan]\n",
      "['+        stringify(actual.getObjectSummaries(), stripTombstones));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  List<String> stringify(List<S3ObjectSummary> objects,', nan]\n",
      "['+      boolean stripTombstones) {', nan]\n",
      "['+    return objects.stream()', nan]\n",
      "['+        .filter(s -> !stripTombstones || !(s.getKey().endsWith(\"/\")))', nan]\n",
      "['+        .map(s -> s.getKey())', nan]\n",
      "['+        .collect(Collectors.toList());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Require the v2 S3 list API.', nan]\n",
      "['*/', nan]\n",
      "['@@ -682,6 +700,22 @@ public void testListingReturnsVersionMetadata() throws Throwable {', nan]\n",
      "['versionId, locatedFileStatus.getVersionId());', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert that the two collections match using', nan]\n",
      "['+   * object equality of the elements within.', nan]\n",
      "['+   * @param message text for the assertion', nan]\n",
      "['+   * @param expected expected list', nan]\n",
      "['+   * @param actual actual list', nan]\n",
      "['+   * @param <T> type of list', nan]\n",
      "['+   */', nan]\n",
      "['+  private <T> void assertCollectionsEqual(String message,', nan]\n",
      "['+      Collection<T> expected,', nan]\n",
      "['+      Collection<T> actual) {', nan]\n",
      "['+    Assertions.assertThat(actual)', nan]\n",
      "['+        .describedAs(message)', nan]\n",
      "['+        .containsExactlyInAnyOrderElementsOf(expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Assert that the two list sizes match; failure message includes the lists.', nan]\n",
      "['* @param message text for the assertion', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardOutOfBandOperations', 'java b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardOutOfBandOpera']\n",
      "['index 32ead7f3fed..2d4173d1c2a 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardOutOfBandOperations.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardOutOfBandOperations.java', nan]\n",
      "['@@ -56,7 +56,8 @@', nan]\n",
      "['import static org.apache.hadoop.fs.contract.ContractTestUtils.touch;', nan]\n",
      "['import static org.apache.hadoop.fs.contract.ContractTestUtils.writeTextFile;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.AUTHORITATIVE_PATH;', nan]\n",
      "['-import static org.apache.hadoop.fs.s3a.Constants.DEFAULT_METADATASTORE_METADATA_TTL;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.CHANGE_DETECT_MODE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.CHANGE_DETECT_MODE_NONE;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.METADATASTORE_AUTHORITATIVE;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.METADATASTORE_METADATA_TTL;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.RETRY_INTERVAL;', nan]\n",
      "['@@ -169,12 +170,16 @@ protected Configuration createConfiguration() {', nan]\n",
      "['RETRY_LIMIT,', nan]\n",
      "['RETRY_INTERVAL,', nan]\n",
      "['S3GUARD_CONSISTENCY_RETRY_INTERVAL,', nan]\n",
      "['-        S3GUARD_CONSISTENCY_RETRY_LIMIT);', nan]\n",
      "['+        S3GUARD_CONSISTENCY_RETRY_LIMIT,', nan]\n",
      "['+        CHANGE_DETECT_MODE,', nan]\n",
      "['+        METADATASTORE_METADATA_TTL);', nan]\n",
      "['conf.setInt(RETRY_LIMIT, 3);', nan]\n",
      "['conf.setInt(S3GUARD_CONSISTENCY_RETRY_LIMIT, 3);', nan]\n",
      "['+    conf.set(CHANGE_DETECT_MODE, CHANGE_DETECT_MODE_NONE);', nan]\n",
      "['final String delay = \"10ms\";', nan]\n",
      "['conf.set(RETRY_INTERVAL, delay);', nan]\n",
      "['conf.set(S3GUARD_CONSISTENCY_RETRY_INTERVAL, delay);', nan]\n",
      "['+    conf.set(METADATASTORE_METADATA_TTL, delay);', nan]\n",
      "['return conf;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@@ -232,12 +237,13 @@ private S3AFileSystem createGuardedFS(boolean authoritativeMode)', nan]\n",
      "['URI uri = testFS.getUri();', nan]\n",
      "[nan, nan]\n",
      "['removeBaseAndBucketOverrides(uri.getHost(), config,', nan]\n",
      "['+        CHANGE_DETECT_MODE,', nan]\n",
      "['METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['METADATASTORE_METADATA_TTL,', nan]\n",
      "['AUTHORITATIVE_PATH);', nan]\n",
      "['config.setBoolean(METADATASTORE_AUTHORITATIVE, authoritativeMode);', nan]\n",
      "['config.setLong(METADATASTORE_METADATA_TTL,', nan]\n",
      "['-        DEFAULT_METADATASTORE_METADATA_TTL);', nan]\n",
      "['+        5_000);', nan]\n",
      "['final S3AFileSystem gFs = createFS(uri, config);', nan]\n",
      "['// set back the same metadata store instance', nan]\n",
      "['gFs.setMetadataStore(realMs);', nan]\n",
      "['@@ -857,7 +863,7 @@ private void verifyFileStatusAsExpected(final String firstText,', nan]\n",
      "['expectedLength, guardedLength);', nan]\n",
      "['} else {', nan]\n",
      "['assertEquals(', nan]\n",
      "['-            \"File length in authoritative table with \" + stats,', nan]\n",
      "['+            \"File length in non-authoritative table with \" + stats,', nan]\n",
      "['expectedLength, guardedLength);', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestConstants.java b/hadoop-', 'ools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestConstants.java']\n",
      "['index 118c9ee773a..c5670b09c3d 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestConstants.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestConstants.java', nan]\n",
      "['@@ -87,10 +87,15 @@', nan]\n",
      "['*/', nan]\n",
      "['String KEY_CSVTEST_FILE = S3A_SCALE_TEST + \"csvfile\";', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * The landsat bucket: {@value}.', nan]\n",
      "['+   */', nan]\n",
      "['+  String LANDSAT_BUCKET = \"s3a://landsat-pds/\";', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Default path for the multi MB test file: {@value}.', nan]\n",
      "['*/', nan]\n",
      "['-  String DEFAULT_CSVTEST_FILE = \"s3a://landsat-pds/scene_list.gz\";', nan]\n",
      "['+  String DEFAULT_CSVTEST_FILE = LANDSAT_BUCKET + \"scene_list.gz\";', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['* Name of the property to define the timeout for scale tests: {@value}.', nan]\n",
      "['@@ -218,4 +223,10 @@', nan]\n",
      "['*/', nan]\n",
      "['String S3GUARD_DDB_TEST_TABLE_NAME_KEY =', nan]\n",
      "['\"fs.s3a.s3guard.ddb.test.table\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Test option to enable audits of the method path after', nan]\n",
      "['+   * every test case.', nan]\n",
      "['+   */', nan]\n",
      "['+  String DIRECTORY_MARKER_AUDIT = \"fs.s3a.directory.marker.audit\";', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestUtils.java b/hadoop-tool', '/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestUtils.java']\n",
      "['index aa5979dbf75..f225800b872 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestUtils.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestUtils.java', nan]\n",
      "['@@ -618,6 +618,14 @@ public static Configuration prepareTestConfiguration(final Configuration conf) {', nan]\n",
      "['// add this so that even on tests where the FS is shared,', nan]\n",
      "['// the FS is always \"magic\"', nan]\n",
      "['conf.setBoolean(MAGIC_COMMITTER_ENABLED, true);', nan]\n",
      "['+', nan]\n",
      "['+    // directory marker policy', nan]\n",
      "['+    String directoryRetention = getTestProperty(', nan]\n",
      "['+        conf,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        DEFAULT_DIRECTORY_MARKER_POLICY);', nan]\n",
      "['+    conf.set(DIRECTORY_MARKER_POLICY, directoryRetention);', nan]\n",
      "['+', nan]\n",
      "['return conf;', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@@ -882,7 +890,8 @@ public static void callQuietly(final Logger log,', nan]\n",
      "['public static S3AFileStatus getStatusWithEmptyDirFlag(', nan]\n",
      "['final S3AFileSystem fs,', nan]\n",
      "['final Path dir) throws IOException {', nan]\n",
      "['-    return fs.innerGetFileStatus(dir, true, StatusProbeEnum.ALL);', nan]\n",
      "['+    return fs.innerGetFileStatus(dir, true,', nan]\n",
      "['+        StatusProbeEnum.ALL);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -1441,4 +1450,26 @@ public static S3AFileStatus awaitFileStatus(S3AFileSystem fs,', nan]\n",
      "['.collect(Collectors.toCollection(TreeSet::new));', nan]\n",
      "['return threads;', nan]\n",
      "['}', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Call the package-private {@code innerGetFileStatus()} method', nan]\n",
      "['+   * on the passed in FS.', nan]\n",
      "['+   * @param fs filesystem', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param needEmptyDirectoryFlag look for empty directory', nan]\n",
      "['+   * @param probes file status probes to perform', nan]\n",
      "['+   * @return the status', nan]\n",
      "['+   * @throws IOException', nan]\n",
      "['+   */', nan]\n",
      "['+  public static S3AFileStatus innerGetFileStatus(', nan]\n",
      "['+      S3AFileSystem fs,', nan]\n",
      "['+      Path path,', nan]\n",
      "['+      boolean needEmptyDirectoryFlag,', nan]\n",
      "['+      Set<StatusProbeEnum> probes) throws IOException {', nan]\n",
      "['+', nan]\n",
      "['+    return fs.innerGetFileStatus(', nan]\n",
      "['+        path,', nan]\n",
      "['+        needEmptyDirectoryFlag,', nan]\n",
      "['+        probes);', nan]\n",
      "['+  }', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AGetFileStatus.java b/had', 'op-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AGetFileStatus.java']\n",
      "['index e90518a9cbd..34a275b580f 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AGetFileStatus.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AGetFileStatus.java', nan]\n",
      "['@@ -76,11 +76,15 @@ public void testFakeDirectory() throws Exception {', nan]\n",
      "['String key = path.toUri().getPath().substring(1);', nan]\n",
      "['when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key))))', nan]\n",
      "['.thenThrow(NOT_FOUND);', nan]\n",
      "['-    ObjectMetadata meta = new ObjectMetadata();', nan]\n",
      "['-    meta.setContentLength(0L);', nan]\n",
      "['-    when(s3.getObjectMetadata(argThat(', nan]\n",
      "['-        correctGetMetadataRequest(BUCKET, key + \"/\"))', nan]\n",
      "['-    )).thenReturn(meta);', nan]\n",
      "['+    String keyDir = key + \"/\";', nan]\n",
      "['+    ListObjectsV2Result listResult = new ListObjectsV2Result();', nan]\n",
      "['+    S3ObjectSummary objectSummary = new S3ObjectSummary();', nan]\n",
      "['+    objectSummary.setKey(keyDir);', nan]\n",
      "['+    objectSummary.setSize(0L);', nan]\n",
      "['+    listResult.getObjectSummaries().add(objectSummary);', nan]\n",
      "['+    when(s3.listObjectsV2(argThat(', nan]\n",
      "['+        matchListV2Request(BUCKET, keyDir))', nan]\n",
      "['+    )).thenReturn(listResult);', nan]\n",
      "['FileStatus stat = fs.getFileStatus(path);', nan]\n",
      "['assertNotNull(stat);', nan]\n",
      "['assertEquals(fs.makeQualified(path), stat.getPath());', nan]\n",
      "['@@ -161,4 +165,14 @@ private void setupListMocks(List<String> prefixes,', nan]\n",
      "['&& request.getBucketName().equals(bucket)', nan]\n",
      "['&& request.getKey().equals(key);', nan]\n",
      "['}', nan]\n",
      "['+', nan]\n",
      "['+  private ArgumentMatcher<ListObjectsV2Request> matchListV2Request(', nan]\n",
      "['+      String bucket, String key) {', nan]\n",
      "['+    return (ListObjectsV2Request request) -> {', nan]\n",
      "['+      return request != null', nan]\n",
      "['+          && request.getBucketName().equals(bucket)', nan]\n",
      "['+          && request.getPrefix().equals(key);', nan]\n",
      "['+    };', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestRestrictedReadAccess.', 'ava b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestRestrictedReadAcc']\n",
      "['index a8e7a570576..2848fb70b6d 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestRestrictedReadAccess.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestRestrictedReadAccess.java', nan]\n",
      "['@@ -410,8 +410,7 @@ public void checkBasicFileOperations() throws Throwable {', nan]\n",
      "[nan, nan]\n",
      "[nan, nan]\n",
      "['// this is HEAD + \"/\" on S3; get on S3Guard auth when the path exists,', nan]\n",
      "['-    accessDeniedIf(!s3guard, () ->', nan]\n",
      "['-        readonlyFS.listStatus(emptyDir));', nan]\n",
      "['+    readonlyFS.listStatus(emptyDir);', nan]\n",
      "[nan, nan]\n",
      "['// a recursive list of the no-read-directory works because', nan]\n",
      "['// there is no directory marker, it becomes a LIST call.', nan]\n",
      "['@@ -421,14 +420,9 @@ public void checkBasicFileOperations() throws Throwable {', nan]\n",
      "['// and so working.', nan]\n",
      "['readonlyFS.getFileStatus(noReadDir);', nan]\n",
      "[nan, nan]\n",
      "['-    // empty dir checks work when guarded because even in non-auth mode', nan]\n",
      "['-    // there are no checks for directories being out of date', nan]\n",
      "['-    // without S3, the HEAD path + \"/\" is blocked', nan]\n",
      "['-    accessDeniedIf(!s3guard, () ->', nan]\n",
      "['-        readonlyFS.getFileStatus(emptyDir));', nan]\n",
      "['-', nan]\n",
      "['+    readonlyFS.getFileStatus(emptyDir);', nan]\n",
      "['// now look at a file; the outcome depends on the mode.', nan]\n",
      "['-    accessDeniedIf(!guardedInAuthMode, () ->', nan]\n",
      "['+    accessDeniedIf(!s3guard, () ->', nan]\n",
      "['readonlyFS.getFileStatus(subdirFile));', nan]\n",
      "[nan, nan]\n",
      "['// irrespective of mode, the attempt to read the data will fail.', nan]\n",
      "['@@ -443,7 +437,7 @@ public void checkBasicFileOperations() throws Throwable {', nan]\n",
      "['// This means that permissions on the file do not get checked.', nan]\n",
      "['// See: HADOOP-16464.', nan]\n",
      "['Optional<FSDataInputStream> optIn = accessDeniedIf(', nan]\n",
      "['-        !guardedInAuthMode, () -> readonlyFS.open(emptyFile));', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+        !s3guard, () -> readonlyFS.open(emptyFile));', nan]\n",
      "['if (optIn.isPresent()) {', nan]\n",
      "['try (FSDataInputStream is = optIn.get()) {', nan]\n",
      "['Assertions.assertThat(is.read())', nan]\n",
      "['@@ -461,17 +455,17 @@ public void checkGlobOperations() throws Throwable {', nan]\n",
      "['describe(\"Glob Status operations\");', nan]\n",
      "['// baseline: the real filesystem on a subdir', nan]\n",
      "['globFS(getFileSystem(), subdirFile, null, false, 1);', nan]\n",
      "['-    // a file fails if not in auth mode', nan]\n",
      "['-    globFS(readonlyFS, subdirFile, null, !guardedInAuthMode, 1);', nan]\n",
      "['+    // a file fails if not guarded', nan]\n",
      "['+    globFS(readonlyFS, subdirFile, null, !s3guard, 1);', nan]\n",
      "[\"// empty directories don't fail.\", nan]\n",
      "['-    FileStatus[] st = globFS(readonlyFS, emptyDir, null, !s3guard, 1);', nan]\n",
      "['+    FileStatus[] st = globFS(readonlyFS, emptyDir, null, false, 1);', nan]\n",
      "['if (s3guard) {', nan]\n",
      "['assertStatusPathEquals(emptyDir, st);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['st = globFS(readonlyFS,', nan]\n",
      "['noReadWildcard,', nan]\n",
      "['-        null, !s3guard, 2);', nan]\n",
      "['+        null, false, 2);', nan]\n",
      "['if (s3guard) {', nan]\n",
      "['Assertions.assertThat(st)', nan]\n",
      "['.extracting(FileStatus::getPath)', nan]\n",
      "['@@ -481,12 +475,12 @@ public void checkGlobOperations() throws Throwable {', nan]\n",
      "['// there is precisely one .docx file (subdir2File2.docx)', nan]\n",
      "['globFS(readonlyFS,', nan]\n",
      "['new Path(noReadDir, \"*/*.docx\"),', nan]\n",
      "['-        null, !s3guard, 1);', nan]\n",
      "['+        null, false, 1);', nan]\n",
      "[nan, nan]\n",
      "['// there are no .doc files.', nan]\n",
      "['globFS(readonlyFS,', nan]\n",
      "['new Path(noReadDir, \"*/*.doc\"),', nan]\n",
      "['-        null, !s3guard, 0);', nan]\n",
      "['+        null, false, 0);', nan]\n",
      "['globFS(readonlyFS, noReadDir,', nan]\n",
      "['EVERYTHING, false, 1);', nan]\n",
      "['// and a filter without any wildcarded pattern only finds', nan]\n",
      "['@@ -513,17 +507,14 @@ public void checkSingleThreadedLocatedFileStatus() throws Throwable {', nan]\n",
      "['true,', nan]\n",
      "['HIDDEN_FILE_FILTER,', nan]\n",
      "['true);', nan]\n",
      "['-    accessDeniedIf(!s3guard,', nan]\n",
      "['-        () -> fetcher.getFileStatuses())', nan]\n",
      "['-        .ifPresent(stats -> {', nan]\n",
      "['-          Assertions.assertThat(stats)', nan]\n",
      "['-              .describedAs(\"result of located scan\").flatExtracting(FileStatus::getPath)', nan]\n",
      "['-              .containsExactlyInAnyOrder(', nan]\n",
      "['-                  emptyFile,', nan]\n",
      "['-                  subdirFile,', nan]\n",
      "['-                  subdir2File1,', nan]\n",
      "['-                  subdir2File2);', nan]\n",
      "['-        });', nan]\n",
      "['+    Assertions.assertThat(fetcher.getFileStatuses())', nan]\n",
      "['+        .describedAs(\"result of located scan\")', nan]\n",
      "['+        .flatExtracting(FileStatus::getPath)', nan]\n",
      "['+        .containsExactlyInAnyOrder(', nan]\n",
      "['+            emptyFile,', nan]\n",
      "['+            subdirFile,', nan]\n",
      "['+            subdir2File1,', nan]\n",
      "['+            subdir2File2);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -542,15 +533,11 @@ public void checkLocatedFileStatusFourThreads() throws Throwable {', nan]\n",
      "['true,', nan]\n",
      "['EVERYTHING,', nan]\n",
      "['true);', nan]\n",
      "['-    accessDeniedIf(!s3guard,', nan]\n",
      "['-        () -> fetcher.getFileStatuses())', nan]\n",
      "['-        .ifPresent(stats -> {', nan]\n",
      "['-          Assertions.assertThat(stats)', nan]\n",
      "['-              .describedAs(\"result of located scan\")', nan]\n",
      "['-              .isNotNull()', nan]\n",
      "['-              .flatExtracting(FileStatus::getPath)', nan]\n",
      "['-              .containsExactlyInAnyOrder(subdirFile, subdir2File1);', nan]\n",
      "['-        });', nan]\n",
      "['+    Assertions.assertThat(fetcher.getFileStatuses())', nan]\n",
      "['+        .describedAs(\"result of located scan\")', nan]\n",
      "['+        .isNotNull()', nan]\n",
      "['+        .flatExtracting(FileStatus::getPath)', nan]\n",
      "['+        .containsExactlyInAnyOrder(subdirFile, subdir2File1);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -567,7 +554,7 @@ public void checkLocatedFileStatusScanFile() throws Throwable {', nan]\n",
      "['true,', nan]\n",
      "['TEXT_FILE,', nan]\n",
      "['true);', nan]\n",
      "['-    accessDeniedIf(!guardedInAuthMode,', nan]\n",
      "['+    accessDeniedIf(!s3guard,', nan]\n",
      "['() -> fetcher.getFileStatuses())', nan]\n",
      "['.ifPresent(stats -> {', nan]\n",
      "['Assertions.assertThat(stats)', nan]\n",
      "['@@ -631,19 +618,16 @@ public void checkLocatedFileStatusNonexistentPath() throws Throwable {', nan]\n",
      "['*/', nan]\n",
      "['public void checkDeleteOperations() throws Throwable {', nan]\n",
      "['describe(\"Testing delete operations\");', nan]\n",
      "['-', nan]\n",
      "['-    if (!authMode) {', nan]\n",
      "['-      // unguarded or non-auth S3Guard to fail on HEAD + /', nan]\n",
      "['-      accessDenied(() -> readonlyFS.delete(emptyDir, true));', nan]\n",
      "['+    readonlyFS.delete(emptyDir, true);', nan]\n",
      "['+    if (!s3guard) {', nan]\n",
      "['// to fail on HEAD', nan]\n",
      "['accessDenied(() -> readonlyFS.delete(emptyFile, true));', nan]\n",
      "['} else {', nan]\n",
      "['-      // auth mode checks DDB for status and then issues the DELETE', nan]\n",
      "['-      readonlyFS.delete(emptyDir, true);', nan]\n",
      "['+      // checks DDB for status and then issues the DELETE', nan]\n",
      "['readonlyFS.delete(emptyFile, true);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-    // this will succeed for both as there is no subdir marker.', nan]\n",
      "['+    // this will succeed for both', nan]\n",
      "['readonlyFS.delete(subDir, true);', nan]\n",
      "['// after which  it is not there', nan]\n",
      "['fileNotFound(() -> readonlyFS.getFileStatus(subDir));', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestDirectoryMarkerPolicy.', 'ava b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestDirectoryMarkerPol']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..194cd645c07', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestDirectoryMarkerPolicy.java', nan]\n",
      "['@@ -0,0 +1,163 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.impl;', nan]\n",
      "['+', nan]\n",
      "['+import java.util.Arrays;', nan]\n",
      "['+import java.util.Collection;', nan]\n",
      "['+import java.util.function.Predicate;', nan]\n",
      "['+', nan]\n",
      "['+import org.assertj.core.api.Assertions;', nan]\n",
      "['+import org.junit.Test;', nan]\n",
      "['+import org.junit.runner.RunWith;', nan]\n",
      "['+import org.junit.runners.Parameterized;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.test.AbstractHadoopTestBase;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Unit tests for directory marker policies.', nan]\n",
      "['+ */', nan]\n",
      "['+@RunWith(Parameterized.class)', nan]\n",
      "['+public class TestDirectoryMarkerPolicy extends AbstractHadoopTestBase {', nan]\n",
      "['+', nan]\n",
      "['+  @Parameterized.Parameters(name = \"{0}\")', nan]\n",
      "['+  public static Collection<Object[]> data() {', nan]\n",
      "['+    return Arrays.asList(new Object[][]{', nan]\n",
      "['+        {', nan]\n",
      "['+            DirectoryPolicy.MarkerPolicy.Delete,', nan]\n",
      "['+            FAIL_IF_INVOKED,', nan]\n",
      "['+            false, false', nan]\n",
      "['+        },', nan]\n",
      "['+        {', nan]\n",
      "['+            DirectoryPolicy.MarkerPolicy.Keep,', nan]\n",
      "['+            FAIL_IF_INVOKED,', nan]\n",
      "['+            true, true', nan]\n",
      "['+        },', nan]\n",
      "['+        {', nan]\n",
      "['+            DirectoryPolicy.MarkerPolicy.Authoritative,', nan]\n",
      "['+            AUTH_PATH_ONLY,', nan]\n",
      "['+            false, true', nan]\n",
      "['+        }', nan]\n",
      "['+    });', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  private final DirectoryPolicy directoryPolicy;', nan]\n",
      "['+', nan]\n",
      "['+  private final boolean expectNonAuthDelete;', nan]\n",
      "['+', nan]\n",
      "['+  private final boolean expectAuthDelete;', nan]\n",
      "['+', nan]\n",
      "['+  public TestDirectoryMarkerPolicy(', nan]\n",
      "['+      final DirectoryPolicy.MarkerPolicy markerPolicy,', nan]\n",
      "['+      final Predicate<Path> authoritativeness,', nan]\n",
      "['+      final boolean expectNonAuthDelete,', nan]\n",
      "['+      final boolean expectAuthDelete) {', nan]\n",
      "['+    this.directoryPolicy = newPolicy(markerPolicy, authoritativeness);', nan]\n",
      "['+    this.expectNonAuthDelete = expectNonAuthDelete;', nan]\n",
      "['+    this.expectAuthDelete = expectAuthDelete;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a new retention policy.', nan]\n",
      "['+   * @param markerPolicy policy option', nan]\n",
      "['+   * @param authoritativeness predicate for determining if', nan]\n",
      "['+   * a path is authoritative.', nan]\n",
      "['+   * @return the retention policy.', nan]\n",
      "['+   */', nan]\n",
      "['+  private DirectoryPolicy newPolicy(', nan]\n",
      "['+      DirectoryPolicy.MarkerPolicy markerPolicy,', nan]\n",
      "['+      Predicate<Path> authoritativeness) {', nan]\n",
      "['+    return new DirectoryPolicyImpl(markerPolicy, authoritativeness);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  private static final Predicate<Path> AUTH_PATH_ONLY =', nan]\n",
      "['+      (p) -> p.toUri().getPath().startsWith(\"/auth/\");', nan]\n",
      "['+', nan]\n",
      "['+  private static final Predicate<Path> FAIL_IF_INVOKED = (p) -> {', nan]\n",
      "['+    throw new RuntimeException(\"failed\");', nan]\n",
      "['+  };', nan]\n",
      "['+', nan]\n",
      "['+  private final Path nonAuthPath = new Path(\"s3a://bucket/nonauth/data\");', nan]\n",
      "['+', nan]\n",
      "['+  private final Path authPath = new Path(\"s3a://bucket/auth/data1\");', nan]\n",
      "['+', nan]\n",
      "['+  private final Path deepAuth = new Path(\"s3a://bucket/auth/d1/d2/data2\");', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert that a path has a retention outcome.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param retain should the marker be retained', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertMarkerRetention(Path path, boolean retain) {', nan]\n",
      "['+    Assertions.assertThat(directoryPolicy.keepDirectoryMarkers(path))', nan]\n",
      "['+        .describedAs(\"Retention of path %s by %s\", path, directoryPolicy)', nan]\n",
      "['+        .isEqualTo(retain);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert that a path has a capability.', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertPathCapability(Path path,', nan]\n",
      "['+      String capability,', nan]\n",
      "['+      boolean outcome) {', nan]\n",
      "['+    Assertions.assertThat(directoryPolicy)', nan]\n",
      "['+        .describedAs(\"%s support for capability %s by path %s\"', nan]\n",
      "['+                + \" expected as %s\",', nan]\n",
      "['+            directoryPolicy, capability, path, outcome)', nan]\n",
      "['+        .matches(p -> p.hasPathCapability(path, capability) == outcome,', nan]\n",
      "['+            \"pathCapability\");', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testNonAuthPath() throws Throwable {', nan]\n",
      "['+    assertMarkerRetention(nonAuthPath, expectNonAuthDelete);', nan]\n",
      "['+    assertPathCapability(nonAuthPath,', nan]\n",
      "['+        STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE,', nan]\n",
      "['+        !expectNonAuthDelete);', nan]\n",
      "['+    assertPathCapability(nonAuthPath,', nan]\n",
      "['+        STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP,', nan]\n",
      "['+        expectNonAuthDelete);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testAuthPath() throws Throwable {', nan]\n",
      "['+    assertMarkerRetention(authPath, expectAuthDelete);', nan]\n",
      "['+    assertPathCapability(authPath,', nan]\n",
      "['+        STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE,', nan]\n",
      "['+        !expectAuthDelete);', nan]\n",
      "['+    assertPathCapability(authPath,', nan]\n",
      "['+        STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP,', nan]\n",
      "['+        expectAuthDelete);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testDeepAuthPath() throws Throwable {', nan]\n",
      "['+    assertMarkerRetention(deepAuth, expectAuthDelete);', nan]\n",
      "['+    assertPathCapability(deepAuth,', nan]\n",
      "['+        STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE,', nan]\n",
      "['+        !expectAuthDelete);', nan]\n",
      "['+    assertPathCapability(deepAuth,', nan]\n",
      "['+        STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP,', nan]\n",
      "['+        expectAuthDelete);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/AbstractS3ACostTest', 'java b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/AbstractS3ACos']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..db0542ddc94', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/AbstractS3ACostTest.java', nan]\n",
      "['@@ -0,0 +1,637 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.performance;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.FileNotFoundException;', nan]\n",
      "['+import java.io.IOException;', nan]\n",
      "['+import java.util.Set;', nan]\n",
      "['+import java.util.concurrent.Callable;', nan]\n",
      "['+', nan]\n",
      "['+import org.assertj.core.api.Assertions;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['+import org.apache.hadoop.fs.FSDataOutputStream;', nan]\n",
      "['+import org.apache.hadoop.fs.FSDataOutputStreamBuilder;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.contract.ContractTestUtils;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.AbstractS3ATestBase;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileStatus;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.Statistic;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.Tristate;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.StatusProbeEnum;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Statistic.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.performance.OperationCost.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.performance.OperationCostValidator.expect;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.performance.OperationCostValidator.probe;', nan]\n",
      "['+import static org.apache.hadoop.test.AssertExtensions.dynamicDescription;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Abstract class for tests which make assertions about cost.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Factored out from {@code ITestS3AFileOperationCost}', nan]\n",
      "['+ */', nan]\n",
      "['+public class AbstractS3ACostTest extends AbstractS3ATestBase {', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameter: should the stores be guarded?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean s3guard;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameter: should directory markers be retained?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean keepMarkers;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Is this an auth mode test run?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean authoritative;', nan]\n",
      "['+', nan]\n",
      "['+  /** probe states calculated from the configuration options. */', nan]\n",
      "['+  private boolean isGuarded;', nan]\n",
      "['+', nan]\n",
      "['+  private boolean isRaw;', nan]\n",
      "['+', nan]\n",
      "['+  private boolean isAuthoritative;', nan]\n",
      "['+', nan]\n",
      "['+  private boolean isNonAuth;', nan]\n",
      "['+', nan]\n",
      "['+  private boolean isKeeping;', nan]\n",
      "['+', nan]\n",
      "['+  private boolean isDeleting;', nan]\n",
      "['+', nan]\n",
      "['+  private OperationCostValidator costValidator;', nan]\n",
      "['+', nan]\n",
      "['+  public AbstractS3ACostTest(', nan]\n",
      "['+      final boolean s3guard,', nan]\n",
      "['+      final boolean keepMarkers,', nan]\n",
      "['+      final boolean authoritative) {', nan]\n",
      "['+    this.s3guard = s3guard;', nan]\n",
      "['+    this.keepMarkers = keepMarkers;', nan]\n",
      "['+    this.authoritative = authoritative;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public Configuration createConfiguration() {', nan]\n",
      "['+    Configuration conf = super.createConfiguration();', nan]\n",
      "['+    String bucketName = getTestBucketName(conf);', nan]\n",
      "['+    removeBucketOverrides(bucketName, conf,', nan]\n",
      "['+        S3_METADATA_STORE_IMPL);', nan]\n",
      "['+    if (!isGuarded()) {', nan]\n",
      "['+      // in a raw run remove all s3guard settings', nan]\n",
      "['+      removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['+          S3_METADATA_STORE_IMPL);', nan]\n",
      "['+    }', nan]\n",
      "['+    removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['+    // directory marker options', nan]\n",
      "['+    conf.set(DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        keepMarkers', nan]\n",
      "['+            ? DIRECTORY_MARKER_POLICY_KEEP', nan]\n",
      "['+            : DIRECTORY_MARKER_POLICY_DELETE);', nan]\n",
      "['+    conf.setBoolean(METADATASTORE_AUTHORITATIVE, authoritative);', nan]\n",
      "['+    disableFilesystemCaching(conf);', nan]\n",
      "['+    return conf;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void setup() throws Exception {', nan]\n",
      "['+    super.setup();', nan]\n",
      "['+    if (isGuarded()) {', nan]\n",
      "['+      // s3guard is required for those test runs where any of the', nan]\n",
      "['+      // guard options are set', nan]\n",
      "['+      assumeS3GuardState(true, getConfiguration());', nan]\n",
      "['+    }', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    skipDuringFaultInjection(fs);', nan]\n",
      "['+', nan]\n",
      "['+    // build up the states', nan]\n",
      "['+    isGuarded = isGuarded();', nan]\n",
      "['+', nan]\n",
      "['+    isRaw = !isGuarded;', nan]\n",
      "['+    isAuthoritative = isGuarded && authoritative;', nan]\n",
      "['+    isNonAuth = isGuarded && !authoritative;', nan]\n",
      "['+', nan]\n",
      "['+    isKeeping = isKeepingMarkers();', nan]\n",
      "['+', nan]\n",
      "['+    isDeleting = !isKeeping;', nan]\n",
      "['+', nan]\n",
      "['+    // insert new metrics so as to keep the list sorted', nan]\n",
      "['+    costValidator = OperationCostValidator.builder(getFileSystem())', nan]\n",
      "['+        .withMetrics(', nan]\n",
      "['+            DIRECTORIES_CREATED,', nan]\n",
      "['+            DIRECTORIES_DELETED,', nan]\n",
      "['+            FAKE_DIRECTORIES_DELETED,', nan]\n",
      "['+            FILES_DELETED,', nan]\n",
      "['+            INVOCATION_COPY_FROM_LOCAL_FILE,', nan]\n",
      "['+            OBJECT_COPY_REQUESTS,', nan]\n",
      "['+            OBJECT_DELETE_REQUESTS,', nan]\n",
      "['+            OBJECT_LIST_REQUESTS,', nan]\n",
      "['+            OBJECT_METADATA_REQUESTS,', nan]\n",
      "['+            OBJECT_PUT_BYTES,', nan]\n",
      "['+            OBJECT_PUT_REQUESTS)', nan]\n",
      "['+        .build();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public void assumeUnguarded() {', nan]\n",
      "['+    assume(\"Unguarded FS only\", !isGuarded());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Is the store guarded authoritatively on the test path?', nan]\n",
      "['+   * @return true if the condition is met on this test run.', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean isAuthoritative() {', nan]\n",
      "['+    return authoritative;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Is the store guarded?', nan]\n",
      "['+   * @return true if the condition is met on this test run.', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean isGuarded() {', nan]\n",
      "['+    return s3guard;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Is the store raw?', nan]\n",
      "['+   * @return true if the condition is met on this test run.', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean isRaw() {', nan]\n",
      "['+    return isRaw;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Is the store guarded non-authoritatively on the test path?', nan]\n",
      "['+   * @return true if the condition is met on this test run.', nan]\n",
      "['+   */', nan]\n",
      "['+  public boolean isNonAuth() {', nan]\n",
      "['+    return isNonAuth;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public boolean isDeleting() {', nan]\n",
      "['+    return isDeleting;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public boolean isKeepingMarkers() {', nan]\n",
      "['+    return keepMarkers;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A special object whose toString() value is the current', nan]\n",
      "['+   * state of the metrics.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Object getMetricSummary() {', nan]\n",
      "['+    return costValidator;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create then close the file through the builder API.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param overwrite overwrite flag', nan]\n",
      "['+   * @param recursive true == skip parent existence check', nan]\n",
      "['+   * @param cost expected cost', nan]\n",
      "['+   * @return path to new object.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Path buildFile(Path path,', nan]\n",
      "['+      boolean overwrite,', nan]\n",
      "['+      boolean recursive,', nan]\n",
      "['+      OperationCost cost) throws Exception {', nan]\n",
      "['+    resetStatistics();', nan]\n",
      "['+    verifyRaw(cost, () -> {', nan]\n",
      "['+      FSDataOutputStreamBuilder builder = getFileSystem().createFile(path)', nan]\n",
      "['+          .overwrite(overwrite);', nan]\n",
      "['+      if (recursive) {', nan]\n",
      "['+        builder.recursive();', nan]\n",
      "['+      }', nan]\n",
      "['+      FSDataOutputStream stream = builder.build();', nan]\n",
      "['+      stream.close();', nan]\n",
      "['+      return stream.toString();', nan]\n",
      "['+    });', nan]\n",
      "['+    return path;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a directory, returning its path.', nan]\n",
      "['+   * @param p path to dir.', nan]\n",
      "['+   * @return path of new dir', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Path dir(Path p) throws IOException {', nan]\n",
      "['+    mkdirs(p);', nan]\n",
      "['+    return p;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a file, returning its path.', nan]\n",
      "['+   * @param p path to file.', nan]\n",
      "['+   * @return path of new file', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Path file(Path p) throws IOException {', nan]\n",
      "['+    return file(p, true);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a file, returning its path.', nan]\n",
      "['+   * @param path path to file.', nan]\n",
      "['+   * @param overwrite overwrite flag', nan]\n",
      "['+   * @return path of new file', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Path file(Path path, final boolean overwrite)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    getFileSystem().create(path, overwrite).close();', nan]\n",
      "['+    return path;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Touch a file, overwriting.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @return path to new object.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Path create(Path path) throws Exception {', nan]\n",
      "['+    return create(path, true, CREATE_FILE_OVERWRITE);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create then close the file.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param overwrite overwrite flag', nan]\n",
      "['+   * @param cost expected cost', nan]\n",
      "['+', nan]\n",
      "['+   * @return path to new object.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Path create(Path path, boolean overwrite,', nan]\n",
      "['+      OperationCost cost) throws Exception {', nan]\n",
      "['+    return verifyRaw(cost, () ->', nan]\n",
      "['+        file(path, overwrite));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute rename, returning the current metrics.', nan]\n",
      "['+   * For use in l-expressions.', nan]\n",
      "['+   * @param source source path.', nan]\n",
      "['+   * @param dest dest path', nan]\n",
      "['+   * @return a string for exceptions.', nan]\n",
      "['+   */', nan]\n",
      "['+  public String execRename(final Path source,', nan]\n",
      "['+      final Path dest) throws IOException {', nan]\n",
      "['+    getFileSystem().rename(source, dest);', nan]\n",
      "['+    return String.format(\"rename(%s, %s): %s\",', nan]\n",
      "['+        dest, source, getMetricSummary());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many directories are in a path?', nan]\n",
      "['+   * @param path path to probe.', nan]\n",
      "['+   * @return the number of entries below root this path is', nan]\n",
      "['+   */', nan]\n",
      "['+  protected int directoriesInPath(Path path) {', nan]\n",
      "['+    return path.isRoot() ? 0 : 1 + directoriesInPath(path.getParent());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Reset all the metrics being tracked.', nan]\n",
      "['+   */', nan]\n",
      "['+  private void resetStatistics() {', nan]\n",
      "['+    costValidator.resetMetricDiffs();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute a closure and verify the metrics.', nan]\n",
      "['+   * @param eval closure to evaluate', nan]\n",
      "['+   * @param expected varargs list of expected diffs', nan]\n",
      "['+   * @param <T> return type.', nan]\n",
      "['+   * @return the result of the evaluation', nan]\n",
      "['+   */', nan]\n",
      "['+  protected <T> T verifyMetrics(', nan]\n",
      "['+      Callable<T> eval,', nan]\n",
      "['+      OperationCostValidator.ExpectedProbe... expected) throws Exception {', nan]\n",
      "['+    return costValidator.exec(eval, expected);', nan]\n",
      "['+', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute a closure, expecting an exception.', nan]\n",
      "['+   * Verify the metrics after the exception has been caught and', nan]\n",
      "['+   * validated.', nan]\n",
      "['+   * @param clazz type of exception', nan]\n",
      "['+   * @param text text to look for in exception (optional)', nan]\n",
      "['+   * @param eval closure to evaluate', nan]\n",
      "['+   * @param expected varargs list of expected diffs', nan]\n",
      "['+   * @param <T> return type of closure', nan]\n",
      "['+   * @param <E> exception type', nan]\n",
      "['+   * @return the exception caught.', nan]\n",
      "['+   * @throws Exception any other exception', nan]\n",
      "['+   */', nan]\n",
      "['+  protected <T, E extends Throwable> E verifyMetricsIntercepting(', nan]\n",
      "['+      Class<E> clazz,', nan]\n",
      "['+      String text,', nan]\n",
      "['+      Callable<T> eval,', nan]\n",
      "['+      OperationCostValidator.ExpectedProbe... expected) throws Exception {', nan]\n",
      "['+    return costValidator.intercepting(clazz, text, eval, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute a closure expecting an exception.', nan]\n",
      "['+   * @param clazz type of exception', nan]\n",
      "['+   * @param text text to look for in exception (optional)', nan]\n",
      "['+   * @param head expected head request count.', nan]\n",
      "['+   * @param list expected list request count.', nan]\n",
      "['+   * @param eval closure to evaluate', nan]\n",
      "['+   * @param <T> return type of closure', nan]\n",
      "['+   * @param <E> exception type', nan]\n",
      "['+   * @return the exception caught.', nan]\n",
      "['+   * @throws Exception any other exception', nan]\n",
      "['+   */', nan]\n",
      "['+  protected <T, E extends Throwable> E interceptRaw(', nan]\n",
      "['+      Class<E> clazz,', nan]\n",
      "['+      String text,', nan]\n",
      "['+      OperationCost cost,', nan]\n",
      "['+      Callable<T> eval) throws Exception {', nan]\n",
      "['+    return verifyMetricsIntercepting(clazz, text, eval, whenRaw(cost));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Declare the expected cost on any FS.', nan]\n",
      "['+   * @param cost costs to expect', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe always(', nan]\n",
      "['+      OperationCost cost) {', nan]\n",
      "['+    return expect(true, cost);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Declare the expected cost on a raw FS.', nan]\n",
      "['+   * @param cost costs to expect', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe whenRaw(', nan]\n",
      "['+      OperationCost cost) {', nan]\n",
      "['+    return expect(isRaw(), cost);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Declare the expected cost on a guarded FS.', nan]\n",
      "['+   * @param cost costs to expect', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe whenGuarded(', nan]\n",
      "['+      OperationCost cost) {', nan]\n",
      "['+    return expect(isGuarded(), cost);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Declare the expected cost on a guarded auth FS.', nan]\n",
      "['+   * @param cost costs to expect', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe whenAuthoritative(', nan]\n",
      "['+      OperationCost cost) {', nan]\n",
      "['+    return expect(isAuthoritative(), cost);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Declare the expected cost on a guarded nonauth FS.', nan]\n",
      "['+   * @param cost costs to expect', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe whenNonauth(', nan]\n",
      "['+      OperationCost cost) {', nan]\n",
      "['+    return expect(isNonAuth(), cost);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is keeping markers.', nan]\n",
      "['+   * @param cost expected cost', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe whenKeeping(', nan]\n",
      "['+      OperationCost cost) {', nan]\n",
      "['+    return expect(isKeepingMarkers(), cost);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is keeping markers.', nan]\n",
      "['+   * @param cost expected cost', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe whenDeleting(', nan]\n",
      "['+      OperationCost cost) {', nan]\n",
      "['+    return expect(isDeleting(), cost);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute a closure expecting a specific number of HEAD/LIST calls', nan]\n",
      "['+   * on <i>raw</i> S3 stores only.', nan]\n",
      "['+   * @param cost expected cost', nan]\n",
      "['+   * @param eval closure to evaluate', nan]\n",
      "['+   * @param <T> return type of closure', nan]\n",
      "['+   * @return the result of the evaluation', nan]\n",
      "['+   */', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+  protected <T> T verifyRaw(', nan]\n",
      "['+      OperationCost cost,', nan]\n",
      "['+      Callable<T> eval) throws Exception {', nan]\n",
      "['+    return verifyMetrics(eval, whenRaw(cost));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute {@code S3AFileSystem#innerGetFileStatus(Path, boolean, Set)}', nan]\n",
      "['+   * for the given probes.', nan]\n",
      "['+   * expect the specific HEAD/LIST count with a raw FS.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param needEmptyDirectoryFlag look for empty directory', nan]\n",
      "['+   * @param probes file status probes to perform', nan]\n",
      "['+   * @param cost expected cost', nan]\n",
      "['+   * @return the status', nan]\n",
      "['+   */', nan]\n",
      "['+  public S3AFileStatus verifyRawInnerGetFileStatus(', nan]\n",
      "['+      Path path,', nan]\n",
      "['+      boolean needEmptyDirectoryFlag,', nan]\n",
      "['+      Set<StatusProbeEnum> probes,', nan]\n",
      "['+      OperationCost cost) throws Exception {', nan]\n",
      "['+    return verifyRaw(cost, () ->', nan]\n",
      "['+        innerGetFileStatus(getFileSystem(),', nan]\n",
      "['+            path,', nan]\n",
      "['+            needEmptyDirectoryFlag,', nan]\n",
      "['+            probes));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute {@code S3AFileSystem#innerGetFileStatus(Path, boolean, Set)}', nan]\n",
      "['+   * for the given probes -expect a FileNotFoundException,', nan]\n",
      "['+   * and the specific HEAD/LIST count with a raw FS.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param needEmptyDirectoryFlag look for empty directory', nan]\n",
      "['+   * @param probes file status probes to perform', nan]\n",
      "['+   * @param cost expected cost', nan]\n",
      "['+   */', nan]\n",
      "['+', nan]\n",
      "['+  public void interceptRawGetFileStatusFNFE(', nan]\n",
      "['+      Path path,', nan]\n",
      "['+      boolean needEmptyDirectoryFlag,', nan]\n",
      "['+      Set<StatusProbeEnum> probes,', nan]\n",
      "['+      OperationCost cost) throws Exception {', nan]\n",
      "['+    interceptRaw(FileNotFoundException.class, \"\",', nan]\n",
      "['+        cost, () ->', nan]\n",
      "['+            innerGetFileStatus(getFileSystem(),', nan]\n",
      "['+                path,', nan]\n",
      "['+                needEmptyDirectoryFlag,', nan]\n",
      "['+                probes));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Probe for a path being a directory.', nan]\n",
      "['+   * Metrics are only checked on unguarded stores.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param expected expected outcome', nan]\n",
      "['+   * @param cost expected cost on a Raw FS.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected void isDir(Path path,', nan]\n",
      "['+      boolean expected,', nan]\n",
      "['+      OperationCost cost) throws Exception {', nan]\n",
      "['+    boolean b = verifyRaw(cost, () ->', nan]\n",
      "['+        getFileSystem().isDirectory(path));', nan]\n",
      "['+    Assertions.assertThat(b)', nan]\n",
      "['+        .describedAs(\"isDirectory(%s)\", path)', nan]\n",
      "['+        .isEqualTo(expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Probe for a path being a file.', nan]\n",
      "['+   * Metrics are only checked on unguarded stores.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param expected expected outcome', nan]\n",
      "['+   * @param cost expected cost on a Raw FS.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected void isFile(Path path,', nan]\n",
      "['+      boolean expected,', nan]\n",
      "['+      OperationCost cost) throws Exception {', nan]\n",
      "['+    boolean b = verifyRaw(cost, () ->', nan]\n",
      "['+        getFileSystem().isFile(path));', nan]\n",
      "['+    Assertions.assertThat(b)', nan]\n",
      "['+        .describedAs(\"isFile(%s)\", path)', nan]\n",
      "['+        .isEqualTo(expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must always hold.', nan]\n",
      "['+   * @param stat metric source', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe with(', nan]\n",
      "['+      final Statistic stat, final int expected) {', nan]\n",
      "['+    return probe(stat, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is unguarded.', nan]\n",
      "['+   * @param stat metric source', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe withWhenRaw(', nan]\n",
      "['+      final Statistic stat, final int expected) {', nan]\n",
      "['+    return probe(isRaw(), stat, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is guarded.', nan]\n",
      "['+   * @param stat metric source', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe withWhenGuarded(', nan]\n",
      "['+      final Statistic stat,', nan]\n",
      "['+      final int expected) {', nan]\n",
      "['+    return probe(isGuarded(), stat, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is guarded + authoritative.', nan]\n",
      "['+   * @param stat metric source', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe withWhenAuthoritative(', nan]\n",
      "['+      final Statistic stat,', nan]\n",
      "['+      final int expected) {', nan]\n",
      "['+    return probe(isAuthoritative(), stat, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is guarded + authoritative.', nan]\n",
      "['+   * @param stat metric source', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe withWhenNonauth(', nan]\n",
      "['+      final Statistic stat,', nan]\n",
      "['+      final int expected) {', nan]\n",
      "['+    return probe(isNonAuth(), stat, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is keeping markers.', nan]\n",
      "['+   * @param stat metric source', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe withWhenKeeping(', nan]\n",
      "['+      final Statistic stat,', nan]\n",
      "['+      final int expected) {', nan]\n",
      "['+    return probe(isKeepingMarkers(), stat, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A metric diff which must hold when the fs is keeping markers.', nan]\n",
      "['+   * @param stat metric source', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return the diff.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected OperationCostValidator.ExpectedProbe withWhenDeleting(', nan]\n",
      "['+      final Statistic stat,', nan]\n",
      "['+      final int expected) {', nan]\n",
      "['+    return probe(isDeleting(), stat, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert the empty directory status of a file is as expected.', nan]\n",
      "['+   * The raised assertion message includes a list of the path.', nan]\n",
      "['+   * @param status status to probe.', nan]\n",
      "['+   * @param expected expected value', nan]\n",
      "['+   */', nan]\n",
      "['+  protected void assertEmptyDirStatus(final S3AFileStatus status,', nan]\n",
      "['+      final Tristate expected) {', nan]\n",
      "['+    Assertions.assertThat(status.isEmptyDirectory())', nan]\n",
      "['+        .describedAs(dynamicDescription(() ->', nan]\n",
      "['+            \"FileStatus says directory is not empty: \" + status', nan]\n",
      "['+                + \"\\\\n\" + ContractTestUtils.ls(', nan]\n",
      "['+                    getFileSystem(), status.getPath())))', nan]\n",
      "['+        .isEqualTo(expected);', nan]\n",
      "['+  }', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestDirectoryMarke', 'Listing.java b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestD']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..ed56802ddfe', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestDirectoryMarkerListin', '.java']\n",
      "['@@ -0,0 +1,824 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.performance;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.FileNotFoundException;', nan]\n",
      "['+import java.io.IOException;', nan]\n",
      "['+import java.util.ArrayList;', nan]\n",
      "['+import java.util.Arrays;', nan]\n",
      "['+import java.util.Collection;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+import java.util.concurrent.Callable;', nan]\n",
      "['+import java.util.stream.Collectors;', nan]\n",
      "['+', nan]\n",
      "['+import com.amazonaws.AmazonClientException;', nan]\n",
      "['+import com.amazonaws.services.s3.AmazonS3;', nan]\n",
      "['+import com.amazonaws.services.s3.model.ObjectMetadata;', nan]\n",
      "['+import org.junit.Test;', nan]\n",
      "['+import org.junit.runner.RunWith;', nan]\n",
      "['+import org.junit.runners.Parameterized;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['+import org.apache.hadoop.fs.FileAlreadyExistsException;', nan]\n",
      "['+import org.apache.hadoop.fs.FileStatus;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;', nan]\n",
      "['+import org.apache.hadoop.fs.RemoteIterator;', nan]\n",
      "['+import org.apache.hadoop.fs.contract.ContractTestUtils;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.AbstractS3ATestBase;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AUtils;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.contract.ContractTestUtils.touch;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.AUTHORITATIVE_PATH;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_KEEP;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.METADATASTORE_AUTHORITATIVE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.S3_METADATA_STORE_IMPL;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.assume;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestBucketName;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.removeBaseAndBucketOverrides;', nan]\n",
      "['+import static org.apache.hadoop.test.LambdaTestUtils.intercept;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * This is a test suite designed to verify that directory markers do', nan]\n",
      "['+ * not get misconstrued as empty directories during operations', nan]\n",
      "['+ * which explicitly or implicitly list directory trees.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * It is also intended it to be backported to all releases', nan]\n",
      "['+ * which are enhanced to read directory trees where markers have', nan]\n",
      "['+ * been retained.', nan]\n",
      "['+ * Hence: it does not use any of the new helper classes to', nan]\n",
      "['+ * measure the cost of operations or attempt to create markers', nan]\n",
      "['+ * through the FS APIs.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Instead, the directory structure to test is created through', nan]\n",
      "['+ * low-level S3 SDK API calls.', nan]\n",
      "['+ * We also skip any probes to measure/assert metrics.', nan]\n",
      "[\"+ * We're testing the semantics here, not the cost of the operations.\", nan]\n",
      "['+ * Doing that makes it a lot easier to backport.', nan]\n",
      "['+ *', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Similarly: JUnit assertions over AssertJ.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * The tests work with unguarded buckets only -the bucket settings are changed', nan]\n",
      "['+ * appropriately.', nan]\n",
      "['+ */', nan]\n",
      "['+@RunWith(Parameterized.class)', nan]\n",
      "['+public class ITestDirectoryMarkerListing extends AbstractS3ATestBase {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(ITestDirectoryMarkerListing.class);', nan]\n",
      "['+', nan]\n",
      "['+  private static final String FILENAME = \"fileUnderMarker\";', nan]\n",
      "['+', nan]\n",
      "['+  private static final String HELLO = \"hello\";', nan]\n",
      "['+', nan]\n",
      "['+  private static final String MARKER = \"marker\";', nan]\n",
      "['+', nan]\n",
      "['+  private static final String MARKER_PEER = \"markerpeer\";', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameterization.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Parameterized.Parameters(name = \"{0}\")', nan]\n",
      "['+  public static Collection<Object[]> params() {', nan]\n",
      "['+    return Arrays.asList(new Object[][]{', nan]\n",
      "['+        {\"keep-markers\",  true},', nan]\n",
      "['+        {\"delete-markers\", false},', nan]\n",
      "['+    });', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Does rename copy markers?', nan]\n",
      "['+   * Value: {@value}', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Older releases: yes.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * The full marker-optimized releases: no.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final boolean RENAME_COPIES_MARKERS = false;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Test configuration name.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final String name;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Does this test configuration keep markers?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean keepMarkers;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Is this FS deleting markers?', nan]\n",
      "['+   */', nan]\n",
      "['+  private final boolean isDeletingMarkers;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Path to a directory which has a marker.', nan]\n",
      "['+   */', nan]\n",
      "['+  private Path markerDir;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Key to the object representing {@link #markerDir}.', nan]\n",
      "['+   */', nan]\n",
      "['+  private String markerKey;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Key to the object representing {@link #markerDir} with', nan]\n",
      "['+   * a trailing / added. This references the actual object', nan]\n",
      "['+   * which has been created.', nan]\n",
      "['+   */', nan]\n",
      "['+  private String markerKeySlash;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * bucket of tests.', nan]\n",
      "['+   */', nan]\n",
      "['+  private String bucket;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * S3 Client of the FS.', nan]\n",
      "['+   */', nan]\n",
      "['+  private AmazonS3 s3client;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Path to a file under the marker.', nan]\n",
      "['+   */', nan]\n",
      "['+  private Path filePathUnderMarker;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Key to a file under the marker.', nan]\n",
      "['+   */', nan]\n",
      "['+  private String fileKeyUnderMarker;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * base path for the test files; the marker dir goes under this.', nan]\n",
      "['+   */', nan]\n",
      "['+  private Path basePath;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Path to a file a peer of markerDir.', nan]\n",
      "['+   */', nan]\n",
      "['+  private Path markerPeer;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Key to a file a peer of markerDir.', nan]\n",
      "['+   */', nan]\n",
      "['+  private String markerPeerKey;', nan]\n",
      "['+', nan]\n",
      "['+  public ITestDirectoryMarkerListing(final String name,', nan]\n",
      "['+      final boolean keepMarkers) {', nan]\n",
      "['+    this.name = name;', nan]\n",
      "['+    this.keepMarkers = keepMarkers;', nan]\n",
      "['+    this.isDeletingMarkers = !keepMarkers;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  protected Configuration createConfiguration() {', nan]\n",
      "['+    Configuration conf = super.createConfiguration();', nan]\n",
      "['+    String bucketName = getTestBucketName(conf);', nan]\n",
      "['+', nan]\n",
      "['+    // Turn off S3Guard', nan]\n",
      "['+    removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['+        S3_METADATA_STORE_IMPL,', nan]\n",
      "['+        METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['+', nan]\n",
      "['+    // directory marker options', nan]\n",
      "['+    removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY);', nan]\n",
      "['+    conf.set(DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        keepMarkers', nan]\n",
      "['+            ? DIRECTORY_MARKER_POLICY_KEEP', nan]\n",
      "['+            : DIRECTORY_MARKER_POLICY_DELETE);', nan]\n",
      "['+    return conf;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * The setup phase includes creating the test objects.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void setup() throws Exception {', nan]\n",
      "['+    super.setup();', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    assume(\"unguarded FS only\",', nan]\n",
      "['+        !fs.hasMetadataStore());', nan]\n",
      "['+    s3client = fs.getAmazonS3ClientForTesting(\"markers\");', nan]\n",
      "['+', nan]\n",
      "['+    bucket = fs.getBucket();', nan]\n",
      "['+    Path base = new Path(methodPath(), \"base\");', nan]\n",
      "['+', nan]\n",
      "['+    createTestObjects(base);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Teardown deletes the objects created before', nan]\n",
      "['+   * the superclass does the directory cleanup.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void teardown() throws Exception {', nan]\n",
      "['+    if (s3client != null) {', nan]\n",
      "['+      deleteObject(markerKey);', nan]\n",
      "['+      deleteObject(markerKeySlash);', nan]\n",
      "['+      deleteObject(markerPeerKey);', nan]\n",
      "['+      deleteObject(fileKeyUnderMarker);', nan]\n",
      "['+    }', nan]\n",
      "['+    // do this ourselves to avoid audits teardown failing', nan]\n",
      "['+    // when surplus markers are found', nan]\n",
      "['+    deleteTestDirInTeardown();', nan]\n",
      "['+    super.teardown();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create the test objects under the given path, setting', nan]\n",
      "['+   * various fields in the process.', nan]\n",
      "['+   * @param path parent path of everything', nan]\n",
      "['+   */', nan]\n",
      "['+  private void createTestObjects(final Path path) throws Exception {', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    basePath = path;', nan]\n",
      "['+    markerDir = new Path(basePath, MARKER);', nan]\n",
      "['+    // peer path has the same initial name to make sure there', nan]\n",
      "['+    // is no confusion there.', nan]\n",
      "['+    markerPeer = new Path(basePath, MARKER_PEER);', nan]\n",
      "['+    markerPeerKey = fs.pathToKey(markerPeer);', nan]\n",
      "['+    markerKey = fs.pathToKey(markerDir);', nan]\n",
      "['+    markerKeySlash = markerKey + \"/\";', nan]\n",
      "['+    fileKeyUnderMarker = markerKeySlash + FILENAME;', nan]\n",
      "['+    filePathUnderMarker = new Path(markerDir, FILENAME);', nan]\n",
      "['+    // put the empty dir', nan]\n",
      "['+    fs.mkdirs(markerDir);', nan]\n",
      "['+    touch(fs, markerPeer);', nan]\n",
      "['+    put(fileKeyUnderMarker, HELLO);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /*', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+    Basic probes', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+  */', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testMarkerExists() throws Throwable {', nan]\n",
      "['+    describe(\"Verify the marker exists\");', nan]\n",
      "['+    head(markerKeySlash);', nan]\n",
      "['+    assertIsDirectory(markerDir);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testObjectUnderMarker() throws Throwable {', nan]\n",
      "['+    describe(\"verify the file under the marker dir exists\");', nan]\n",
      "['+    assertIsFile(filePathUnderMarker);', nan]\n",
      "['+    head(fileKeyUnderMarker);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /*', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+    The listing operations', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+  */', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testListStatusMarkerDir() throws Throwable {', nan]\n",
      "['+    describe(\"list the marker directory and expect to see the file\");', nan]\n",
      "['+    assertContainsFileUnderMarkerOnly(', nan]\n",
      "['+        toList(getFileSystem().listStatus(markerDir)));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testListFilesMarkerDirFlat() throws Throwable {', nan]\n",
      "['+    assertContainsFileUnderMarkerOnly(toList(', nan]\n",
      "['+        getFileSystem().listFiles(markerDir, false)));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testListFilesMarkerDirRecursive() throws Throwable {', nan]\n",
      "['+    List<FileStatus> statuses = toList(', nan]\n",
      "['+        getFileSystem().listFiles(markerDir, true));', nan]\n",
      "['+    assertContainsFileUnderMarkerOnly(statuses);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Path listing above the base dir MUST only find the file', nan]\n",
      "['+   * and not the marker.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testListStatusBaseDirRecursive() throws Throwable {', nan]\n",
      "['+    List<FileStatus> statuses = toList(', nan]\n",
      "['+        getFileSystem().listFiles(basePath, true));', nan]\n",
      "['+    assertContainsExactlyStatusOfPaths(statuses, filePathUnderMarker,', nan]\n",
      "['+        markerPeer);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testGlobStatusBaseDirRecursive() throws Throwable {', nan]\n",
      "['+    Path escapedPath = new Path(escape(basePath.toUri().getPath()));', nan]\n",
      "['+    List<FileStatus> statuses =', nan]\n",
      "['+        exec(\"glob\", () ->', nan]\n",
      "['+            toList(getFileSystem().globStatus(new Path(escapedPath, \"*\"))));', nan]\n",
      "['+    assertContainsExactlyStatusOfPaths(statuses, markerDir, markerPeer);', nan]\n",
      "['+    assertIsFileAtPath(markerPeer, statuses.get(1));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testGlobStatusMarkerDir() throws Throwable {', nan]\n",
      "['+    Path escapedPath = new Path(escape(markerDir.toUri().getPath()));', nan]\n",
      "['+    List<FileStatus> statuses =', nan]\n",
      "['+        exec(\"glob\", () ->', nan]\n",
      "['+            toList(getFileSystem().globStatus(new Path(escapedPath, \"*\"))));', nan]\n",
      "['+    assertContainsFileUnderMarkerOnly(statuses);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Call {@code listLocatedStatus(basePath)}', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * The list here returns the marker peer before the', nan]\n",
      "['+   * dir. Reason: the listing iterators return', nan]\n",
      "['+   * the objects before the common prefixes, and the', nan]\n",
      "['+   * marker dir is coming back as a prefix.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testListLocatedStatusBaseDir() throws Throwable {', nan]\n",
      "['+    List<FileStatus> statuses =', nan]\n",
      "['+        exec(\"listLocatedStatus\", () ->', nan]\n",
      "['+            toList(getFileSystem().listLocatedStatus(basePath)));', nan]\n",
      "['+', nan]\n",
      "['+    assertContainsExactlyStatusOfPaths(statuses, markerPeer, markerDir);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Call {@code listLocatedStatus(markerDir)}; expect', nan]\n",
      "['+   * the file entry only.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testListLocatedStatusMarkerDir() throws Throwable {', nan]\n",
      "['+    List<FileStatus> statuses =', nan]\n",
      "['+        exec(\"listLocatedStatus\", () ->', nan]\n",
      "['+            toList(getFileSystem().listLocatedStatus(markerDir)));', nan]\n",
      "['+', nan]\n",
      "['+    assertContainsFileUnderMarkerOnly(statuses);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  /*', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+    Creation Rejection', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+  */', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCreateNoOverwriteMarkerDir() throws Throwable {', nan]\n",
      "['+    describe(\"create no-overwrite over the marker dir fails\");', nan]\n",
      "['+    head(markerKeySlash);', nan]\n",
      "['+    intercept(FileAlreadyExistsException.class, () ->', nan]\n",
      "['+        exec(\"create\", () ->', nan]\n",
      "['+            getFileSystem().create(markerDir, false)));', nan]\n",
      "['+    // dir is still there.', nan]\n",
      "['+    head(markerKeySlash);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCreateNoOverwriteFile() throws Throwable {', nan]\n",
      "['+    describe(\"create-no-overwrite on the file fails\");', nan]\n",
      "['+', nan]\n",
      "['+    head(fileKeyUnderMarker);', nan]\n",
      "['+    intercept(FileAlreadyExistsException.class, () ->', nan]\n",
      "['+        exec(\"create\", () ->', nan]\n",
      "['+            getFileSystem().create(filePathUnderMarker, false)));', nan]\n",
      "['+    assertTestObjectsExist();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCreateFileNoOverwrite() throws Throwable {', nan]\n",
      "['+    describe(\"verify the createFile() API also fails\");', nan]\n",
      "['+    head(fileKeyUnderMarker);', nan]\n",
      "['+    intercept(FileAlreadyExistsException.class, () ->', nan]\n",
      "['+        exec(\"create\", () ->', nan]\n",
      "['+            getFileSystem().createFile(filePathUnderMarker)', nan]\n",
      "['+                .overwrite(false)', nan]\n",
      "['+                .build()));', nan]\n",
      "['+    assertTestObjectsExist();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /*', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+    Delete.', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+  */', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testDelete() throws Throwable {', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    // a non recursive delete MUST fail because', nan]\n",
      "['+    // it is not empty', nan]\n",
      "['+    intercept(PathIsNotEmptyDirectoryException.class, () ->', nan]\n",
      "['+        fs.delete(markerDir, false));', nan]\n",
      "['+    // file is still there', nan]\n",
      "['+    head(fileKeyUnderMarker);', nan]\n",
      "['+', nan]\n",
      "['+    // recursive delete MUST succeed', nan]\n",
      "['+    fs.delete(markerDir, true);', nan]\n",
      "['+    // and the markers are gone', nan]\n",
      "['+    head404(fileKeyUnderMarker);', nan]\n",
      "['+    head404(markerKeySlash);', nan]\n",
      "['+    // just for completeness', nan]\n",
      "['+    fs.delete(basePath, true);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /*', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+    Rename.', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+  */', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Rename the base directory, expect the source files to move.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Whether or not the marker itself is copied depends on whether', nan]\n",
      "[\"+   * the release's rename operation explicitly skips\", nan]\n",
      "['+   * markers on renames.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRenameBase() throws Throwable {', nan]\n",
      "['+    describe(\"rename base directory\");', nan]\n",
      "['+', nan]\n",
      "['+    Path src = basePath;', nan]\n",
      "['+    Path dest = new Path(methodPath(), \"dest\");', nan]\n",
      "['+    assertRenamed(src, dest);', nan]\n",
      "['+', nan]\n",
      "['+    assertPathDoesNotExist(\"source\", src);', nan]\n",
      "['+    assertPathDoesNotExist(\"source\", filePathUnderMarker);', nan]\n",
      "['+    assertPathExists(\"dest not found\", dest);', nan]\n",
      "['+', nan]\n",
      "['+    // all the paths dest relative', nan]\n",
      "['+    Path destMarkerDir = new Path(dest, MARKER);', nan]\n",
      "['+    // peer path has the same initial name to make sure there', nan]\n",
      "['+    // is no confusion there.', nan]\n",
      "['+    Path destMarkerPeer = new Path(dest, MARKER_PEER);', nan]\n",
      "['+    String destMarkerKey = toKey(destMarkerDir);', nan]\n",
      "['+    String destMarkerKeySlash = destMarkerKey + \"/\";', nan]\n",
      "['+    String destFileKeyUnderMarker = destMarkerKeySlash + FILENAME;', nan]\n",
      "['+    Path destFilePathUnderMarker = new Path(destMarkerDir, FILENAME);', nan]\n",
      "['+    assertIsFile(destFilePathUnderMarker);', nan]\n",
      "['+    assertIsFile(destMarkerPeer);', nan]\n",
      "['+    head(destFileKeyUnderMarker);', nan]\n",
      "['+', nan]\n",
      "['+    // probe for the marker based on expected rename', nan]\n",
      "['+    // behavior', nan]\n",
      "['+    if (RENAME_COPIES_MARKERS) {', nan]\n",
      "['+      head(destMarkerKeySlash);', nan]\n",
      "['+    } else {', nan]\n",
      "['+      head404(destMarkerKeySlash);', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Rename a file under a marker by passing in the marker', nan]\n",
      "['+   * directory as the destination; the final path is derived', nan]\n",
      "['+   * from the original filename.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * After the rename:', nan]\n",
      "['+   * <ol>', nan]\n",
      "['+   *   <li>The data must be at the derived destination path.</li>', nan]\n",
      "['+   *   <li>The source file must not exist.</li>', nan]\n",
      "['+   *   <li>The parent dir of the source file must exist.</li>', nan]\n",
      "['+   *   <li>The marker above the destination file must not exist.</li>', nan]\n",
      "['+   * </ol>', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRenameUnderMarkerDir() throws Throwable {', nan]\n",
      "['+    describe(\"directory rename under an existing marker\");', nan]\n",
      "['+    String file = \"sourceFile\";', nan]\n",
      "['+    Path srcDir = new Path(basePath, \"srcdir\");', nan]\n",
      "['+    mkdirs(srcDir);', nan]\n",
      "['+    Path src = new Path(srcDir, file);', nan]\n",
      "['+    String srcKey = toKey(src);', nan]\n",
      "['+    put(srcKey, file);', nan]\n",
      "['+    head(srcKey);', nan]\n",
      "['+', nan]\n",
      "['+    // set the destination to be the marker directory.', nan]\n",
      "['+    Path dest = markerDir;', nan]\n",
      "['+    // rename the source file under the dest dir.', nan]\n",
      "['+    assertRenamed(src, dest);', nan]\n",
      "['+    assertIsFile(new Path(dest, file));', nan]\n",
      "['+    assertIsDirectory(srcDir);', nan]\n",
      "['+    if (isDeletingMarkers) {', nan]\n",
      "['+      head404(markerKeySlash);', nan]\n",
      "['+    } else {', nan]\n",
      "['+      head(markerKeySlash);', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Rename file under a marker, giving the full path to the destination', nan]\n",
      "['+   * file.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * After the rename:', nan]\n",
      "['+   * <ol>', nan]\n",
      "['+   *   <li>The data must be at the explicit destination path.</li>', nan]\n",
      "['+   *   <li>The source file must not exist.</li>', nan]\n",
      "['+   *   <li>The parent dir of the source file must exist.</li>', nan]\n",
      "['+   *   <li>The marker above the destination file must not exist.</li>', nan]\n",
      "['+   * </ol>', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRenameUnderMarkerWithPath() throws Throwable {', nan]\n",
      "['+    describe(\"directory rename under an existing marker\");', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    String file = \"sourceFile\";', nan]\n",
      "['+    Path srcDir = new Path(basePath, \"srcdir\");', nan]\n",
      "['+    mkdirs(srcDir);', nan]\n",
      "['+    Path src = new Path(srcDir, file);', nan]\n",
      "['+    String srcKey = toKey(src);', nan]\n",
      "['+    put(srcKey, file);', nan]\n",
      "['+    head(srcKey);', nan]\n",
      "['+', nan]\n",
      "['+    // set the destination to be the final file', nan]\n",
      "['+    Path dest = new Path(markerDir, \"destFile\");', nan]\n",
      "['+    // rename the source file to the destination file', nan]\n",
      "['+    assertRenamed(src, dest);', nan]\n",
      "['+    assertIsFile(dest);', nan]\n",
      "['+    assertIsDirectory(srcDir);', nan]\n",
      "['+    if (isDeletingMarkers) {', nan]\n",
      "['+      head404(markerKeySlash);', nan]\n",
      "['+    } else {', nan]\n",
      "['+      head(markerKeySlash);', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * This test creates an empty dir and renames it over the directory marker.', nan]\n",
      "['+   * If the dest was considered to be empty, the rename would fail.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRenameEmptyDirOverMarker() throws Throwable {', nan]\n",
      "['+    describe(\"rename an empty directory over the marker\");', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    String dir = \"sourceDir\";', nan]\n",
      "['+    Path src = new Path(basePath, dir);', nan]\n",
      "['+    fs.mkdirs(src);', nan]\n",
      "['+    assertIsDirectory(src);', nan]\n",
      "['+    String srcKey = toKey(src) + \"/\";', nan]\n",
      "['+    head(srcKey);', nan]\n",
      "['+    Path dest = markerDir;', nan]\n",
      "['+    // renamed into the dest dir', nan]\n",
      "['+    assertFalse(\"rename(\" + src + \", \" + dest + \") should have failed\",', nan]\n",
      "['+        getFileSystem().rename(src, dest));', nan]\n",
      "['+    // source is still there', nan]\n",
      "['+    assertIsDirectory(src);', nan]\n",
      "['+    head(srcKey);', nan]\n",
      "['+    // and a non-recursive delete lets us verify it is considered', nan]\n",
      "['+    // an empty dir', nan]\n",
      "['+    assertDeleted(src, false);', nan]\n",
      "['+    assertTestObjectsExist();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /*', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+    Utility methods and assertions.', nan]\n",
      "['+  =================================================================', nan]\n",
      "['+  */', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert the test objects exist.', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertTestObjectsExist() throws Exception {', nan]\n",
      "['+    head(fileKeyUnderMarker);', nan]\n",
      "['+    head(markerKeySlash);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Put a string to a path.', nan]\n",
      "['+   * @param key key', nan]\n",
      "['+   * @param content string', nan]\n",
      "['+   */', nan]\n",
      "['+  private void put(final String key, final String content) throws Exception {', nan]\n",
      "['+    exec(\"PUT \" + key, () ->', nan]\n",
      "['+        s3client.putObject(bucket, key, content));', nan]\n",
      "['+  }', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Delete an object.', nan]\n",
      "['+   * @param key key', nan]\n",
      "['+   * @param content string', nan]\n",
      "['+   */', nan]\n",
      "['+  private void deleteObject(final String key) throws Exception {', nan]\n",
      "['+    exec(\"DELETE \" + key, () -> {', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+      s3client.deleteObject(bucket, key);', nan]\n",
      "['+      return \"deleted \" + key;', nan]\n",
      "['+    });', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Issue a HEAD request.', nan]\n",
      "['+   * @param key', nan]\n",
      "['+   * @return a description of the object.', nan]\n",
      "['+   */', nan]\n",
      "['+  private String head(final String key) throws Exception {', nan]\n",
      "['+    ObjectMetadata md = exec(\"HEAD \" + key, () ->', nan]\n",
      "['+        s3client.getObjectMetadata(bucket, key));', nan]\n",
      "['+    return String.format(\"Object %s of length %d\",', nan]\n",
      "['+        key, md.getInstanceLength());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Issue a HEAD request and expect a 404 back.', nan]\n",
      "['+   * @param key', nan]\n",
      "['+   * @return the metadata', nan]\n",
      "['+   */', nan]\n",
      "['+  private void head404(final String key) throws Exception {', nan]\n",
      "['+    intercept(FileNotFoundException.class, \"\",', nan]\n",
      "['+        \"Expected 404 of \" + key, () ->', nan]\n",
      "['+        head(key));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute an operation; transate AWS exceptions.', nan]\n",
      "['+   * @param op operation', nan]\n",
      "['+   * @param call call to make', nan]\n",
      "['+   * @param <T> returned type', nan]\n",
      "['+   * @return result of the call.', nan]\n",
      "['+   * @throws Exception failure', nan]\n",
      "['+   */', nan]\n",
      "['+  private <T> T exec(String op, Callable<T> call) throws Exception {', nan]\n",
      "['+    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();', nan]\n",
      "['+    try {', nan]\n",
      "['+      return call.call();', nan]\n",
      "['+    } catch (AmazonClientException ex) {', nan]\n",
      "['+      throw S3AUtils.translateException(op, \"\", ex);', nan]\n",
      "['+    } finally {', nan]\n",
      "['+      timer.end(op);', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert that the listing contains only the status', nan]\n",
      "['+   * of the file under the marker.', nan]\n",
      "['+   * @param statuses status objects', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertContainsFileUnderMarkerOnly(', nan]\n",
      "['+      final List<FileStatus> statuses) {', nan]\n",
      "['+', nan]\n",
      "['+    assertContainsExactlyStatusOfPaths(statuses, filePathUnderMarker);', nan]\n",
      "['+    assertIsFileUnderMarker(statuses.get(0));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Expect the list of status objects to match that of the paths.', nan]\n",
      "['+   * @param statuses status object list', nan]\n",
      "['+   * @param paths ordered varargs list of paths', nan]\n",
      "['+   * @param <T> type of status objects', nan]\n",
      "['+   */', nan]\n",
      "['+  private <T extends FileStatus> void assertContainsExactlyStatusOfPaths(', nan]\n",
      "['+      List<T> statuses, Path... paths) {', nan]\n",
      "['+', nan]\n",
      "['+    String actual = statuses.stream()', nan]\n",
      "['+        .map(Object::toString)', nan]\n",
      "['+        .collect(Collectors.joining(\";\"));', nan]\n",
      "['+    String expected = Arrays.stream(paths)', nan]\n",
      "['+        .map(Object::toString)', nan]\n",
      "['+        .collect(Collectors.joining(\";\"));', nan]\n",
      "['+    String summary = \"expected [\" + expected + \"]\"', nan]\n",
      "['+        + \" actual = [\" + actual + \"]\";', nan]\n",
      "['+    assertEquals(\"mismatch in size of listing \" + summary,', nan]\n",
      "['+        paths.length, statuses.size());', nan]\n",
      "['+    for (int i = 0; i < statuses.size(); i++) {', nan]\n",
      "['+      assertEquals(\"Path mismatch at element \" + i + \" in \" + summary,', nan]\n",
      "['+          paths[i], statuses.get(i).getPath());', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert the status object refers to the file created', nan]\n",
      "['+   * under the marker.', nan]\n",
      "['+   * @param stat status object', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertIsFileUnderMarker(final FileStatus stat) {', nan]\n",
      "['+    assertIsFileAtPath(filePathUnderMarker, stat);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert the status object refers to a path at the given name.', nan]\n",
      "['+   * @param path path', nan]\n",
      "['+   * @param stat status object', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertIsFileAtPath(final Path path, final FileStatus stat) {', nan]\n",
      "['+    assertTrue(\"Is not file \" + stat, stat.isFile());', nan]\n",
      "['+    assertPathEquals(path, stat);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "[\"+   * Assert a status object's path matches expected.\", nan]\n",
      "['+   * @param path path to expect', nan]\n",
      "['+   * @param stat status object', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertPathEquals(final Path path, final FileStatus stat) {', nan]\n",
      "['+    assertEquals(\"filename is not the expected path :\" + stat,', nan]\n",
      "['+        path, stat.getPath());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Given a remote iterator of status objects,', nan]\n",
      "['+   * build a list of the values.', nan]\n",
      "['+   * @param status status list', nan]\n",
      "['+   * @param <T> actual type.', nan]\n",
      "['+   * @return source.', nan]\n",
      "['+   * @throws IOException', nan]\n",
      "['+   */', nan]\n",
      "['+  private <T extends FileStatus> List<FileStatus> toList(', nan]\n",
      "['+      RemoteIterator<T> status) throws IOException {', nan]\n",
      "['+', nan]\n",
      "['+    List<FileStatus> l = new ArrayList<>();', nan]\n",
      "['+    while (status.hasNext()) {', nan]\n",
      "['+      l.add(status.next());', nan]\n",
      "['+    }', nan]\n",
      "['+    return dump(l);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Given an array of status objects,', nan]\n",
      "['+   * build a list of the values.', nan]\n",
      "['+   * @param status status list', nan]\n",
      "['+   * @param <T> actual type.', nan]\n",
      "['+   * @return source.', nan]\n",
      "['+   * @throws IOException', nan]\n",
      "['+   */', nan]\n",
      "['+  private <T extends FileStatus> List<FileStatus> toList(', nan]\n",
      "['+      T[] status) throws IOException {', nan]\n",
      "['+    return dump(Arrays.asList(status));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Dump the string values of a list to the log; return', nan]\n",
      "['+   * the list.', nan]\n",
      "['+   * @param l source.', nan]\n",
      "['+   * @param <T> source type', nan]\n",
      "['+   * @return the list', nan]\n",
      "['+   */', nan]\n",
      "['+  private <T> List<T> dump(List<T> l) {', nan]\n",
      "['+    int c = 1;', nan]\n",
      "['+    for (T t : l) {', nan]\n",
      "['+      LOG.info(\"{}\\\\t{}\", c++, t);', nan]\n",
      "['+    }', nan]\n",
      "['+    return l;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Rename: assert the outcome is true.', nan]\n",
      "['+   * @param src source path', nan]\n",
      "['+   * @param dest dest path', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assertRenamed(final Path src, final Path dest)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    assertTrue(\"rename(\" + src + \", \" + dest + \") failed\",', nan]\n",
      "['+        getFileSystem().rename(src, dest));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Convert a path to a key; does not add any trailing / .', nan]\n",
      "['+   * @param path path in', nan]\n",
      "['+   * @return key out', nan]\n",
      "['+   */', nan]\n",
      "['+  private String toKey(final Path path) {', nan]\n",
      "['+    return getFileSystem().pathToKey(path);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Escape paths before handing to globStatus; this is needed as', nan]\n",
      "['+   * parameterized runs produce paths with [] in them.', nan]\n",
      "['+   * @param pathstr source path string', nan]\n",
      "['+   * @return an escaped path string', nan]\n",
      "['+   */', nan]\n",
      "['+  private String escape(String pathstr) {', nan]\n",
      "['+    StringBuilder r = new StringBuilder();', nan]\n",
      "['+    for (char c : pathstr.toCharArray()) {', nan]\n",
      "['+      String ch = Character.toString(c);', nan]\n",
      "['+      if (\"?*[{\".contains(ch)) {', nan]\n",
      "['+        r.append(\"\\\\\\\\\");', nan]\n",
      "['+      }', nan]\n",
      "['+      r.append(ch);', nan]\n",
      "['+    }', nan]\n",
      "['+    return r.toString();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3ADeleteCost.', 'ava b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3ADeleteC']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..d3d976e9289', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3ADeleteCost.java', nan]\n",
      "['@@ -0,0 +1,218 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.performance;', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+import java.util.Arrays;', nan]\n",
      "['+import java.util.Collection;', nan]\n",
      "['+', nan]\n",
      "['+import org.junit.Test;', nan]\n",
      "['+import org.junit.runner.RunWith;', nan]\n",
      "['+import org.junit.runners.Parameterized;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileStatus;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.Tristate;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.impl.StatusProbeEnum;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Statistic.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.performance.OperationCost.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.performance.OperationCostValidator.probe;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Use metrics to assert about the cost of file API calls.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Parameterized on guarded vs raw. and directory marker keep vs delete.', nan]\n",
      "['+ */', nan]\n",
      "['+@RunWith(Parameterized.class)', nan]\n",
      "['+public class ITestS3ADeleteCost extends AbstractS3ACostTest {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(ITestS3ADeleteCost.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameterization.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Parameterized.Parameters(name = \"{0}\")', nan]\n",
      "['+  public static Collection<Object[]> params() {', nan]\n",
      "['+    return Arrays.asList(new Object[][]{', nan]\n",
      "['+        {\"raw-keep-markers\", false, true, false},', nan]\n",
      "['+        {\"raw-delete-markers\", false, false, false},', nan]\n",
      "['+        {\"nonauth-keep-markers\", true, true, false},', nan]\n",
      "['+        {\"auth-delete-markers\", true, false, true}', nan]\n",
      "['+    });', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public ITestS3ADeleteCost(final String name,', nan]\n",
      "['+      final boolean s3guard,', nan]\n",
      "['+      final boolean keepMarkers,', nan]\n",
      "['+      final boolean authoritative) {', nan]\n",
      "['+    super(s3guard, keepMarkers, authoritative);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void teardown() throws Exception {', nan]\n",
      "['+    if (isKeepingMarkers()) {', nan]\n",
      "['+      // do this ourselves to avoid audits teardown failing', nan]\n",
      "['+      // when surplus markers are found', nan]\n",
      "['+      deleteTestDirInTeardown();', nan]\n",
      "['+    }', nan]\n",
      "['+    super.teardown();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * This creates a directory with a child and then deletes it.', nan]\n",
      "['+   * The parent dir must be found and declared as empty.', nan]\n",
      "['+   * <p>When deleting markers, that forces the recreation of a new marker.</p>', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testDeleteSingleFileInDir() throws Throwable {', nan]\n",
      "['+    describe(\"delete a file\");', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    // creates the marker', nan]\n",
      "['+    Path dir = dir(methodPath());', nan]\n",
      "['+    // file creation may have deleted that marker, but it may', nan]\n",
      "['+    // still be there', nan]\n",
      "['+    Path simpleFile = file(new Path(dir, \"simple.txt\"));', nan]\n",
      "['+', nan]\n",
      "['+    boolean rawAndKeeping = isRaw() && isDeleting();', nan]\n",
      "['+    boolean rawAndDeleting = isRaw() && isDeleting();', nan]\n",
      "['+    verifyMetrics(() -> {', nan]\n",
      "['+          fs.delete(simpleFile, false);', nan]\n",
      "['+          return \"after fs.delete(simpleFile) \" + getMetricSummary();', nan]\n",
      "['+        },', nan]\n",
      "['+        probe(rawAndKeeping, OBJECT_METADATA_REQUESTS,', nan]\n",
      "['+            FILESTATUS_FILE_PROBE_H),', nan]\n",
      "['+        // if deleting markers, look for the parent too', nan]\n",
      "['+        probe(rawAndDeleting, OBJECT_METADATA_REQUESTS,', nan]\n",
      "['+            FILESTATUS_FILE_PROBE_H + FILESTATUS_DIR_PROBE_H),', nan]\n",
      "['+        withWhenRaw(OBJECT_LIST_REQUESTS,', nan]\n",
      "['+            FILESTATUS_FILE_PROBE_L + FILESTATUS_DIR_PROBE_L),', nan]\n",
      "['+        with(DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        with(FILES_DELETED, 1),', nan]\n",
      "['+', nan]\n",
      "['+        // keeping: create no parent dirs or delete parents', nan]\n",
      "['+        withWhenKeeping(DIRECTORIES_CREATED, 0),', nan]\n",
      "['+        withWhenKeeping(OBJECT_DELETE_REQUESTS, DELETE_OBJECT_REQUEST),', nan]\n",
      "['+', nan]\n",
      "['+        // deleting: create a parent and delete any of its parents', nan]\n",
      "['+        withWhenDeleting(DIRECTORIES_CREATED, 1),', nan]\n",
      "['+        withWhenDeleting(OBJECT_DELETE_REQUESTS,', nan]\n",
      "['+            DELETE_OBJECT_REQUEST', nan]\n",
      "['+                + DELETE_MARKER_REQUEST)', nan]\n",
      "['+    );', nan]\n",
      "['+    // there is an empty dir for a parent', nan]\n",
      "['+    S3AFileStatus status = verifyRawInnerGetFileStatus(dir, true,', nan]\n",
      "['+        StatusProbeEnum.ALL, GET_FILE_STATUS_ON_DIR);', nan]\n",
      "['+    assertEmptyDirStatus(status, Tristate.TRUE);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * This creates a directory with a two files and then deletes one of the', nan]\n",
      "['+   * files.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testDeleteFileInDir() throws Throwable {', nan]\n",
      "['+    describe(\"delete a file in a directory with multiple files\");', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+    // creates the marker', nan]\n",
      "['+    Path dir = dir(methodPath());', nan]\n",
      "['+    // file creation may have deleted that marker, but it may', nan]\n",
      "['+    // still be there', nan]\n",
      "['+    Path file1 = file(new Path(dir, \"file1.txt\"));', nan]\n",
      "['+    Path file2 = file(new Path(dir, \"file2.txt\"));', nan]\n",
      "['+', nan]\n",
      "['+    boolean rawAndKeeping = isRaw() && isDeleting();', nan]\n",
      "['+    boolean rawAndDeleting = isRaw() && isDeleting();', nan]\n",
      "['+    verifyMetrics(() -> {', nan]\n",
      "['+      fs.delete(file1, false);', nan]\n",
      "['+      return \"after fs.delete(file1simpleFile) \" + getMetricSummary();', nan]\n",
      "['+    },', nan]\n",
      "[\"+        // delete file. For keeping: that's it\", nan]\n",
      "['+        probe(rawAndKeeping, OBJECT_METADATA_REQUESTS,', nan]\n",
      "['+            FILESTATUS_FILE_PROBE_H),', nan]\n",
      "['+        // if deleting markers, look for the parent too', nan]\n",
      "['+        probe(rawAndDeleting, OBJECT_METADATA_REQUESTS,', nan]\n",
      "['+            FILESTATUS_FILE_PROBE_H + FILESTATUS_DIR_PROBE_H),', nan]\n",
      "['+        withWhenRaw(OBJECT_LIST_REQUESTS,', nan]\n",
      "['+            FILESTATUS_FILE_PROBE_L + FILESTATUS_DIR_PROBE_L),', nan]\n",
      "['+        with(DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        with(FILES_DELETED, 1),', nan]\n",
      "['+', nan]\n",
      "['+        // no need to create a parent', nan]\n",
      "['+        with(DIRECTORIES_CREATED, 0),', nan]\n",
      "['+', nan]\n",
      "['+        // keeping: create no parent dirs or delete parents', nan]\n",
      "['+        withWhenKeeping(OBJECT_DELETE_REQUESTS, DELETE_OBJECT_REQUEST),', nan]\n",
      "['+', nan]\n",
      "['+        // deleting: create a parent and delete any of its parents', nan]\n",
      "['+        withWhenDeleting(OBJECT_DELETE_REQUESTS,', nan]\n",
      "['+            DELETE_OBJECT_REQUEST));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testDirMarkersSubdir() throws Throwable {', nan]\n",
      "['+    describe(\"verify cost of deep subdir creation\");', nan]\n",
      "['+', nan]\n",
      "['+    Path subDir = new Path(methodPath(), \"1/2/3/4/5/6\");', nan]\n",
      "['+    // one dir created, possibly a parent removed', nan]\n",
      "['+    verifyMetrics(() -> {', nan]\n",
      "['+      mkdirs(subDir);', nan]\n",
      "['+      return \"after mkdir(subDir) \" + getMetricSummary();', nan]\n",
      "['+    },', nan]\n",
      "['+        with(DIRECTORIES_CREATED, 1),', nan]\n",
      "['+        with(DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        withWhenKeeping(OBJECT_DELETE_REQUESTS, 0),', nan]\n",
      "['+        withWhenKeeping(FAKE_DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        withWhenDeleting(OBJECT_DELETE_REQUESTS, DELETE_MARKER_REQUEST),', nan]\n",
      "['+        // delete all possible fake dirs above the subdirectory', nan]\n",
      "['+        withWhenDeleting(FAKE_DIRECTORIES_DELETED,', nan]\n",
      "['+            directoriesInPath(subDir) - 1));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testDirMarkersFileCreation() throws Throwable {', nan]\n",
      "['+    describe(\"verify cost of file creation\");', nan]\n",
      "['+', nan]\n",
      "['+    Path srcBaseDir = dir(methodPath());', nan]\n",
      "['+', nan]\n",
      "['+    Path srcDir = dir(new Path(srcBaseDir, \"1/2/3/4/5/6\"));', nan]\n",
      "['+', nan]\n",
      "['+    // creating a file should trigger demise of the src dir marker', nan]\n",
      "['+    // unless markers are being kept', nan]\n",
      "['+', nan]\n",
      "['+    verifyMetrics(() -> {', nan]\n",
      "['+      file(new Path(srcDir, \"source.txt\"));', nan]\n",
      "['+      return \"after touch(fs, srcFilePath) \" + getMetricSummary();', nan]\n",
      "['+    },', nan]\n",
      "['+        with(DIRECTORIES_CREATED, 0),', nan]\n",
      "['+        with(DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        // keeping: no delete operations.', nan]\n",
      "['+        withWhenKeeping(OBJECT_DELETE_REQUESTS, 0),', nan]\n",
      "['+        withWhenKeeping(FAKE_DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        // delete all possible fake dirs above the file', nan]\n",
      "['+        withWhenDeleting(OBJECT_DELETE_REQUESTS, 1),', nan]\n",
      "['+        withWhenDeleting(FAKE_DIRECTORIES_DELETED,', nan]\n",
      "['+            directoriesInPath(srcDir)));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3ARenameCost.', 'ava b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3ARenameC']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..85c70768356', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3ARenameCost.java', nan]\n",
      "['@@ -0,0 +1,207 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.performance;', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+import java.util.Arrays;', nan]\n",
      "['+import java.util.Collection;', nan]\n",
      "['+import java.util.UUID;', nan]\n",
      "['+', nan]\n",
      "['+import org.assertj.core.api.Assertions;', nan]\n",
      "['+import org.junit.Test;', nan]\n",
      "['+import org.junit.runner.RunWith;', nan]\n",
      "['+import org.junit.runners.Parameterized;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Statistic.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.performance.OperationCost.*;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Use metrics to assert about the cost of file API calls.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Parameterized on guarded vs raw. and directory marker keep vs delete', nan]\n",
      "['+ */', nan]\n",
      "['+@RunWith(Parameterized.class)', nan]\n",
      "['+public class ITestS3ARenameCost extends AbstractS3ACostTest {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(ITestS3ARenameCost.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Parameterization.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Parameterized.Parameters(name = \"{0}\")', nan]\n",
      "['+  public static Collection<Object[]> params() {', nan]\n",
      "['+    return Arrays.asList(new Object[][]{', nan]\n",
      "['+        {\"raw-keep-markers\", false, true, false},', nan]\n",
      "['+        {\"raw-delete-markers\", false, false, false},', nan]\n",
      "['+        {\"nonauth-keep-markers\", true, true, false},', nan]\n",
      "['+        {\"auth-delete-markers\", true, false, true}', nan]\n",
      "['+    });', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public ITestS3ARenameCost(final String name,', nan]\n",
      "['+      final boolean s3guard,', nan]\n",
      "['+      final boolean keepMarkers,', nan]\n",
      "['+      final boolean authoritative) {', nan]\n",
      "['+    super(s3guard, keepMarkers, authoritative);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRenameFileToDifferentDirectory() throws Throwable {', nan]\n",
      "['+    describe(\"rename a file to a different directory, \"', nan]\n",
      "['+        + \"keeping the source dir present\");', nan]\n",
      "['+', nan]\n",
      "['+    Path baseDir = dir(methodPath());', nan]\n",
      "['+', nan]\n",
      "['+    Path srcDir = new Path(baseDir, \"1/2/3/4/5/6\");', nan]\n",
      "['+    final Path srcFilePath = file(new Path(srcDir, \"source.txt\"));', nan]\n",
      "['+', nan]\n",
      "['+    // create a new source file.', nan]\n",
      "['+    // Explicitly use a new path object to guarantee that the parent paths', nan]\n",
      "['+    // are different object instances and so equals() rather than ==', nan]\n",
      "['+    // is', nan]\n",
      "['+    Path parent2 = srcFilePath.getParent();', nan]\n",
      "['+    Path srcFile2 = file(new Path(parent2, \"source2.txt\"));', nan]\n",
      "['+    Assertions.assertThat(srcDir)', nan]\n",
      "['+        .isNotSameAs(parent2);', nan]\n",
      "['+    Assertions.assertThat(srcFilePath.getParent())', nan]\n",
      "['+        .isEqualTo(srcFile2.getParent());', nan]\n",
      "['+', nan]\n",
      "['+    // create a directory tree, expect the dir to be created and', nan]\n",
      "['+    // possibly a request to delete all parent directories made.', nan]\n",
      "['+    Path destBaseDir = new Path(baseDir, \"dest\");', nan]\n",
      "['+    Path destDir = dir(new Path(destBaseDir, \"a/b/c/d\"));', nan]\n",
      "['+    Path destFilePath = new Path(destDir, \"dest.txt\");', nan]\n",
      "['+', nan]\n",
      "['+    // rename the source file to the destination file.', nan]\n",
      "['+    // this tests file rename, not dir rename', nan]\n",
      "['+    // as srcFile2 exists, the parent dir of srcFilePath must not be created.', nan]\n",
      "['+    verifyMetrics(() ->', nan]\n",
      "['+            execRename(srcFilePath, destFilePath),', nan]\n",
      "['+        whenRaw(RENAME_SINGLE_FILE_DIFFERENT_DIR),', nan]\n",
      "['+        with(DIRECTORIES_CREATED, 0),', nan]\n",
      "['+        with(DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        // keeping: only the core delete operation is issued.', nan]\n",
      "['+        withWhenKeeping(OBJECT_DELETE_REQUESTS, DELETE_OBJECT_REQUEST),', nan]\n",
      "['+        withWhenKeeping(FAKE_DIRECTORIES_DELETED, 0),', nan]\n",
      "['+        // deleting: delete any fake marker above the destination.', nan]\n",
      "['+        withWhenDeleting(OBJECT_DELETE_REQUESTS,', nan]\n",
      "['+            DELETE_OBJECT_REQUEST + DELETE_MARKER_REQUEST),', nan]\n",
      "['+        withWhenDeleting(FAKE_DIRECTORIES_DELETED,', nan]\n",
      "['+            directoriesInPath(destDir)));', nan]\n",
      "['+', nan]\n",
      "['+    assertIsFile(destFilePath);', nan]\n",
      "['+    assertIsDirectory(srcDir);', nan]\n",
      "['+    assertPathDoesNotExist(\"should have gone in the rename\", srcFilePath);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "[\"+   * Same directory rename is lower cost as there's no need to\", nan]\n",
      "['+   * look for the parent dir of the dest path or worry about', nan]\n",
      "['+   * deleting markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRenameSameDirectory() throws Throwable {', nan]\n",
      "['+    describe(\"rename a file to the same directory\");', nan]\n",
      "['+', nan]\n",
      "['+    Path baseDir = dir(methodPath());', nan]\n",
      "['+    final Path sourceFile = file(new Path(baseDir, \"source.txt\"));', nan]\n",
      "['+', nan]\n",
      "['+    // create a new source file.', nan]\n",
      "['+    // Explicitly use a new path object to guarantee that the parent paths', nan]\n",
      "['+    // are different object instances and so equals() rather than ==', nan]\n",
      "['+    // is', nan]\n",
      "['+    Path parent2 = sourceFile.getParent();', nan]\n",
      "['+    Path destFile = new Path(parent2, \"dest\");', nan]\n",
      "['+    verifyMetrics(() ->', nan]\n",
      "['+            execRename(sourceFile, destFile),', nan]\n",
      "['+        whenRaw(RENAME_SINGLE_FILE_SAME_DIR),', nan]\n",
      "['+        with(OBJECT_COPY_REQUESTS, 1),', nan]\n",
      "['+        with(DIRECTORIES_CREATED, 0),', nan]\n",
      "['+        with(OBJECT_DELETE_REQUESTS, DELETE_OBJECT_REQUEST),', nan]\n",
      "['+        with(FAKE_DIRECTORIES_DELETED, 0));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCostOfRootFileRename() throws Throwable {', nan]\n",
      "['+    describe(\"assert that a root file rename doesn\\'t\"', nan]\n",
      "['+        + \" do much in terms of parent dir operations\");', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+', nan]\n",
      "[\"+    // unique name, so that even when run in parallel tests, there's no conflict\", nan]\n",
      "['+    String uuid = UUID.randomUUID().toString();', nan]\n",
      "['+    Path src = file(new Path(\"/src-\" + uuid));', nan]\n",
      "['+    Path dest = new Path(\"/dest-\" + uuid);', nan]\n",
      "['+    try {', nan]\n",
      "['+      verifyMetrics(() -> {', nan]\n",
      "['+        fs.rename(src, dest);', nan]\n",
      "['+        return \"after fs.rename(/src,/dest) \" + getMetricSummary();', nan]\n",
      "['+      },', nan]\n",
      "['+          whenRaw(FILE_STATUS_FILE_PROBE', nan]\n",
      "['+              .plus(GET_FILE_STATUS_FNFE)', nan]\n",
      "['+              .plus(COPY_OP)),', nan]\n",
      "['+          // here we expect there to be no fake directories', nan]\n",
      "['+          with(DIRECTORIES_CREATED, 0),', nan]\n",
      "['+          // one for the renamed file only', nan]\n",
      "['+          with(OBJECT_DELETE_REQUESTS,', nan]\n",
      "['+              DELETE_OBJECT_REQUEST),', nan]\n",
      "['+          // no directories are deleted: This is root', nan]\n",
      "['+          with(DIRECTORIES_DELETED, 0),', nan]\n",
      "['+          // no fake directories are deleted: This is root', nan]\n",
      "['+          with(FAKE_DIRECTORIES_DELETED, 0),', nan]\n",
      "['+          with(FILES_DELETED, 1));', nan]\n",
      "['+    } finally {', nan]\n",
      "['+      fs.delete(src, false);', nan]\n",
      "['+      fs.delete(dest, false);', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCostOfRootFileDelete() throws Throwable {', nan]\n",
      "['+    describe(\"assert that a root file delete doesn\\'t\"', nan]\n",
      "['+        + \" do much in terms of parent dir operations\");', nan]\n",
      "['+    S3AFileSystem fs = getFileSystem();', nan]\n",
      "['+', nan]\n",
      "[\"+    // unique name, so that even when run in parallel tests, there's no conflict\", nan]\n",
      "['+    String uuid = UUID.randomUUID().toString();', nan]\n",
      "['+    Path src = file(new Path(\"/src-\" + uuid));', nan]\n",
      "['+    try {', nan]\n",
      "['+      // delete that destination file, assert only the file delete was issued', nan]\n",
      "['+      verifyMetrics(() -> {', nan]\n",
      "['+        fs.delete(src, false);', nan]\n",
      "['+        return \"after fs.delete(/dest) \" + getMetricSummary();', nan]\n",
      "['+      },', nan]\n",
      "['+          with(DIRECTORIES_CREATED, 0),', nan]\n",
      "['+          with(DIRECTORIES_DELETED, 0),', nan]\n",
      "['+          with(FAKE_DIRECTORIES_DELETED, 0),', nan]\n",
      "['+          with(FILES_DELETED, 1),', nan]\n",
      "['+          with(OBJECT_DELETE_REQUESTS, DELETE_OBJECT_REQUEST),', nan]\n",
      "['+          whenRaw(FILE_STATUS_FILE_PROBE)); /* no need to look at parent. */', nan]\n",
      "['+', nan]\n",
      "['+    } finally {', nan]\n",
      "['+      fs.delete(src, false);', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/OperationCost.java', '/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/OperationCost.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..46a6b712c49', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/OperationCost.java', nan]\n",
      "['@@ -0,0 +1,201 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.performance;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Declaration of the costs of head and list calls for various FS IO', nan]\n",
      "['+ * operations.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * An instance declares the number of head and list calls expected for', nan]\n",
      "['+ * various operations -with a {@link #plus(OperationCost)}', nan]\n",
      "['+ * method to add operation costs together to produce an', nan]\n",
      "['+ * aggregate cost. These can then be validated in tests', nan]\n",
      "['+ * via {@link OperationCostValidator}.', nan]\n",
      "['+ *', nan]\n",
      "['+ */', nan]\n",
      "['+public final class OperationCost {', nan]\n",
      "['+', nan]\n",
      "['+  /** Head costs for getFileStatus() directory probe: {@value}. */', nan]\n",
      "['+  public static final int FILESTATUS_DIR_PROBE_H = 0;', nan]\n",
      "['+', nan]\n",
      "['+  /** List costs for getFileStatus() directory probe: {@value}. */', nan]\n",
      "['+  public static final int FILESTATUS_DIR_PROBE_L = 1;', nan]\n",
      "['+', nan]\n",
      "['+  /** Head cost getFileStatus() file probe only. */', nan]\n",
      "['+  public static final int FILESTATUS_FILE_PROBE_H = 1;', nan]\n",
      "['+', nan]\n",
      "['+  /** Liast cost getFileStatus() file probe only. */', nan]\n",
      "['+', nan]\n",
      "['+  public static final int FILESTATUS_FILE_PROBE_L = 0;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Delete cost when deleting an object.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final int DELETE_OBJECT_REQUEST = 1;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Delete cost when deleting a marker.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final int DELETE_MARKER_REQUEST = DELETE_OBJECT_REQUEST;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * No IO takes place.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost NO_IO =', nan]\n",
      "['+      new OperationCost(0, 0);', nan]\n",
      "['+', nan]\n",
      "['+  /** A HEAD operation. */', nan]\n",
      "['+  public static final OperationCost HEAD_OPERATION = new OperationCost(1, 0);', nan]\n",
      "['+', nan]\n",
      "['+  /** A LIST operation. */', nan]\n",
      "['+  public static final OperationCost LIST_OPERATION = new OperationCost(0, 1);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Cost of {@link org.apache.hadoop.fs.s3a.impl.StatusProbeEnum#DIRECTORIES}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost FILE_STATUS_DIR_PROBE = LIST_OPERATION;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Cost of {@link org.apache.hadoop.fs.s3a.impl.StatusProbeEnum#FILE}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost FILE_STATUS_FILE_PROBE = HEAD_OPERATION;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Cost of {@link org.apache.hadoop.fs.s3a.impl.StatusProbeEnum#ALL}.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost FILE_STATUS_ALL_PROBES =', nan]\n",
      "['+      FILE_STATUS_FILE_PROBE.plus(FILE_STATUS_DIR_PROBE);', nan]\n",
      "['+', nan]\n",
      "['+  /** getFileStatus() on a file which exists. */', nan]\n",
      "['+  public static final OperationCost GET_FILE_STATUS_ON_FILE =', nan]\n",
      "['+      FILE_STATUS_FILE_PROBE;', nan]\n",
      "['+', nan]\n",
      "['+  /** List costs for getFileStatus() on a non-empty directory: {@value}. */', nan]\n",
      "['+  public static final OperationCost GET_FILE_STATUS_ON_DIR =', nan]\n",
      "['+      FILE_STATUS_FILE_PROBE.plus(FILE_STATUS_DIR_PROBE);', nan]\n",
      "['+', nan]\n",
      "['+  /** Costs for getFileStatus() on an empty directory: {@value}. */', nan]\n",
      "['+  public static final OperationCost GET_FILE_STATUS_ON_EMPTY_DIR =', nan]\n",
      "['+      GET_FILE_STATUS_ON_DIR;', nan]\n",
      "['+', nan]\n",
      "['+  /** getFileStatus() directory marker which exists. */', nan]\n",
      "['+  public static final OperationCost GET_FILE_STATUS_ON_DIR_MARKER =', nan]\n",
      "['+      GET_FILE_STATUS_ON_EMPTY_DIR;', nan]\n",
      "['+', nan]\n",
      "['+  /** getFileStatus() call which fails to find any entry. */', nan]\n",
      "['+  public static final OperationCost GET_FILE_STATUS_FNFE =', nan]\n",
      "['+      FILE_STATUS_ALL_PROBES;', nan]\n",
      "['+', nan]\n",
      "['+  /** listLocatedStatus always does a LIST. */', nan]\n",
      "['+  public static final OperationCost LIST_LOCATED_STATUS_LIST_OP =', nan]\n",
      "['+      new OperationCost(0, 1);', nan]\n",
      "['+', nan]\n",
      "['+  /** listFiles always does a LIST. */', nan]\n",
      "['+  public static final OperationCost LIST_FILES_LIST_OP =', nan]\n",
      "['+      new OperationCost(0, 1);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Metadata cost of a copy operation, as used during rename.', nan]\n",
      "['+   * This happens even if the store is guarded.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost COPY_OP =', nan]\n",
      "['+      new OperationCost(1, 0);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Cost of renaming a file to a different directory.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * LIST on dest not found, look for dest dir, and then, at', nan]\n",
      "['+   * end of rename, whether a parent dir needs to be created.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost RENAME_SINGLE_FILE_DIFFERENT_DIR =', nan]\n",
      "['+      FILE_STATUS_FILE_PROBE              // source file probe', nan]\n",
      "['+          .plus(GET_FILE_STATUS_FNFE)     // dest does not exist', nan]\n",
      "['+          .plus(FILE_STATUS_DIR_PROBE)    // parent dir of dest', nan]\n",
      "['+          .plus(FILE_STATUS_DIR_PROBE)    // recreate source parent dir?', nan]\n",
      "['+          .plus(COPY_OP);                 // metadata read on copy', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Cost of renaming a file to the same directory', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * No need to look for parent directories, so only file', nan]\n",
      "['+   * existence checks and the copy.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost RENAME_SINGLE_FILE_SAME_DIR =', nan]\n",
      "['+      FILE_STATUS_FILE_PROBE              // source file probe', nan]\n",
      "['+          .plus(GET_FILE_STATUS_FNFE)     // dest must not exist', nan]\n",
      "['+          .plus(COPY_OP);                 // metadata read on copy', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * create(overwrite = true) does not look for the file existing.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost CREATE_FILE_OVERWRITE =', nan]\n",
      "['+      FILE_STATUS_DIR_PROBE;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * create(overwrite = false) runs all the checks.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final OperationCost CREATE_FILE_NO_OVERWRITE =', nan]\n",
      "['+      FILE_STATUS_ALL_PROBES;', nan]\n",
      "['+', nan]\n",
      "['+  /** Expected HEAD count. */', nan]\n",
      "['+  private final int head;', nan]\n",
      "['+', nan]\n",
      "['+  /** Expected LIST count. */', nan]\n",
      "['+  private final int list;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Constructor.', nan]\n",
      "['+   * @param head head requests.', nan]\n",
      "['+   * @param list list requests.', nan]\n",
      "['+   */', nan]\n",
      "['+  public OperationCost(final int head,', nan]\n",
      "['+      final int list) {', nan]\n",
      "['+    this.head = head;', nan]\n",
      "['+    this.list = list;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /** Expected HEAD count. */', nan]\n",
      "['+  int head() {', nan]\n",
      "['+    return head;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /** Expected LIST count. */', nan]\n",
      "['+  int list() {', nan]\n",
      "['+    return list;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Add to create a new cost.', nan]\n",
      "['+   * @param that the other entry', nan]\n",
      "['+   * @return cost of the combined operation.', nan]\n",
      "['+   */', nan]\n",
      "['+  public OperationCost plus(OperationCost that) {', nan]\n",
      "['+    return new OperationCost(', nan]\n",
      "['+        head + that.head,', nan]\n",
      "['+        list + that.list);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public String toString() {', nan]\n",
      "['+    return \"OperationCost{\" +', nan]\n",
      "['+        \"head=\" + head +', nan]\n",
      "['+        \", list=\" + list +', nan]\n",
      "[\"+        '}';\", nan]\n",
      "['+  }', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/OperationCostValida', 'or.java b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/OperationCo']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..c351d1b185a', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/OperationCostValidator.jav', nan]\n",
      "['@@ -0,0 +1,483 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.performance;', nan]\n",
      "['+', nan]\n",
      "['+import java.util.ArrayList;', nan]\n",
      "['+import java.util.Arrays;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+import java.util.Map;', nan]\n",
      "['+import java.util.TreeMap;', nan]\n",
      "['+import java.util.concurrent.Callable;', nan]\n",
      "['+import java.util.stream.Collectors;', nan]\n",
      "['+', nan]\n",
      "['+import org.assertj.core.api.Assumptions;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3ATestUtils;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.Statistic;', nan]\n",
      "['+', nan]\n",
      "['+import static java.util.Objects.requireNonNull;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Statistic.OBJECT_LIST_REQUESTS;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Statistic.OBJECT_METADATA_REQUESTS;', nan]\n",
      "['+import static org.apache.hadoop.test.LambdaTestUtils.intercept;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Support for declarative assertions about operation cost.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Usage: A builder is used to declare the set of statistics', nan]\n",
      "['+ * to be monitored in the filesystem.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * A call to {@link #exec(Callable, ExpectedProbe...)}', nan]\n",
      "['+ * executes the callable if 1+ probe is enabled; after', nan]\n",
      "['+ * invocation the probes are validated.', nan]\n",
      "['+ * The result of the callable is returned.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * A call of {@link #intercepting(Class, String, Callable, ExpectedProbe...)}', nan]\n",
      "['+ * Invokes the callable if 1+ probe is enabled, expects an exception', nan]\n",
      "['+ * to be raised and then verifies metrics declared in the probes.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * Probes are built up from the static method to create probes', nan]\n",
      "['+ * for metrics:', nan]\n",
      "['+ * <ul>', nan]\n",
      "['+ *   <li>{@link #probe(boolean, Statistic, int)} </li>', nan]\n",
      "['+ *   <li>{@link #probe(Statistic, int)} </li>', nan]\n",
      "['+ *   <li>{@link #probes(boolean, ExpectedProbe...)} (Statistic, int)} </li>', nan]\n",
      "['+ *   <li>{@link #always()}</li>', nan]\n",
      "['+ * </ul>', nan]\n",
      "['+ * If any probe evaluates to false, an assertion is raised.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * When this happens: look in the logs!', nan]\n",
      "['+ * The logs will contain the whole set of metrics, the probe details', nan]\n",
      "['+ * and the result of the call.', nan]\n",
      "['+ */', nan]\n",
      "['+public final class OperationCostValidator {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(OperationCostValidator.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * The empty probe: declared as disabled.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final ExpectedProbe EMPTY_PROBE =', nan]\n",
      "['+      new EmptyProbe(\"empty\", false);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A probe which is always enabled.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final ExpectedProbe ALWAYS_PROBE =', nan]\n",
      "['+      new EmptyProbe(\"always\", true);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * The map of metric diffs to track.', nan]\n",
      "['+   */', nan]\n",
      "['+  private final Map<String, S3ATestUtils.MetricDiff> metricDiffs', nan]\n",
      "['+      = new TreeMap<>();', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Build the instance.', nan]\n",
      "['+   * @param builder builder containing all options.', nan]\n",
      "['+   */', nan]\n",
      "['+  private OperationCostValidator(Builder builder) {', nan]\n",
      "['+    builder.metrics.forEach(stat ->', nan]\n",
      "['+        metricDiffs.put(stat.getSymbol(),', nan]\n",
      "['+            new S3ATestUtils.MetricDiff(builder.filesystem, stat)));', nan]\n",
      "['+    builder.metrics.clear();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Reset all the metrics being tracked.', nan]\n",
      "['+   */', nan]\n",
      "['+  public void resetMetricDiffs() {', nan]\n",
      "['+    metricDiffs.values().forEach(S3ATestUtils.MetricDiff::reset);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the diff of a statistic.', nan]\n",
      "['+   * @param stat statistic to look up', nan]\n",
      "['+   * @return the value', nan]\n",
      "['+   * @throws NullPointerException if there is no match', nan]\n",
      "['+   */', nan]\n",
      "['+  public S3ATestUtils.MetricDiff get(Statistic stat) {', nan]\n",
      "['+    S3ATestUtils.MetricDiff diff =', nan]\n",
      "['+        requireNonNull(metricDiffs.get(stat.getSymbol()),', nan]\n",
      "['+            () -> \"No metric tracking for \" + stat);', nan]\n",
      "['+    return diff;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute a closure and verify the metrics.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * If no probes are active, the operation will', nan]\n",
      "['+   * raise an Assumption exception for the test to be skipped.', nan]\n",
      "['+   * @param eval closure to evaluate', nan]\n",
      "['+   * @param expected varargs list of expected diffs', nan]\n",
      "['+   * @param <T> return type.', nan]\n",
      "['+   * @return the result of the evaluation', nan]\n",
      "['+   */', nan]\n",
      "['+  public <T> T exec(', nan]\n",
      "['+      Callable<T> eval,', nan]\n",
      "['+      ExpectedProbe... expectedA) throws Exception {', nan]\n",
      "['+    List<ExpectedProbe> expected = Arrays.asList(expectedA);', nan]\n",
      "['+    resetMetricDiffs();', nan]\n",
      "['+    // verify that 1+ probe is enabled', nan]\n",
      "['+    assumeProbesEnabled(expected);', nan]\n",
      "['+    // if we get here, then yes.', nan]\n",
      "['+    // evaluate it', nan]\n",
      "['+    T r = eval.call();', nan]\n",
      "['+    // build the text for errors', nan]\n",
      "['+    String text =', nan]\n",
      "['+        \"operation returning \"', nan]\n",
      "['+            + (r != null ? r.toString() : \"null\");', nan]\n",
      "['+    LOG.info(\"{}\", text);', nan]\n",
      "['+    LOG.info(\"state {}\", this);', nan]\n",
      "['+    LOG.info(\"probes {}\", expected);', nan]\n",
      "['+    for (ExpectedProbe ed : expected) {', nan]\n",
      "['+      ed.verify(this, text);', nan]\n",
      "['+    }', nan]\n",
      "['+    return r;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Scan all probes for being enabled.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * If none of them are enabled, the evaluation will be skipped.', nan]\n",
      "['+   * @param expected list of expected probes', nan]\n",
      "['+   */', nan]\n",
      "['+  private void assumeProbesEnabled(List<ExpectedProbe> expected) {', nan]\n",
      "['+    boolean enabled = false;', nan]\n",
      "['+    for (ExpectedProbe ed : expected) {', nan]\n",
      "['+      enabled |= ed.isEnabled();', nan]\n",
      "['+    }', nan]\n",
      "['+    String pstr = expected.stream()', nan]\n",
      "['+        .map(Object::toString)', nan]\n",
      "['+        .collect(Collectors.joining(\", \"));', nan]\n",
      "['+    Assumptions.assumeThat(enabled)', nan]\n",
      "['+        .describedAs(\"metrics to probe for are not enabled in %s\", pstr)', nan]\n",
      "['+        .isTrue();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute a closure, expecting an exception.', nan]\n",
      "['+   * Verify the metrics after the exception has been caught and', nan]\n",
      "['+   * validated.', nan]\n",
      "['+   * @param clazz type of exception', nan]\n",
      "['+   * @param text text to look for in exception (optional)', nan]\n",
      "['+   * @param eval closure to evaluate', nan]\n",
      "['+   * @param expected varargs list of expected diffs', nan]\n",
      "['+   * @param <T> return type of closure', nan]\n",
      "['+   * @param <E> exception type', nan]\n",
      "['+   * @return the exception caught.', nan]\n",
      "['+   * @throws Exception any other exception', nan]\n",
      "['+   */', nan]\n",
      "['+  public <T, E extends Throwable> E intercepting(', nan]\n",
      "['+      Class<E> clazz,', nan]\n",
      "['+      String text,', nan]\n",
      "['+      Callable<T> eval,', nan]\n",
      "['+      ExpectedProbe... expected) throws Exception {', nan]\n",
      "['+', nan]\n",
      "['+    return exec(() ->', nan]\n",
      "['+            intercept(clazz, text, eval),', nan]\n",
      "['+        expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public String toString() {', nan]\n",
      "['+    return metricDiffs.values().stream()', nan]\n",
      "['+        .map(S3ATestUtils.MetricDiff::toString)', nan]\n",
      "['+        .collect(Collectors.joining(\", \"));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a builder for the cost checker.', nan]\n",
      "['+   *', nan]\n",
      "['+   * @param fs filesystem.', nan]\n",
      "['+   * @return builder.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static Builder builder(S3AFileSystem fs) {', nan]\n",
      "['+    return new Builder(fs);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * builder.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final class Builder {', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Filesystem.', nan]\n",
      "['+     */', nan]\n",
      "['+    private final S3AFileSystem filesystem;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Metrics to create.', nan]\n",
      "['+     */', nan]\n",
      "['+    private final List<Statistic> metrics = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Create with a required filesystem.', nan]\n",
      "['+     * @param filesystem monitored filesystem', nan]\n",
      "['+     */', nan]\n",
      "['+    public Builder(final S3AFileSystem filesystem) {', nan]\n",
      "['+      this.filesystem = requireNonNull(filesystem);', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Add a single metric.', nan]\n",
      "['+     * @param statistic statistic to monitor.', nan]\n",
      "['+     * @return this', nan]\n",
      "['+     */', nan]\n",
      "['+    public Builder withMetric(Statistic statistic) {', nan]\n",
      "['+      return withMetric(statistic);', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Add a varargs list of metrics.', nan]\n",
      "['+     * @param stat statistics to monitor.', nan]\n",
      "['+     * @return this.', nan]\n",
      "['+     */', nan]\n",
      "['+    public Builder withMetrics(Statistic...stats) {', nan]\n",
      "['+      metrics.addAll(Arrays.asList(stats));', nan]\n",
      "['+      return this;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Instantiate.', nan]\n",
      "['+     * @return the validator.', nan]\n",
      "['+     */', nan]\n",
      "['+    public OperationCostValidator build() {', nan]\n",
      "['+      return new OperationCostValidator(this);', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get the \"always\" probe.', nan]\n",
      "['+   * @return a probe which always triggers execution.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static ExpectedProbe always() {', nan]\n",
      "['+    return ALWAYS_PROBE;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a probe of a statistic which is enabled whenever the expected', nan]\n",
      "['+   * value is greater than zero.', nan]\n",
      "['+   * @param statistic statistic to check.', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static ExpectedProbe probe(', nan]\n",
      "['+      final Statistic statistic,', nan]\n",
      "['+      final int expected) {', nan]\n",
      "['+    return probe(expected >= 0, statistic, expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a probe of a statistic which is conditionally enabled.', nan]\n",
      "['+   * @param enabled is the probe enabled?', nan]\n",
      "['+   * @param statistic statistic to check.', nan]\n",
      "['+   * @param expected expected value.', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static ExpectedProbe probe(', nan]\n",
      "['+      final boolean enabled,', nan]\n",
      "['+      final Statistic statistic,', nan]\n",
      "['+      final int expected) {', nan]\n",
      "['+    return enabled', nan]\n",
      "['+        ? new ExpectSingleStatistic(statistic, expected)', nan]\n",
      "['+        : EMPTY_PROBE;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create an aggregate probe from a vararges list of probes.', nan]\n",
      "['+   * @param enabled should the probes be enabled?', nan]\n",
      "['+   * @param plist probe list', nan]\n",
      "['+   * @return a probe', nan]\n",
      "['+   */', nan]\n",
      "['+  public static ExpectedProbe probes(', nan]\n",
      "['+      final boolean enabled,', nan]\n",
      "['+      final ExpectedProbe...plist) {', nan]\n",
      "['+    return enabled', nan]\n",
      "['+        ? new ProbeList(Arrays.asList(plist))', nan]\n",
      "['+        : EMPTY_PROBE;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Expect the exact head and list requests of the operation', nan]\n",
      "['+   * cost supplied.', nan]\n",
      "['+   * @param enabled is the probe enabled?', nan]\n",
      "['+   * @param cost expected cost.', nan]\n",
      "['+   * @return a probe.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static ExpectedProbe expect(', nan]\n",
      "['+      boolean enabled, OperationCost cost) {', nan]\n",
      "['+    return probes(enabled,', nan]\n",
      "['+        probe(OBJECT_METADATA_REQUESTS, cost.head()),', nan]\n",
      "['+        probe(OBJECT_LIST_REQUESTS, cost.list()));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * An expected probe to verify given criteria to trigger an eval.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Probes can be conditional, in which case they are only evaluated', nan]\n",
      "['+   * when true.', nan]\n",
      "['+   */', nan]\n",
      "['+  public interface ExpectedProbe {', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Verify a diff if the FS instance is compatible.', nan]\n",
      "['+     * @param message message to print; metric name is appended', nan]\n",
      "['+     */', nan]\n",
      "['+    void verify(OperationCostValidator diffs, String message);', nan]\n",
      "['+', nan]\n",
      "['+    boolean isEnabled();', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Simple probe is a single statistic.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static final class ExpectSingleStatistic implements ExpectedProbe {', nan]\n",
      "['+', nan]\n",
      "['+    private final Statistic statistic;', nan]\n",
      "['+', nan]\n",
      "['+    private final int expected;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Create.', nan]\n",
      "['+     * @param statistic statistic', nan]\n",
      "['+     * @param expected expected value.', nan]\n",
      "['+     */', nan]\n",
      "['+    private ExpectSingleStatistic(final Statistic statistic,', nan]\n",
      "['+        final int expected) {', nan]\n",
      "['+      this.statistic = statistic;', nan]\n",
      "['+      this.expected = expected;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Verify a diff if the FS instance is compatible.', nan]\n",
      "['+     * @param message message to print; metric name is appended', nan]\n",
      "['+     */', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public void verify(OperationCostValidator diffs, String message) {', nan]\n",
      "['+      diffs.get(statistic).assertDiffEquals(message, expected);', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    public Statistic getStatistic() {', nan]\n",
      "['+      return statistic;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    public int getExpected() {', nan]\n",
      "['+      return expected;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public boolean isEnabled() {', nan]\n",
      "['+      return true;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public String toString() {', nan]\n",
      "['+      String sb = \"ExpectSingleStatistic{\"', nan]\n",
      "['+          + statistic', nan]\n",
      "['+          + \", expected=\" + expected', nan]\n",
      "['+          + \", enabled=\" + isEnabled()', nan]\n",
      "[\"+          + '}';\", nan]\n",
      "['+      return sb;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * A list of probes; the verify operation', nan]\n",
      "['+   * verifies them all.', nan]\n",
      "['+   */', nan]\n",
      "['+  public static class ProbeList implements ExpectedProbe {', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Probe list.', nan]\n",
      "['+     */', nan]\n",
      "['+    private final List<ExpectedProbe> probes;', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Constructor.', nan]\n",
      "['+     * @param probes probe list.', nan]\n",
      "['+     */', nan]\n",
      "['+    public ProbeList(final List<ExpectedProbe> probes) {', nan]\n",
      "['+      this.probes = probes;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public void verify(final OperationCostValidator diffs,', nan]\n",
      "['+        final String message) {', nan]\n",
      "['+      probes.forEach(p -> p.verify(diffs, message));', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Enabled if 1+ probe is enabled.', nan]\n",
      "['+     * @return true if enabled.', nan]\n",
      "['+     */', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public boolean isEnabled() {', nan]\n",
      "['+      boolean enabled = false;', nan]\n",
      "['+      for (ExpectedProbe probe : probes) {', nan]\n",
      "['+        enabled |= probe.isEnabled();', nan]\n",
      "['+      }', nan]\n",
      "['+      return enabled;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public String toString() {', nan]\n",
      "['+      String pstr = probes.stream()', nan]\n",
      "['+          .map(Object::toString)', nan]\n",
      "['+          .collect(Collectors.joining(\", \"));', nan]\n",
      "['+      return \"ProbeList{\" + pstr + \\'}\\';', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * The empty probe always runs; it can be used to force', nan]\n",
      "['+   * a verification to execute.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final class EmptyProbe implements ExpectedProbe {', nan]\n",
      "['+', nan]\n",
      "['+    private final String name;', nan]\n",
      "['+', nan]\n",
      "['+    private final boolean enabled;', nan]\n",
      "['+', nan]\n",
      "['+    private EmptyProbe(final String name, boolean enabled) {', nan]\n",
      "['+      this.name = name;', nan]\n",
      "['+      this.enabled = enabled;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public void verify(final OperationCostValidator diffs,', nan]\n",
      "['+        final String message) {', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public boolean isEnabled() {', nan]\n",
      "['+      return enabled;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    @Override', nan]\n",
      "['+    public String toString() {', nan]\n",
      "['+      return name;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/AbstractS3GuardToolTest', 'ase.java b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/AbstractS3Guar']\n",
      "['index 3e187a15156..64057d02f82 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/AbstractS3GuardToolTestBase.ja', 'a']\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/AbstractS3GuardToolTestBase.ja', 'a']\n",
      "['@@ -60,10 +60,14 @@', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.S3GUARD_METASTORE_NULL;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.S3_METADATA_STORE_IMPL;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3AUtils.clearBucketOption;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.BucketInfo.IS_MARKER_AWARE;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.E_BAD_STATE;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.INVALID_ARGUMENT;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.SUCCESS;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.s3guard.S3GuardToolTestHelper.exec;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardToolTestHelper.runS3GuardCommand;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.MARKERS;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_NOT_ACCEPTABLE;', nan]\n",
      "['import static org.apache.hadoop.test.LambdaTestUtils.intercept;', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -124,7 +128,7 @@ protected static void expectResult(int expected,', nan]\n",
      "['public static String expectSuccess(', nan]\n",
      "['String message,', nan]\n",
      "['S3GuardTool tool,', nan]\n",
      "['-      String... args) throws Exception {', nan]\n",
      "['+      Object... args) throws Exception {', nan]\n",
      "['ByteArrayOutputStream buf = new ByteArrayOutputStream();', nan]\n",
      "['exec(SUCCESS, message, tool, buf, args);', nan]\n",
      "['return buf.toString();', nan]\n",
      "['@@ -137,9 +141,9 @@ public static String expectSuccess(', nan]\n",
      "['* @return the return code', nan]\n",
      "['* @throws Exception any exception', nan]\n",
      "['*/', nan]\n",
      "['-  protected int run(Configuration conf, String... args)', nan]\n",
      "['+  protected int run(Configuration conf, Object... args)', nan]\n",
      "['throws Exception {', nan]\n",
      "['-    return S3GuardTool.run(conf, args);', nan]\n",
      "['+    return runS3GuardCommand(conf, args);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -149,8 +153,8 @@ protected int run(Configuration conf, String... args)', nan]\n",
      "['* @return the return code', nan]\n",
      "['* @throws Exception any exception', nan]\n",
      "['*/', nan]\n",
      "['-  protected int run(String... args) throws Exception {', nan]\n",
      "['-    return S3GuardTool.run(getConfiguration(), args);', nan]\n",
      "['+  protected int run(Object... args) throws Exception {', nan]\n",
      "['+    return runS3GuardCommand(getConfiguration(), args);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -160,11 +164,12 @@ protected int run(String... args) throws Exception {', nan]\n",
      "['* @param args argument list', nan]\n",
      "['* @throws Exception any exception', nan]\n",
      "['*/', nan]\n",
      "['-  protected void runToFailure(int status, String... args)', nan]\n",
      "['+  protected void runToFailure(int status, Object... args)', nan]\n",
      "['throws Exception {', nan]\n",
      "['+    final Configuration conf = getConfiguration();', nan]\n",
      "['ExitUtil.ExitException ex =', nan]\n",
      "['-        intercept(ExitUtil.ExitException.class,', nan]\n",
      "['-            () -> run(args));', nan]\n",
      "['+        intercept(ExitUtil.ExitException.class, () ->', nan]\n",
      "['+            runS3GuardCommand(conf, args));', nan]\n",
      "['if (ex.status != status) {', nan]\n",
      "['throw ex;', nan]\n",
      "['}', nan]\n",
      "['@@ -445,6 +450,44 @@ public void testBucketInfoUnguarded() throws Exception {', nan]\n",
      "['info.contains(\"S3A Client\"));', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Verify that the {@code -markers aware} option works.', nan]\n",
      "['+   * This test case is in this class for ease of backporting.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testBucketInfoMarkerAware() throws Throwable {', nan]\n",
      "['+    final Configuration conf = getConfiguration();', nan]\n",
      "['+    URI fsUri = getFileSystem().getUri();', nan]\n",
      "['+', nan]\n",
      "['+    // run a bucket info command and look for', nan]\n",
      "['+    // confirmation that it got the output from DDB diags', nan]\n",
      "['+    S3GuardTool.BucketInfo infocmd = toClose(new S3GuardTool.BucketInfo(conf));', nan]\n",
      "['+    String info = exec(infocmd, S3GuardTool.BucketInfo.NAME,', nan]\n",
      "['+        \"-\" + MARKERS, S3GuardTool.BucketInfo.MARKERS_AWARE,', nan]\n",
      "['+        fsUri.toString());', nan]\n",
      "['+', nan]\n",
      "['+    assertTrue(\"Output should contain information about S3A client \" + info,', nan]\n",
      "['+        info.contains(IS_MARKER_AWARE));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Verify that the {@code -markers} option fails on unknown options.', nan]\n",
      "['+   * This test case is in this class for ease of backporting.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testBucketInfoMarkerPolicyUnknown() throws Throwable {', nan]\n",
      "['+    final Configuration conf = getConfiguration();', nan]\n",
      "['+    URI fsUri = getFileSystem().getUri();', nan]\n",
      "['+', nan]\n",
      "['+    // run a bucket info command and look for', nan]\n",
      "['+    // confirmation that it got the output from DDB diags', nan]\n",
      "['+    S3GuardTool.BucketInfo infocmd = toClose(new S3GuardTool.BucketInfo(conf));', nan]\n",
      "['+    intercept(ExitUtil.ExitException.class, \"\"+ EXIT_NOT_ACCEPTABLE, () ->', nan]\n",
      "['+        exec(infocmd, S3GuardTool.BucketInfo.NAME,', nan]\n",
      "['+            \"-\" + MARKERS, \"unknown\",', nan]\n",
      "['+            fsUri.toString()));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['@Test', nan]\n",
      "['public void testSetCapacityFailFastIfNotGuarded() throws Exception{', nan]\n",
      "['Configuration conf = getConfiguration();', nan]\n",
      "['@@ -654,4 +697,5 @@ public void testInitFailsIfNoBucketNameOrDDBTableSet() throws Exception {', nan]\n",
      "['assertEquals(\"Mismatched s3 outputs: \" + actualOut, filesOnS3, actualOnS3);', nan]\n",
      "['assertFalse(\"Diff contained duplicates\", duplicates);', nan]\n",
      "['}', nan]\n",
      "['+', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardDDBRootOper', 'tions.java b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3Guard']\n",
      "['index b2e6b3e93a8..afb0fd8c55a 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardDDBRootOperations.', 'ava']\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardDDBRootOperations.', 'ava']\n",
      "['@@ -38,9 +38,12 @@', nan]\n",
      "['import org.apache.hadoop.fs.s3a.impl.StoreContext;', nan]\n",
      "[nan, nan]\n",
      "['import static com.google.common.base.Preconditions.checkNotNull;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.ENABLE_MULTI_DELETE;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.Constants.S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3ATestUtils.assume;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.disableFilesystemCaching;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestBucketName;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3ATestUtils.removeBucketOverrides;', nan]\n",
      "['import static org.apache.hadoop.fs.s3a.S3AUtils.applyLocatedFiles;', nan]\n",
      "['@@ -52,6 +55,8 @@', nan]\n",
      "['* integration tests.', nan]\n",
      "['* <p>', nan]\n",
      "['* The tests only run if DynamoDB is the metastore.', nan]\n",
      "['+ * <p></p>', nan]\n",
      "['+ * The marker policy is fixed to \"delete\"', nan]\n",
      "['*/', nan]\n",
      "['@FixMethodOrder(MethodSorters.NAME_ASCENDING)', nan]\n",
      "['public class ITestS3GuardDDBRootOperations extends AbstractS3ATestBase {', nan]\n",
      "['@@ -82,9 +87,15 @@ protected int getTestTimeoutMillis() {', nan]\n",
      "['protected Configuration createConfiguration() {', nan]\n",
      "['Configuration conf = super.createConfiguration();', nan]\n",
      "['String bucketName = getTestBucketName(conf);', nan]\n",
      "['+    disableFilesystemCaching(conf);', nan]\n",
      "[nan, nan]\n",
      "['+    removeBucketOverrides(bucketName, conf,', nan]\n",
      "['+        S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,', nan]\n",
      "['+        ENABLE_MULTI_DELETE,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY);', nan]\n",
      "['+    conf.set(DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY_DELETE);', nan]\n",
      "['// set a sleep time of 0 on pruning, for speedier test runs.', nan]\n",
      "['-    removeBucketOverrides(bucketName, conf, ENABLE_MULTI_DELETE);', nan]\n",
      "['conf.setTimeDuration(', nan]\n",
      "['S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,', nan]\n",
      "['0,', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardToolTestHelper.j', 'va b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardToolTestHelpe']\n",
      "['index 4a5e55eb61e..89b4051de87 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardToolTestHelper.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardToolTestHelper.java', nan]\n",
      "['@@ -20,12 +20,16 @@', nan]\n",
      "[nan, nan]\n",
      "['import java.io.ByteArrayOutputStream;', nan]\n",
      "['import java.io.PrintStream;', nan]\n",
      "['+import java.util.Arrays;', nan]\n",
      "[nan, nan]\n",
      "['import org.slf4j.Logger;', nan]\n",
      "['import org.slf4j.LoggerFactory;', nan]\n",
      "[nan, nan]\n",
      "['+import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['import org.apache.hadoop.util.ExitCodeProvider;', nan]\n",
      "['+import org.apache.hadoop.util.ExitUtil;', nan]\n",
      "[nan, nan]\n",
      "['+import static org.apache.hadoop.test.LambdaTestUtils.intercept;', nan]\n",
      "['import static org.junit.Assert.assertEquals;', nan]\n",
      "[nan, nan]\n",
      "['/**', nan]\n",
      "['@@ -48,7 +52,7 @@ private S3GuardToolTestHelper() {', nan]\n",
      "['* @param args argument list', nan]\n",
      "['* @throws Exception on any failure', nan]\n",
      "['*/', nan]\n",
      "['-  public static String exec(S3GuardTool cmd, String... args) throws Exception {', nan]\n",
      "['+  public static String exec(S3GuardTool cmd, Object... args) throws Exception {', nan]\n",
      "['return expectExecResult(0, cmd, args);', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['@@ -64,7 +68,7 @@ public static String exec(S3GuardTool cmd, String... args) throws Exception {', nan]\n",
      "['public static String expectExecResult(', nan]\n",
      "['final int expectedResult,', nan]\n",
      "['final S3GuardTool cmd,', nan]\n",
      "['-      final String... args) throws Exception {', nan]\n",
      "['+      final Object... args) throws Exception {', nan]\n",
      "['ByteArrayOutputStream buf = new ByteArrayOutputStream();', nan]\n",
      "['try {', nan]\n",
      "['exec(expectedResult, \"\", cmd, buf, args);', nan]\n",
      "['@@ -77,6 +81,17 @@ public static String expectExecResult(', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Given an array of objects, conver to an array of strings.', nan]\n",
      "['+   * @param oargs object args', nan]\n",
      "['+   * @return string equivalent', nan]\n",
      "['+   */', nan]\n",
      "['+  public static String[] varargsToString(final Object[] oargs) {', nan]\n",
      "['+    return Arrays.stream(oargs)', nan]\n",
      "['+        .map(Object::toString)', nan]\n",
      "['+        .toArray(String[]::new);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['/**', nan]\n",
      "['* Execute a command, saving the output into the buffer.', nan]\n",
      "['* @param expectedResult expected result of the command.', nan]\n",
      "['@@ -91,8 +106,9 @@ public static void exec(final int expectedResult,', nan]\n",
      "['final String errorText,', nan]\n",
      "['final S3GuardTool cmd,', nan]\n",
      "['final ByteArrayOutputStream buf,', nan]\n",
      "['-      final String... args)', nan]\n",
      "['+      final Object... oargs)', nan]\n",
      "['throws Exception {', nan]\n",
      "['+    final String[] args = varargsToString(oargs);', nan]\n",
      "['LOG.info(\"exec {}\", (Object) args);', nan]\n",
      "['int r;', nan]\n",
      "['try (PrintStream out = new PrintStream(buf)) {', nan]\n",
      "['@@ -116,4 +132,43 @@ public static void exec(final int expectedResult,', nan]\n",
      "['}', nan]\n",
      "['}', nan]\n",
      "[nan, nan]\n",
      "['+  /**', nan]\n",
      "['+   * Run a S3GuardTool command from a varags list.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * Warning: if the filesystem is retrieved from the cache,', nan]\n",
      "['+   * it will be closed afterwards.', nan]\n",
      "['+   * @param conf configuration', nan]\n",
      "['+   * @param args argument list', nan]\n",
      "['+   * @return the return code', nan]\n",
      "['+   * @throws Exception any exception', nan]\n",
      "['+   */', nan]\n",
      "['+  public static int runS3GuardCommand(Configuration conf, Object... args)', nan]\n",
      "['+      throws Exception {', nan]\n",
      "['+    return S3GuardTool.run(conf, varargsToString(args));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Run a S3GuardTool command from a varags list, catch any raised', nan]\n",
      "['+   * ExitException and verify the status code matches that expected.', nan]\n",
      "['+   * @param conf configuration', nan]\n",
      "['+   * @param status expected status code of the exception', nan]\n",
      "['+   * @param args argument list', nan]\n",
      "['+   * @throws Exception any exception', nan]\n",
      "['+   */', nan]\n",
      "['+  public static void runS3GuardCommandToFailure(Configuration conf,', nan]\n",
      "['+      int status,', nan]\n",
      "['+      Object... args) throws Exception {', nan]\n",
      "['+', nan]\n",
      "['+    ExitUtil.ExitException ex =', nan]\n",
      "['+        intercept(ExitUtil.ExitException.class,', nan]\n",
      "['+            () -> {', nan]\n",
      "['+              int ec = runS3GuardCommand(conf, args);', nan]\n",
      "['+              if (ec != 0) {', nan]\n",
      "['+                throw new ExitUtil.ExitException(ec, \"exit code \" + ec);', nan]\n",
      "['+              }', nan]\n",
      "['+            });', nan]\n",
      "['+    if (ex.status != status) {', nan]\n",
      "['+      throw ex;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDirListingMetadata.', 'ava b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDirListingMetad']\n",
      "['index 1ce3ee56ce0..5f7a6fbd072 100644', nan]\n",
      "['--- a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDirListingMetadata.java', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDirListingMetadata.java', nan]\n",
      "['@@ -316,18 +316,23 @@ public void testRemoveExpiredEntriesFromListing() {', nan]\n",
      "['List<PathMetadata> listing = Arrays.asList(pathMeta1, pathMeta2, pathMeta3);', nan]\n",
      "['DirListingMetadata meta = new DirListingMetadata(path, listing, false);', nan]\n",
      "[nan, nan]\n",
      "['-    meta.removeExpiredEntriesFromListing(ttl, now);', nan]\n",
      "['+    List<PathMetadata> expired = meta.removeExpiredEntriesFromListing(ttl,', nan]\n",
      "['+        now);', nan]\n",
      "[nan, nan]\n",
      "['Assertions.assertThat(meta.getListing())', nan]\n",
      "['.describedAs(\"Metadata listing for %s\", path)', nan]\n",
      "['.doesNotContain(pathMeta1)', nan]\n",
      "['.contains(pathMeta2)', nan]\n",
      "['.contains(pathMeta3);', nan]\n",
      "['+    Assertions.assertThat(expired)', nan]\n",
      "['+        .describedAs(\"Expire entries underr %s\", path)', nan]\n",
      "['+        .doesNotContain(pathMeta2)', nan]\n",
      "['+        .contains(pathMeta1);', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['}', nan]\n",
      "[nan, nan]\n",
      "['-  /*', nan]\n",
      "['+  /**', nan]\n",
      "['* Create DirListingMetadata with two dirs and one file living in directory', nan]\n",
      "[\"-   * 'parent'\", nan]\n",
      "[\"+   * 'parent'.\", nan]\n",
      "['*/', nan]\n",
      "['private static DirListingMetadata makeTwoDirsOneFile(Path parent) {', nan]\n",
      "['PathMetadata pathMeta1 = new PathMetadata(', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/AbstractMarkerToolTest.ja', 'a b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/AbstractMarkerToolTest.']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..00e62d94910', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/AbstractMarkerToolTest.java', nan]\n",
      "['@@ -0,0 +1,334 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.tools;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.File;', nan]\n",
      "['+import java.io.FileReader;', nan]\n",
      "['+import java.io.IOException;', nan]\n",
      "['+import java.net.URI;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+', nan]\n",
      "['+import org.assertj.core.api.Assertions;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.conf.Configuration;', nan]\n",
      "['+import org.apache.hadoop.fs.FileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.AbstractS3ATestBase;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+import org.apache.hadoop.io.IOUtils;', nan]\n",
      "['+import org.apache.hadoop.util.StringUtils;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.*;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.disableFilesystemCaching;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestBucketName;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.removeBaseAndBucketOverrides;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.S3ATestUtils.removeBucketOverrides;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.VERBOSE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardToolTestHelper.runS3GuardCommand;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardToolTestHelper.runS3GuardCommandToFailure;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.UNLIMITED_LISTING;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Class for marker tool tests -sets up keeping/deleting filesystems,', nan]\n",
      "['+ * has methods to invoke.', nan]\n",
      "['+ */', nan]\n",
      "['+public class AbstractMarkerToolTest extends AbstractS3ATestBase {', nan]\n",
      "['+', nan]\n",
      "['+  private static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(AbstractMarkerToolTest.class);', nan]\n",
      "['+', nan]\n",
      "['+  /** the -verbose option. */', nan]\n",
      "['+  protected static final String V = AbstractMarkerToolTest.m(VERBOSE);', nan]\n",
      "['+', nan]\n",
      "['+  /** FS which keeps markers. */', nan]\n",
      "['+  private S3AFileSystem keepingFS;', nan]\n",
      "['+', nan]\n",
      "['+  /** FS which deletes markers. */', nan]\n",
      "['+  private S3AFileSystem deletingFS;', nan]\n",
      "['+', nan]\n",
      "['+  /** FS which mixes markers; only created in some tests. */', nan]\n",
      "['+  private S3AFileSystem mixedFS;', nan]\n",
      "['+  @Override', nan]\n",
      "['+  protected Configuration createConfiguration() {', nan]\n",
      "['+    Configuration conf = super.createConfiguration();', nan]\n",
      "['+    String bucketName = getTestBucketName(conf);', nan]\n",
      "['+    removeBaseAndBucketOverrides(bucketName, conf,', nan]\n",
      "['+        S3A_BUCKET_PROBE,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        S3_METADATA_STORE_IMPL,', nan]\n",
      "['+        METADATASTORE_AUTHORITATIVE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['+    // base FS is legacy', nan]\n",
      "['+    conf.set(DIRECTORY_MARKER_POLICY, DIRECTORY_MARKER_POLICY_DELETE);', nan]\n",
      "['+    conf.set(S3_METADATA_STORE_IMPL, S3GUARD_METASTORE_NULL);', nan]\n",
      "['+', nan]\n",
      "['+    // turn off bucket probes for a bit of speedup in the connectors we create.', nan]\n",
      "['+    conf.setInt(S3A_BUCKET_PROBE, 0);', nan]\n",
      "['+    return conf;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void setup() throws Exception {', nan]\n",
      "['+    super.setup();', nan]\n",
      "['+    setKeepingFS(createFS(DIRECTORY_MARKER_POLICY_KEEP, null));', nan]\n",
      "['+    setDeletingFS(createFS(DIRECTORY_MARKER_POLICY_DELETE, null));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void teardown() throws Exception {', nan]\n",
      "['+    // do this ourselves to avoid audits teardown failing', nan]\n",
      "['+    // when surplus markers are found', nan]\n",
      "['+    deleteTestDirInTeardown();', nan]\n",
      "['+    super.teardown();', nan]\n",
      "['+    IOUtils.cleanupWithLogger(LOG, getKeepingFS(),', nan]\n",
      "['+        getMixedFS(), getDeletingFS());', nan]\n",
      "['+', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * FS which deletes markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  public S3AFileSystem getDeletingFS() {', nan]\n",
      "['+    return deletingFS;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  public void setDeletingFS(final S3AFileSystem deletingFS) {', nan]\n",
      "['+    this.deletingFS = deletingFS;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * FS which keeps markers.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected S3AFileSystem getKeepingFS() {', nan]\n",
      "['+    return keepingFS;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  private void setKeepingFS(S3AFileSystem keepingFS) {', nan]\n",
      "['+    this.keepingFS = keepingFS;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /** only created on demand. */', nan]\n",
      "['+  private S3AFileSystem getMixedFS() {', nan]\n",
      "['+    return mixedFS;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  protected void setMixedFS(S3AFileSystem mixedFS) {', nan]\n",
      "['+    this.mixedFS = mixedFS;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Get a filename for a temp file.', nan]\n",
      "['+   * The generated file is deleted.', nan]\n",
      "['+   *', nan]\n",
      "['+   * @return a file path for a output file', nan]\n",
      "['+   */', nan]\n",
      "['+  protected File tempAuditFile() throws IOException {', nan]\n",
      "['+    final File audit = File.createTempFile(\"audit\", \".txt\");', nan]\n",
      "['+    audit.delete();', nan]\n",
      "['+    return audit;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Read the audit output and verify it has the expected number of lines.', nan]\n",
      "['+   * @param auditFile audit file to read', nan]\n",
      "['+   * @param expected expected line count', nan]\n",
      "['+   */', nan]\n",
      "['+  protected void expectMarkersInOutput(final File auditFile,', nan]\n",
      "['+      final int expected)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    final List<String> lines = readOutput(auditFile);', nan]\n",
      "['+    Assertions.assertThat(lines)', nan]\n",
      "['+        .describedAs(\"Content of %s\", auditFile)', nan]\n",
      "['+        .hasSize(expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Read the output file in. Logs the contents at info.', nan]\n",
      "['+   * @param outputFile audit output file.', nan]\n",
      "['+   * @return the lines', nan]\n",
      "['+   */', nan]\n",
      "['+  protected List<String> readOutput(final File outputFile)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    try (FileReader reader = new FileReader(outputFile)) {', nan]\n",
      "['+      final List<String> lines =', nan]\n",
      "['+          org.apache.commons.io.IOUtils.readLines(reader);', nan]\n",
      "['+', nan]\n",
      "['+      LOG.info(\"contents of output file {}\\\\n{}\", outputFile,', nan]\n",
      "['+          StringUtils.join(\"\\\\n\", lines));', nan]\n",
      "['+      return lines;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a new FS with given marker policy and path.', nan]\n",
      "['+   * This filesystem MUST be closed in test teardown.', nan]\n",
      "['+   * @param markerPolicy markers', nan]\n",
      "['+   * @param authPath authoritative path. If null: no path.', nan]\n",
      "['+   * @return a new FS.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected S3AFileSystem createFS(String markerPolicy,', nan]\n",
      "['+      String authPath) throws Exception {', nan]\n",
      "['+    S3AFileSystem testFS = getFileSystem();', nan]\n",
      "['+    Configuration conf = new Configuration(testFS.getConf());', nan]\n",
      "['+    URI testFSUri = testFS.getUri();', nan]\n",
      "['+    String bucketName = getTestBucketName(conf);', nan]\n",
      "['+    removeBucketOverrides(bucketName, conf,', nan]\n",
      "['+        DIRECTORY_MARKER_POLICY,', nan]\n",
      "['+        S3_METADATA_STORE_IMPL,', nan]\n",
      "['+        BULK_DELETE_PAGE_SIZE,', nan]\n",
      "['+        AUTHORITATIVE_PATH);', nan]\n",
      "['+    if (authPath != null) {', nan]\n",
      "['+      conf.set(AUTHORITATIVE_PATH, authPath);', nan]\n",
      "['+    }', nan]\n",
      "['+    // Use a very small page size to force the paging', nan]\n",
      "['+    // code to be tested.', nan]\n",
      "['+    conf.setInt(BULK_DELETE_PAGE_SIZE, 2);', nan]\n",
      "['+    conf.set(S3_METADATA_STORE_IMPL, S3GUARD_METASTORE_NULL);', nan]\n",
      "['+    conf.set(DIRECTORY_MARKER_POLICY, markerPolicy);', nan]\n",
      "['+    S3AFileSystem fs2 = new S3AFileSystem();', nan]\n",
      "['+    fs2.initialize(testFSUri, conf);', nan]\n",
      "['+    LOG.info(\"created new filesystem with policy {} and auth path {}\",', nan]\n",
      "['+        markerPolicy,', nan]\n",
      "['+        (authPath == null ? \"(null)\": authPath));', nan]\n",
      "['+    return fs2;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute the marker tool, expecting the execution to succeed.', nan]\n",
      "['+   * @param sourceFS filesystem to use', nan]\n",
      "['+   * @param path path to scan', nan]\n",
      "['+   * @param doPurge should markers be purged', nan]\n",
      "['+   * @param expectedMarkerCount number of markers expected', nan]\n",
      "['+   * @return the result', nan]\n",
      "['+   */', nan]\n",
      "['+  protected MarkerTool.ScanResult markerTool(', nan]\n",
      "['+      final FileSystem sourceFS,', nan]\n",
      "['+      final Path path,', nan]\n",
      "['+      final boolean doPurge,', nan]\n",
      "['+      final int expectedMarkerCount)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    return markerTool(0, sourceFS, path, doPurge,', nan]\n",
      "['+        expectedMarkerCount,', nan]\n",
      "['+        UNLIMITED_LISTING, false);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Run a S3GuardTool command from a varags list and the', nan]\n",
      "['+   * configuration returned by {@code getConfiguration()}.', nan]\n",
      "['+   * @param args argument list', nan]\n",
      "['+   * @return the return code', nan]\n",
      "['+   * @throws Exception any exception', nan]\n",
      "['+   */', nan]\n",
      "['+  protected int run(Object... args) throws Exception {', nan]\n",
      "['+    return runS3GuardCommand(uncachedFSConfig(getConfiguration()), args);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Take a configuration, copy it and disable FS Caching on', nan]\n",
      "['+   * the new one.', nan]\n",
      "['+   * @param conf source config', nan]\n",
      "['+   * @return a new, patched, config', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Configuration uncachedFSConfig(final Configuration conf) {', nan]\n",
      "['+    Configuration c = new Configuration(conf);', nan]\n",
      "['+    disableFilesystemCaching(c);', nan]\n",
      "['+    return c;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * given an FS instance, create a matching configuration where caching', nan]\n",
      "['+   * is disabled.', nan]\n",
      "['+   * @param fs source', nan]\n",
      "['+   * @return new config.', nan]\n",
      "['+   */', nan]\n",
      "['+  protected Configuration uncachedFSConfig(final FileSystem fs) {', nan]\n",
      "['+    return uncachedFSConfig(fs.getConf());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Run a S3GuardTool command from a varags list, catch any raised', nan]\n",
      "['+     * ExitException and verify the status code matches that expected.', nan]\n",
      "['+     * @param status expected status code of the exception', nan]\n",
      "['+     * @param args argument list', nan]\n",
      "['+     * @throws Exception any exception', nan]\n",
      "['+     */', nan]\n",
      "['+  protected void runToFailure(int status, Object... args)', nan]\n",
      "['+      throws Exception {', nan]\n",
      "['+    Configuration conf = uncachedFSConfig(getConfiguration());', nan]\n",
      "['+    runS3GuardCommandToFailure(conf, status, args);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Given a base and a filename, create a new path.', nan]\n",
      "['+   * @param base base path', nan]\n",
      "['+   * @param name name: may be empty, in which case the base path is returned', nan]\n",
      "['+   * @return a path', nan]\n",
      "['+   */', nan]\n",
      "['+  protected static Path toPath(final Path base, final String name) {', nan]\n",
      "['+    return name.isEmpty() ? base : new Path(base, name);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Execute the marker tool, expecting the execution to', nan]\n",
      "['+   * return a specific exit code.', nan]\n",
      "['+   *', nan]\n",
      "['+   * @param sourceFS filesystem to use', nan]\n",
      "['+   * @param exitCode exit code to expect.', nan]\n",
      "['+   * @param path path to scan', nan]\n",
      "['+   * @param doPurge should markers be purged', nan]\n",
      "['+   * @param expectedMarkers number of markers expected', nan]\n",
      "[\"+   * @param limit limit of files to scan; -1 for 'unlimited'\", nan]\n",
      "['+   * @param nonAuth only use nonauth path count for failure rules', nan]\n",
      "['+   * @return the result', nan]\n",
      "['+   */', nan]\n",
      "['+  public static MarkerTool.ScanResult markerTool(', nan]\n",
      "['+      final int exitCode,', nan]\n",
      "['+      final FileSystem sourceFS,', nan]\n",
      "['+      final Path path,', nan]\n",
      "['+      final boolean doPurge,', nan]\n",
      "['+      final int expectedMarkers,', nan]\n",
      "['+      final int limit,', nan]\n",
      "['+      final boolean nonAuth) throws IOException {', nan]\n",
      "['+', nan]\n",
      "['+    MarkerTool.ScanResult result = MarkerTool.execMarkerTool(', nan]\n",
      "['+        sourceFS,', nan]\n",
      "['+        path,', nan]\n",
      "['+        doPurge,', nan]\n",
      "['+        expectedMarkers,', nan]\n",
      "['+        limit, nonAuth);', nan]\n",
      "['+    Assertions.assertThat(result.getExitCode())', nan]\n",
      "['+        .describedAs(\"Exit code of marker(%s, %s, %d) -> %s\",', nan]\n",
      "['+            path, doPurge, expectedMarkers, result)', nan]\n",
      "['+        .isEqualTo(exitCode);', nan]\n",
      "['+    return result;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Add a \"-\" prefix to a string.', nan]\n",
      "['+   * @param s string to prefix', nan]\n",
      "['+   * @return a string for passing into the CLI', nan]\n",
      "['+   */', nan]\n",
      "['+  protected static String m(String s) {', nan]\n",
      "['+    return \"-\" + s;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/ITestMarkerTool.java b/ha', 'oop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/ITestMarkerTool.java']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..4a81b1aba91', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/ITestMarkerTool.java', nan]\n",
      "['@@ -0,0 +1,533 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.tools;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.File;', nan]\n",
      "['+import java.io.IOException;', nan]\n",
      "['+import java.util.ArrayList;', nan]\n",
      "['+import java.util.List;', nan]\n",
      "['+', nan]\n",
      "['+import org.assertj.core.api.Assertions;', nan]\n",
      "['+import org.junit.Test;', nan]\n",
      "['+import org.slf4j.Logger;', nan]\n",
      "['+import org.slf4j.LoggerFactory;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.FileSystem;', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+import org.apache.hadoop.fs.contract.ContractTestUtils;', nan]\n",
      "['+import org.apache.hadoop.fs.s3a.S3AFileSystem;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_AUTHORITATIVE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_DELETE;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.Constants.DIRECTORY_MARKER_POLICY_KEEP;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.BucketInfo.BUCKET_INFO;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardToolTestHelper.runS3GuardCommand;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.s3guard.S3GuardToolTestHelper.runS3GuardCommandToFailure;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.*;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_INTERRUPTED;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_NOT_ACCEPTABLE;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_NOT_FOUND;', nan]\n",
      "['+import static org.apache.hadoop.service.launcher.LauncherExitCodes.EXIT_USAGE;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Test the marker tool and use it to compare the behavior', nan]\n",
      "['+ * of keeping vs legacy S3A FS instances.', nan]\n",
      "['+ */', nan]\n",
      "['+public class ITestMarkerTool extends AbstractMarkerToolTest {', nan]\n",
      "['+', nan]\n",
      "['+  protected static final Logger LOG =', nan]\n",
      "['+      LoggerFactory.getLogger(ITestMarkerTool.class);', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many files to expect.', nan]\n",
      "['+   */', nan]\n",
      "['+  private int expectedFileCount;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many markers to expect under dir1.', nan]\n",
      "['+   */', nan]\n",
      "['+  private int expectedMarkersUnderDir1;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many markers to expect under dir2.', nan]\n",
      "['+   */', nan]\n",
      "['+  private int expectedMarkersUnderDir2;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many markers to expect across both dirs?', nan]\n",
      "['+   */', nan]\n",
      "['+  private int expectedMarkers;', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * How many markers to expect including the base directory?', nan]\n",
      "['+   */', nan]\n",
      "['+  private int expectedMarkersWithBaseDir;', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCleanMarkersLegacyDir() throws Throwable {', nan]\n",
      "['+    describe(\"Clean markers under a deleting FS -expect none\");', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(getDeletingFS(), methodPath());', nan]\n",
      "['+    markerTool(getDeletingFS(), createdPaths.base, false, 0);', nan]\n",
      "['+    markerTool(getDeletingFS(), createdPaths.base, true, 0);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCleanMarkersFileLimit() throws Throwable {', nan]\n",
      "['+    describe(\"Clean markers under a keeping FS -with file limit\");', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());', nan]\n",
      "['+', nan]\n",
      "['+    // audit will be interrupted', nan]\n",
      "['+    markerTool(EXIT_INTERRUPTED, getDeletingFS(),', nan]\n",
      "['+        createdPaths.base, false, 0, 1, false);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testCleanMarkersKeepingDir() throws Throwable {', nan]\n",
      "['+    describe(\"Audit then clean markers under a deleting FS \"', nan]\n",
      "['+        + \"-expect markers to be found and then cleaned up\");', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());', nan]\n",
      "['+', nan]\n",
      "['+    // audit will find the expected entries', nan]\n",
      "['+    int expectedMarkerCount = createdPaths.dirs.size();', nan]\n",
      "['+    S3AFileSystem fs = getDeletingFS();', nan]\n",
      "['+    LOG.info(\"Auditing a directory with retained markers -expect failure\");', nan]\n",
      "['+    markerTool(EXIT_NOT_ACCEPTABLE, fs,', nan]\n",
      "['+        createdPaths.base, false, 0, UNLIMITED_LISTING, false);', nan]\n",
      "['+', nan]\n",
      "['+    LOG.info(\"Auditing a directory expecting retained markers\");', nan]\n",
      "['+    markerTool(fs, createdPaths.base, false,', nan]\n",
      "['+        expectedMarkerCount);', nan]\n",
      "['+', nan]\n",
      "[\"+    // we require that a purge didn't take place, so run the\", nan]\n",
      "['+    // audit again.', nan]\n",
      "['+    LOG.info(\"Auditing a directory expecting retained markers\");', nan]\n",
      "['+    markerTool(fs, createdPaths.base, false,', nan]\n",
      "['+        expectedMarkerCount);', nan]\n",
      "['+', nan]\n",
      "['+    LOG.info(\"Purging a directory of retained markers\");', nan]\n",
      "['+    // purge cleans up', nan]\n",
      "['+    assertMarkersDeleted(expectedMarkerCount,', nan]\n",
      "['+        markerTool(fs, createdPaths.base, true, expectedMarkerCount));', nan]\n",
      "[\"+    // and a rerun doesn't find markers\", nan]\n",
      "['+    LOG.info(\"Auditing a directory with retained markers -expect success\");', nan]\n",
      "['+    assertMarkersDeleted(0,', nan]\n",
      "['+        markerTool(fs, createdPaths.base, true, 0));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRenameKeepingFS() throws Throwable {', nan]\n",
      "['+    describe(\"Rename with the keeping FS -verify that no markers\"', nan]\n",
      "['+        + \" exist at far end\");', nan]\n",
      "['+    Path base = methodPath();', nan]\n",
      "['+    Path source = new Path(base, \"source\");', nan]\n",
      "['+    Path dest = new Path(base, \"dest\");', nan]\n",
      "['+', nan]\n",
      "['+    S3AFileSystem fs = getKeepingFS();', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(fs, source);', nan]\n",
      "['+', nan]\n",
      "['+    // audit will find three entries', nan]\n",
      "['+    int expectedMarkerCount = createdPaths.dirs.size();', nan]\n",
      "['+', nan]\n",
      "['+    markerTool(fs, source, false, expectedMarkerCount);', nan]\n",
      "['+    fs.rename(source, dest);', nan]\n",
      "['+    assertIsDirectory(dest);', nan]\n",
      "['+', nan]\n",
      "['+    // there are no markers', nan]\n",
      "['+    markerTool(fs, dest, false, 0);', nan]\n",
      "['+    LOG.info(\"Auditing destination paths\");', nan]\n",
      "['+    verifyRenamed(dest, createdPaths);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create a FS where only dir2 in the source tree keeps markers;', nan]\n",
      "['+   * verify all is good.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testAuthPathIsMixed() throws Throwable {', nan]\n",
      "['+    describe(\"Create a source tree with mixed semantics\");', nan]\n",
      "['+    Path base = methodPath();', nan]\n",
      "['+    Path source = new Path(base, \"source\");', nan]\n",
      "['+    Path dest = new Path(base, \"dest\");', nan]\n",
      "['+    Path dir2 = new Path(source, \"dir2\");', nan]\n",
      "['+    S3AFileSystem mixedFSDir2 = createFS(DIRECTORY_MARKER_POLICY_AUTHORITATIVE,', nan]\n",
      "['+        dir2.toUri().toString());', nan]\n",
      "['+    // line up for close in teardown', nan]\n",
      "['+    setMixedFS(mixedFSDir2);', nan]\n",
      "['+    // some of these paths will retain markers, some will not', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(mixedFSDir2, source);', nan]\n",
      "['+', nan]\n",
      "['+    // markers are only under dir2', nan]\n",
      "['+    markerTool(mixedFSDir2, toPath(source, \"dir1\"), false, 0);', nan]\n",
      "['+    markerTool(mixedFSDir2, source, false, expectedMarkersUnderDir2);', nan]\n",
      "['+', nan]\n",
      "['+    // full scan of source will fail', nan]\n",
      "['+    markerTool(EXIT_NOT_ACCEPTABLE,', nan]\n",
      "['+        mixedFSDir2, source, false, 0, 0, false);', nan]\n",
      "['+', nan]\n",
      "['+    // but add the -nonauth option and the markers under dir2 are skipped', nan]\n",
      "['+    markerTool(0, mixedFSDir2, source, false, 0, 0, true);', nan]\n",
      "['+', nan]\n",
      "['+    // if we now rename, all will be good', nan]\n",
      "['+    LOG.info(\"Executing rename\");', nan]\n",
      "['+    mixedFSDir2.rename(source, dest);', nan]\n",
      "['+    assertIsDirectory(dest);', nan]\n",
      "['+', nan]\n",
      "['+    // there are no markers', nan]\n",
      "['+    MarkerTool.ScanResult scanResult = markerTool(mixedFSDir2, dest, false, 0);', nan]\n",
      "['+    // there are exactly the files we want', nan]\n",
      "['+    Assertions.assertThat(scanResult)', nan]\n",
      "['+        .describedAs(\"Scan result %s\", scanResult)', nan]\n",
      "['+        .extracting(s -> s.getTracker().getFilesFound())', nan]\n",
      "['+        .isEqualTo(expectedFileCount);', nan]\n",
      "['+    verifyRenamed(dest, createdPaths);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Assert that an expected number of markers were deleted.', nan]\n",
      "['+   * @param expected expected count.', nan]\n",
      "['+   * @param result scan result', nan]\n",
      "['+   */', nan]\n",
      "['+  private static void assertMarkersDeleted(int expected,', nan]\n",
      "['+      MarkerTool.ScanResult result) {', nan]\n",
      "['+', nan]\n",
      "['+    Assertions.assertThat(result.getPurgeSummary())', nan]\n",
      "['+        .describedAs(\"Purge result of scan %s\", result)', nan]\n",
      "['+        .isNotNull()', nan]\n",
      "['+        .extracting(f -> f.getMarkersDeleted())', nan]\n",
      "['+        .isEqualTo(expected);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Marker tool with no args.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunNoArgs() throws Throwable {', nan]\n",
      "['+    runToFailure(EXIT_USAGE, MARKERS);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunWrongBucket() throws Throwable {', nan]\n",
      "['+    runToFailure(EXIT_NOT_FOUND, MARKERS,', nan]\n",
      "['+        AUDIT,', nan]\n",
      "['+        \"s3a://this-bucket-does-not-exist-hopefully\");', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "[\"+   * Run with a path that doesn't exist.\", nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunUnknownPath() throws Throwable {', nan]\n",
      "['+    runToFailure(EXIT_NOT_FOUND, MARKERS,', nan]\n",
      "['+        AUDIT,', nan]\n",
      "['+        methodPath());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Having both -audit and -clean on the command line is an error.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunTooManyActions() throws Throwable {', nan]\n",
      "['+    runToFailure(EXIT_USAGE, MARKERS,', nan]\n",
      "['+        AUDIT, CLEAN,', nan]\n",
      "['+        methodPath());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunAuditWithExpectedMarkers() throws Throwable {', nan]\n",
      "['+    describe(\"Run a verbose audit expecting some markers\");', nan]\n",
      "['+    // a run under the keeping FS will create paths', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());', nan]\n",
      "['+    final File audit = tempAuditFile();', nan]\n",
      "['+    run(MARKERS, V,', nan]\n",
      "['+        AUDIT,', nan]\n",
      "['+        m(OPT_LIMIT), 0,', nan]\n",
      "['+        m(OPT_OUT), audit,', nan]\n",
      "['+        m(OPT_EXPECTED), expectedMarkersWithBaseDir,', nan]\n",
      "['+        createdPaths.base);', nan]\n",
      "['+    expectMarkersInOutput(audit, expectedMarkersWithBaseDir);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunAuditWithExcessMarkers() throws Throwable {', nan]\n",
      "['+    describe(\"Run a verbose audit failing as surplus markers were found\");', nan]\n",
      "['+    // a run under the keeping FS will create paths', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());', nan]\n",
      "['+    final File audit = tempAuditFile();', nan]\n",
      "['+    runToFailure(EXIT_NOT_ACCEPTABLE, MARKERS, V,', nan]\n",
      "['+        AUDIT,', nan]\n",
      "['+        m(OPT_OUT), audit,', nan]\n",
      "['+        createdPaths.base);', nan]\n",
      "['+    expectMarkersInOutput(audit, expectedMarkersWithBaseDir);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunLimitedAudit() throws Throwable {', nan]\n",
      "['+    describe(\"Audit with a limited number of files (2)\");', nan]\n",
      "['+    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());', nan]\n",
      "['+    runToFailure(EXIT_INTERRUPTED,', nan]\n",
      "['+        MARKERS, V,', nan]\n",
      "['+        m(OPT_LIMIT), 2,', nan]\n",
      "['+        CLEAN,', nan]\n",
      "['+        createdPaths.base);', nan]\n",
      "['+    run(MARKERS, V,', nan]\n",
      "['+        AUDIT,', nan]\n",
      "['+        createdPaths.base);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Run an audit against the landsat bucket.', nan]\n",
      "['+   * <p></p>', nan]\n",
      "['+   * This tests paging/scale against a larger bucket without', nan]\n",
      "['+   * worrying about setup costs.', nan]\n",
      "['+   */', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testRunLimitedLandsatAudit() throws Throwable {', nan]\n",
      "['+    describe(\"Audit a few thousand landsat objects\");', nan]\n",
      "['+    final File audit = tempAuditFile();', nan]\n",
      "['+', nan]\n",
      "['+    run(MARKERS,', nan]\n",
      "['+        AUDIT,', nan]\n",
      "['+        m(OPT_LIMIT), 3000,', nan]\n",
      "['+        m(OPT_OUT), audit,', nan]\n",
      "['+        LANDSAT_BUCKET);', nan]\n",
      "['+    readOutput(audit);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testBucketInfoKeepingOnDeleting() throws Throwable {', nan]\n",
      "['+    describe(\"Run bucket info with the keeping config on the deleting fs\");', nan]\n",
      "['+    runS3GuardCommandToFailure(uncachedFSConfig(getDeletingFS()),', nan]\n",
      "['+        EXIT_NOT_ACCEPTABLE,', nan]\n",
      "['+        BUCKET_INFO,', nan]\n",
      "['+        m(MARKERS), DIRECTORY_MARKER_POLICY_KEEP,', nan]\n",
      "['+        methodPath());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testBucketInfoKeepingOnKeeping() throws Throwable {', nan]\n",
      "['+    describe(\"Run bucket info with the keeping config on the keeping fs\");', nan]\n",
      "['+    runS3GuardCommand(uncachedFSConfig(getKeepingFS()),', nan]\n",
      "['+        BUCKET_INFO,', nan]\n",
      "['+        m(MARKERS), DIRECTORY_MARKER_POLICY_KEEP,', nan]\n",
      "['+        methodPath());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testBucketInfoDeletingOnDeleting() throws Throwable {', nan]\n",
      "['+    describe(\"Run bucket info with the deleting config on the deleting fs\");', nan]\n",
      "['+    runS3GuardCommand(uncachedFSConfig(getDeletingFS()),', nan]\n",
      "['+        BUCKET_INFO,', nan]\n",
      "['+        m(MARKERS), DIRECTORY_MARKER_POLICY_DELETE,', nan]\n",
      "['+        methodPath());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void testBucketInfoAuthOnAuth() throws Throwable {', nan]\n",
      "['+    describe(\"Run bucket info with the auth FS\");', nan]\n",
      "['+    Path base = methodPath();', nan]\n",
      "['+', nan]\n",
      "['+    S3AFileSystem authFS = createFS(DIRECTORY_MARKER_POLICY_AUTHORITATIVE,', nan]\n",
      "['+        base.toUri().toString());', nan]\n",
      "['+    // line up for close in teardown', nan]\n",
      "['+    setMixedFS(authFS);', nan]\n",
      "['+    runS3GuardCommand(uncachedFSConfig(authFS),', nan]\n",
      "['+        BUCKET_INFO,', nan]\n",
      "['+        m(MARKERS), DIRECTORY_MARKER_POLICY_AUTHORITATIVE,', nan]\n",
      "['+        methodPath());', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Tracker of created paths.', nan]\n",
      "['+   */', nan]\n",
      "['+  private static final class CreatedPaths {', nan]\n",
      "['+', nan]\n",
      "['+    private final FileSystem fs;', nan]\n",
      "['+', nan]\n",
      "['+    private final Path base;', nan]\n",
      "['+', nan]\n",
      "['+    private List<Path> files = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+    private List<Path> dirs = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+    private List<Path> emptyDirs = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+    private List<String> filesUnderBase = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+    private List<String> dirsUnderBase = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+    private List<String> emptyDirsUnderBase = new ArrayList<>();', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Constructor.', nan]\n",
      "['+     * @param fs filesystem.', nan]\n",
      "['+     * @param base base directory for all creation operations.', nan]\n",
      "['+     */', nan]\n",
      "['+    private CreatedPaths(final FileSystem fs,', nan]\n",
      "['+        final Path base) {', nan]\n",
      "['+      this.fs = fs;', nan]\n",
      "['+      this.base = base;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Make a set of directories.', nan]\n",
      "['+     * @param names varargs list of paths under the base.', nan]\n",
      "['+     * @return number of entries created.', nan]\n",
      "['+     * @throws IOException failure', nan]\n",
      "['+     */', nan]\n",
      "['+    private int dirs(String... names) throws IOException {', nan]\n",
      "['+      for (String name : names) {', nan]\n",
      "['+        mkdir(name);', nan]\n",
      "['+      }', nan]\n",
      "['+      return names.length;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Create a single directory under the base.', nan]\n",
      "['+     * @param name name/relative names of the directory', nan]\n",
      "['+     * @return the path of the new entry.', nan]\n",
      "['+     */', nan]\n",
      "['+    private Path mkdir(String name) throws IOException {', nan]\n",
      "['+      Path dir = toPath(base, name);', nan]\n",
      "['+      fs.mkdirs(dir);', nan]\n",
      "['+      dirs.add(dir);', nan]\n",
      "['+      dirsUnderBase.add(name);', nan]\n",
      "['+      return dir;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Make a set of empty directories.', nan]\n",
      "['+     * @param names varargs list of paths under the base.', nan]\n",
      "['+     * @return number of entries created.', nan]\n",
      "['+     * @throws IOException failure', nan]\n",
      "['+     */', nan]\n",
      "['+    private int emptydirs(String... names) throws IOException {', nan]\n",
      "['+      for (String name : names) {', nan]\n",
      "['+        emptydir(name);', nan]\n",
      "['+      }', nan]\n",
      "['+      return names.length;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Create an empty directory.', nan]\n",
      "['+     * @param name name under the base dir', nan]\n",
      "['+     * @return the path', nan]\n",
      "['+     * @throws IOException failure', nan]\n",
      "['+     */', nan]\n",
      "['+    private Path emptydir(String name) throws IOException {', nan]\n",
      "['+      Path dir = toPath(base, name);', nan]\n",
      "['+      fs.mkdirs(dir);', nan]\n",
      "['+      emptyDirs.add(dir);', nan]\n",
      "['+      emptyDirsUnderBase.add(name);', nan]\n",
      "['+      return dir;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Make a set of files.', nan]\n",
      "['+     * @param names varargs list of paths under the base.', nan]\n",
      "['+     * @return number of entries created.', nan]\n",
      "['+     * @throws IOException failure', nan]\n",
      "['+     */', nan]\n",
      "['+    private int files(String... names) throws IOException {', nan]\n",
      "['+      for (String name : names) {', nan]\n",
      "['+        mkfile(name);', nan]\n",
      "['+      }', nan]\n",
      "['+      return names.length;', nan]\n",
      "['+    }', nan]\n",
      "['+', nan]\n",
      "['+    /**', nan]\n",
      "['+     * Create a 0-byte file.', nan]\n",
      "['+     * @param name name under the base dir', nan]\n",
      "['+     * @return the path', nan]\n",
      "['+     * @throws IOException failure', nan]\n",
      "['+     */', nan]\n",
      "['+    private Path mkfile(String name)', nan]\n",
      "['+        throws IOException {', nan]\n",
      "['+      Path file = toPath(base, name);', nan]\n",
      "['+      ContractTestUtils.touch(fs, file);', nan]\n",
      "['+      files.add(file);', nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+      filesUnderBase.add(name);', nan]\n",
      "['+      return file;', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Create the \"standard\" test paths.', nan]\n",
      "['+   * @param fs filesystem', nan]\n",
      "['+   * @param base base dir', nan]\n",
      "['+   * @return the details on what was created.', nan]\n",
      "['+   */', nan]\n",
      "['+  private CreatedPaths createPaths(FileSystem fs, Path base)', nan]\n",
      "['+      throws IOException {', nan]\n",
      "['+    CreatedPaths r = new CreatedPaths(fs, base);', nan]\n",
      "['+    // the directories under which we will create files,', nan]\n",
      "['+    // so expect to have markers', nan]\n",
      "['+    r.mkdir(\"\");', nan]\n",
      "['+', nan]\n",
      "['+    // create the empty dirs', nan]\n",
      "['+    r.emptydir(\"empty\");', nan]\n",
      "['+', nan]\n",
      "['+    // dir 1 has a file underneath', nan]\n",
      "['+    r.mkdir(\"dir1\");', nan]\n",
      "['+    expectedFileCount = r.files(\"dir1/file1\");', nan]\n",
      "['+', nan]\n",
      "['+    expectedMarkersUnderDir1 = 1;', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+    // dir2 has a subdir', nan]\n",
      "['+    r.dirs(\"dir2\", \"dir2/dir3\");', nan]\n",
      "['+    // an empty subdir', nan]\n",
      "['+    r.emptydir(\"dir2/empty2\");', nan]\n",
      "['+', nan]\n",
      "['+    // and a file under itself and dir3', nan]\n",
      "['+    expectedFileCount += r.files(', nan]\n",
      "['+        \"dir2/file2\",', nan]\n",
      "['+        \"dir2/dir3/file3\");', nan]\n",
      "['+', nan]\n",
      "['+', nan]\n",
      "['+    // wrap up the expectations.', nan]\n",
      "['+    expectedMarkersUnderDir2 = 2;', nan]\n",
      "['+    expectedMarkers = expectedMarkersUnderDir1 + expectedMarkersUnderDir2;', nan]\n",
      "['+    expectedMarkersWithBaseDir = expectedMarkers + 1;', nan]\n",
      "['+    return r;', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  /**', nan]\n",
      "['+   * Verify that all the paths renamed from the source exist', nan]\n",
      "['+   * under the destination, including all empty directories.', nan]\n",
      "['+   * @param dest destination to look under.', nan]\n",
      "['+   * @param createdPaths list of created paths.', nan]\n",
      "['+   */', nan]\n",
      "['+  void verifyRenamed(final Path dest,', nan]\n",
      "['+      final CreatedPaths createdPaths) throws IOException {', nan]\n",
      "['+    // all leaf directories exist', nan]\n",
      "['+    for (String p : createdPaths.emptyDirsUnderBase) {', nan]\n",
      "['+      assertIsDirectory(toPath(dest, p));', nan]\n",
      "['+    }', nan]\n",
      "['+    // non-empty dirs', nan]\n",
      "['+    for (String p : createdPaths.dirsUnderBase) {', nan]\n",
      "['+      assertIsDirectory(toPath(dest, p));', nan]\n",
      "['+    }', nan]\n",
      "['+    // all files exist', nan]\n",
      "['+    for (String p : createdPaths.filesUnderBase) {', nan]\n",
      "['+      assertIsFile(toPath(dest, p));', nan]\n",
      "['+    }', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n",
      "['diff --git a/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/ITestMarkerToolRootOperat', 'ons.java b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/ITestMarkerToolR']\n",
      "['new file mode 100644', nan]\n",
      "['index 00000000000..02fec81513f', nan]\n",
      "['--- /dev/null', nan]\n",
      "['+++ b/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/tools/ITestMarkerToolRootOperations.ja', 'a']\n",
      "['@@ -0,0 +1,70 @@', nan]\n",
      "['+/*', nan]\n",
      "['+ * Licensed to the Apache Software Foundation (ASF) under one', nan]\n",
      "['+ * or more contributor license agreements.  See the NOTICE file', nan]\n",
      "['+ * distributed with this work for additional information', nan]\n",
      "['+ * regarding copyright ownership.  The ASF licenses this file', nan]\n",
      "['+ * to you under the Apache License, Version 2.0 (the', nan]\n",
      "['+ * \"License\"); you may not use this file except in compliance', nan]\n",
      "['+ * with the License.  You may obtain a copy of the License at', nan]\n",
      "['+ *', nan]\n",
      "['+ *     http://www.apache.org/licenses/LICENSE-2.0', nan]\n",
      "['+ *', nan]\n",
      "['+ * Unless required by applicable law or agreed to in writing, software', nan]\n",
      "['+ * distributed under the License is distributed on an \"AS IS\" BASIS,', nan]\n",
      "['+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', nan]\n",
      "['+ * See the License for the specific language governing permissions and', nan]\n",
      "['+ * limitations under the License.', nan]\n",
      "['+ */', nan]\n",
      "['+', nan]\n",
      "['+package org.apache.hadoop.fs.s3a.tools;', nan]\n",
      "['+', nan]\n",
      "['+import java.io.File;', nan]\n",
      "['+', nan]\n",
      "['+import org.junit.FixMethodOrder;', nan]\n",
      "['+import org.junit.Test;', nan]\n",
      "['+import org.junit.runners.MethodSorters;', nan]\n",
      "['+', nan]\n",
      "['+import org.apache.hadoop.fs.Path;', nan]\n",
      "['+', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.AUDIT;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.CLEAN;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.MARKERS;', nan]\n",
      "['+import static org.apache.hadoop.fs.s3a.tools.MarkerTool.OPT_OUT;', nan]\n",
      "['+', nan]\n",
      "['+/**', nan]\n",
      "['+ * Marker tool tests against the root FS; run in the sequential phase.', nan]\n",
      "['+ */', nan]\n",
      "['+@FixMethodOrder(MethodSorters.NAME_ASCENDING)', nan]\n",
      "['+public class ITestMarkerToolRootOperations extends AbstractMarkerToolTest {', nan]\n",
      "['+', nan]\n",
      "['+  private Path rootPath;', nan]\n",
      "['+', nan]\n",
      "['+  @Override', nan]\n",
      "['+  public void setup() throws Exception {', nan]\n",
      "['+    super.setup();', nan]\n",
      "['+    rootPath = getFileSystem().makeQualified(new Path(\"/\"));', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void test_100_audit_root_noauth() throws Throwable {', nan]\n",
      "['+    describe(\"Run a verbose audit\");', nan]\n",
      "['+    final File audit = tempAuditFile();', nan]\n",
      "['+    run(MARKERS, V,', nan]\n",
      "['+        AUDIT,', nan]\n",
      "['+        m(OPT_OUT), audit,', nan]\n",
      "['+        rootPath);', nan]\n",
      "['+    readOutput(audit);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+  @Test', nan]\n",
      "['+  public void test_200_clean_root() throws Throwable {', nan]\n",
      "['+    describe(\"Clean the root path\");', nan]\n",
      "['+    final File audit = tempAuditFile();', nan]\n",
      "['+    run(MARKERS, V,', nan]\n",
      "['+        CLEAN,', nan]\n",
      "['+        m(OPT_OUT), audit,', nan]\n",
      "['+        rootPath);', nan]\n",
      "['+    readOutput(audit);', nan]\n",
      "['+  }', nan]\n",
      "['+', nan]\n",
      "['+}', nan]\n"
     ]
    }
   ],
   "source": [
    "for i in df.iterrows():\n",
    "    print(list(i[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
